[0m11:12:04.540412 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026EBD9F5FD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026EBB700190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026EBB577890>]}


============================== 11:12:04.549461 | 18224296-6a48-47be-addb-4cdccc136156 ==============================
[0m11:12:04.549461 [info ] [MainThread]: Running with dbt=1.10.13
[0m11:12:04.550212 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'write_json': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'profiles_dir': 'D:\\lab2\\dbt_project', 'debug': 'False', 'static_parser': 'True', 'log_cache_events': 'False', 'empty': 'None', 'log_format': 'default', 'quiet': 'False', 'use_colors': 'True', 'partial_parse': 'True', 'version_check': 'True', 'log_path': 'D:\\lab2\\dbt_project\\logs', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'fail_fast': 'False', 'no_print': 'None', 'invocation_command': 'dbt debug', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'target_path': 'None', 'use_experimental_parser': 'False'}
[0m11:12:04.601555 [info ] [MainThread]: dbt version: 1.10.13
[0m11:12:04.602032 [info ] [MainThread]: python version: 3.13.2
[0m11:12:04.602399 [info ] [MainThread]: python path: C:\Users\HP\AppData\Local\Programs\Python\Python313\python.exe
[0m11:12:04.602760 [info ] [MainThread]: os info: Windows-11-10.0.26200-SP0
[0m11:12:04.789373 [info ] [MainThread]: Using profiles dir at D:\lab2\dbt_project
[0m11:12:04.789886 [info ] [MainThread]: Using profiles.yml file at D:\lab2\dbt_project\profiles.yml
[0m11:12:04.790305 [info ] [MainThread]: Using dbt_project.yml file at D:\lab2\dbt_project\dbt_project.yml
[0m11:12:04.793462 [info ] [MainThread]: adapter type: duckdb
[0m11:12:04.793940 [info ] [MainThread]: adapter version: 1.10.0
[0m11:12:04.805951 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality
The `data-paths` config has been renamed to `seed-paths`. Please update your
`dbt_project.yml` configuration to reflect this change.
[0m11:12:04.806578 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '18224296-6a48-47be-addb-4cdccc136156', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026EBFE7C8A0>]}
[0m11:12:04.918068 [info ] [MainThread]: Configuration:
[0m11:12:04.918715 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m11:12:04.919104 [info ] [MainThread]:   dbt_project.yml file [[31mERROR invalid[0m]
[0m11:12:04.919488 [info ] [MainThread]: Required dependencies:
[0m11:12:04.919880 [debug] [MainThread]: Executing "git --help"
[0m11:12:04.975929 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--no-lazy-fetch]\n           [--no-optional-locks] [--no-advice] [--bare] [--git-dir=<path>]\n           [--work-tree=<path>] [--namespace=<name>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone      Clone a repository into a new directory\n   init       Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add        Add file contents to the index\n   mv         Move or rename a file, a directory, or a symlink\n   restore    Restore working tree files\n   rm         Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect     Use binary search to find the commit that introduced a bug\n   diff       Show changes between commits, commit and working tree, etc\n   grep       Print lines matching a pattern\n   log        Show commit logs\n   show       Show various types of objects\n   status     Show the working tree status\n\ngrow, mark and tweak your common history\n   backfill   Download missing objects in a partial clone\n   branch     List, create, or delete branches\n   commit     Record changes to the repository\n   merge      Join two or more development histories together\n   rebase     Reapply commits on top of another base tip\n   reset      Reset current HEAD to the specified state\n   switch     Switch branches\n   tag        Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch      Download objects and refs from another repository\n   pull       Fetch from and integrate with another repository or a local branch\n   push       Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m11:12:04.976510 [debug] [MainThread]: STDERR: "b''"
[0m11:12:04.976994 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m11:12:04.977409 [info ] [MainThread]: Connection:
[0m11:12:04.977791 [info ] [MainThread]:   database: memory
[0m11:12:04.978164 [info ] [MainThread]:   schema: main
[0m11:12:04.978523 [info ] [MainThread]:   path: :memory:
[0m11:12:04.978879 [info ] [MainThread]:   config_options: None
[0m11:12:04.979234 [info ] [MainThread]:   extensions: None
[0m11:12:04.979585 [info ] [MainThread]:   settings: {}
[0m11:12:04.979947 [info ] [MainThread]:   external_root: .
[0m11:12:04.980354 [info ] [MainThread]:   use_credential_provider: None
[0m11:12:04.980712 [info ] [MainThread]:   attach: None
[0m11:12:04.981062 [info ] [MainThread]:   filesystems: None
[0m11:12:04.981409 [info ] [MainThread]:   remote: None
[0m11:12:04.981753 [info ] [MainThread]:   plugins: None
[0m11:12:04.982109 [info ] [MainThread]:   disable_transactions: False
[0m11:12:04.982676 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m11:12:05.647787 [debug] [MainThread]: Acquiring new duckdb connection 'debug'
[0m11:12:11.207779 [debug] [MainThread]: Using duckdb connection "debug"
[0m11:12:11.208698 [debug] [MainThread]: On debug: select 1 as id
[0m11:12:11.209443 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:12:11.288059 [debug] [MainThread]: SQL status: OK in 0.078 seconds
[0m11:12:11.290453 [debug] [MainThread]: On debug: Close
[0m11:12:11.291338 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m11:12:11.292250 [info ] [MainThread]: [31m1 check failed:[0m
[0m11:12:11.293134 [info ] [MainThread]: Project loading failed for the following reason:
Runtime Error
  at path ['sources']: [{'name': 'raw_data', 'tables': [{'name': 'final_data_csv', 'columns': [{'name': 'title'}, {'name': 'location'}, {'name': 'postedTime'}, {'name': 'publishedAt'}, {'name': 'jobUrl'}, {'name': 'companyName'}, {'name': 'companyUrl'}, {'name': 'description'}, {'name': 'contractType'}, {'name': 'workType'}]}]}] is not of type 'object'

Error encountered in D:\lab2\dbt_project\dbt_project.yml


[0m11:12:11.294498 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- ConfigDataPathDeprecation: 1 occurrence
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m11:12:11.296837 [debug] [MainThread]: Command `dbt debug` failed at 11:12:11.296553 after 7.00 seconds
[0m11:12:11.297669 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m11:12:11.298605 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026EBFEFD7B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026EBFEFD6A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026EBED43A50>]}
[0m11:12:11.299667 [debug] [MainThread]: Flushing usage events
[0m11:12:13.382221 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:13:22.822978 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001888B1E1D30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001888BCB9E50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018888D4D6D0>]}


============================== 11:13:22.834559 | 2ceadd52-98a5-4191-9740-8e2f43734681 ==============================
[0m11:13:22.834559 [info ] [MainThread]: Running with dbt=1.10.13
[0m11:13:22.836245 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'target_path': 'None', 'cache_selected_only': 'False', 'warn_error': 'None', 'profiles_dir': 'D:\\lab2\\dbt_project', 'debug': 'False', 'static_parser': 'True', 'log_cache_events': 'False', 'empty': 'None', 'quiet': 'False', 'log_format': 'default', 'use_colors': 'True', 'partial_parse': 'True', 'version_check': 'True', 'log_path': 'D:\\lab2\\dbt_project\\logs', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'fail_fast': 'False', 'no_print': 'None', 'invocation_command': 'dbt debug', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'write_json': 'True'}
[0m11:13:22.894263 [info ] [MainThread]: dbt version: 1.10.13
[0m11:13:22.895262 [info ] [MainThread]: python version: 3.13.2
[0m11:13:22.896035 [info ] [MainThread]: python path: C:\Users\HP\AppData\Local\Programs\Python\Python313\python.exe
[0m11:13:22.897034 [info ] [MainThread]: os info: Windows-11-10.0.26200-SP0
[0m11:13:23.056176 [info ] [MainThread]: Using profiles dir at D:\lab2\dbt_project
[0m11:13:23.056913 [info ] [MainThread]: Using profiles.yml file at D:\lab2\dbt_project\profiles.yml
[0m11:13:23.057371 [info ] [MainThread]: Using dbt_project.yml file at D:\lab2\dbt_project\dbt_project.yml
[0m11:13:23.060182 [info ] [MainThread]: adapter type: duckdb
[0m11:13:23.060797 [info ] [MainThread]: adapter version: 1.10.0
[0m11:13:23.070656 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality
The `data-paths` config has been renamed to `seed-paths`. Please update your
`dbt_project.yml` configuration to reflect this change.
[0m11:13:23.071394 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '2ceadd52-98a5-4191-9740-8e2f43734681', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001888D5A88A0>]}
[0m11:13:23.162936 [info ] [MainThread]: Configuration:
[0m11:13:23.163602 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m11:13:23.164136 [info ] [MainThread]:   dbt_project.yml file [[31mERROR invalid[0m]
[0m11:13:23.164695 [info ] [MainThread]: Required dependencies:
[0m11:13:23.165239 [debug] [MainThread]: Executing "git --help"
[0m11:13:23.211030 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--no-lazy-fetch]\n           [--no-optional-locks] [--no-advice] [--bare] [--git-dir=<path>]\n           [--work-tree=<path>] [--namespace=<name>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone      Clone a repository into a new directory\n   init       Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add        Add file contents to the index\n   mv         Move or rename a file, a directory, or a symlink\n   restore    Restore working tree files\n   rm         Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect     Use binary search to find the commit that introduced a bug\n   diff       Show changes between commits, commit and working tree, etc\n   grep       Print lines matching a pattern\n   log        Show commit logs\n   show       Show various types of objects\n   status     Show the working tree status\n\ngrow, mark and tweak your common history\n   backfill   Download missing objects in a partial clone\n   branch     List, create, or delete branches\n   commit     Record changes to the repository\n   merge      Join two or more development histories together\n   rebase     Reapply commits on top of another base tip\n   reset      Reset current HEAD to the specified state\n   switch     Switch branches\n   tag        Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch      Download objects and refs from another repository\n   pull       Fetch from and integrate with another repository or a local branch\n   push       Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m11:13:23.211607 [debug] [MainThread]: STDERR: "b''"
[0m11:13:23.212076 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m11:13:23.212789 [info ] [MainThread]: Connection:
[0m11:13:23.213522 [info ] [MainThread]:   database: memory
[0m11:13:23.214274 [info ] [MainThread]:   schema: main
[0m11:13:23.214920 [info ] [MainThread]:   path: :memory:
[0m11:13:23.215527 [info ] [MainThread]:   config_options: None
[0m11:13:23.216082 [info ] [MainThread]:   extensions: None
[0m11:13:23.216616 [info ] [MainThread]:   settings: {}
[0m11:13:23.217155 [info ] [MainThread]:   external_root: .
[0m11:13:23.217671 [info ] [MainThread]:   use_credential_provider: None
[0m11:13:23.218170 [info ] [MainThread]:   attach: None
[0m11:13:23.218725 [info ] [MainThread]:   filesystems: None
[0m11:13:23.219298 [info ] [MainThread]:   remote: None
[0m11:13:23.219835 [info ] [MainThread]:   plugins: None
[0m11:13:23.220372 [info ] [MainThread]:   disable_transactions: False
[0m11:13:23.221094 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m11:13:23.494773 [debug] [MainThread]: Acquiring new duckdb connection 'debug'
[0m11:13:23.566631 [debug] [MainThread]: Using duckdb connection "debug"
[0m11:13:23.566976 [debug] [MainThread]: On debug: select 1 as id
[0m11:13:23.567236 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:13:23.582029 [debug] [MainThread]: SQL status: OK in 0.015 seconds
[0m11:13:23.582967 [debug] [MainThread]: On debug: Close
[0m11:13:23.583286 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m11:13:23.583919 [info ] [MainThread]: [31m1 check failed:[0m
[0m11:13:23.584383 [info ] [MainThread]: Project loading failed for the following reason:
Runtime Error
  at path ['sources']: [{'name': 'raw_data', 'tables': [{'name': 'final_data_csv', 'columns': [{'name': 'title'}, {'name': 'location'}, {'name': 'postedTime'}, {'name': 'publishedAt'}, {'name': 'jobUrl'}, {'name': 'companyName'}, {'name': 'companyUrl'}, {'name': 'description'}, {'name': 'contractType'}, {'name': 'workType'}]}]}] is not of type 'object'

Error encountered in D:\lab2\dbt_project\dbt_project.yml


[0m11:13:23.585023 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- ConfigDataPathDeprecation: 1 occurrence
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m11:13:23.586107 [debug] [MainThread]: Command `dbt debug` failed at 11:13:23.586003 after 1.05 seconds
[0m11:13:23.586404 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m11:13:23.586705 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001888D6297B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001888D6296A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001888C523A50>]}
[0m11:13:23.587027 [debug] [MainThread]: Flushing usage events
[0m11:13:27.921886 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:13:50.738242 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018EFB455FD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018EF9160190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018EF8FD7890>]}


============================== 11:13:50.746909 | 7d07b98c-f5ef-481a-b1a4-2d7b26e24237 ==============================
[0m11:13:50.746909 [info ] [MainThread]: Running with dbt=1.10.13
[0m11:13:50.747995 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'target_path': 'None', 'cache_selected_only': 'False', 'warn_error': 'None', 'profiles_dir': 'D:\\lab2\\dbt_project', 'debug': 'False', 'static_parser': 'True', 'log_cache_events': 'False', 'empty': 'None', 'log_format': 'default', 'quiet': 'False', 'use_colors': 'True', 'partial_parse': 'True', 'version_check': 'True', 'log_path': 'D:\\lab2\\dbt_project\\logs', 'introspect': 'True', 'no_print': 'None', 'fail_fast': 'False', 'invocation_command': 'dbt debug', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'write_json': 'True', 'use_experimental_parser': 'False'}
[0m11:13:50.787879 [info ] [MainThread]: dbt version: 1.10.13
[0m11:13:50.788549 [info ] [MainThread]: python version: 3.13.2
[0m11:13:50.789057 [info ] [MainThread]: python path: C:\Users\HP\AppData\Local\Programs\Python\Python313\python.exe
[0m11:13:50.789529 [info ] [MainThread]: os info: Windows-11-10.0.26200-SP0
[0m11:13:50.937288 [info ] [MainThread]: Using profiles dir at D:\lab2\dbt_project
[0m11:13:50.937688 [info ] [MainThread]: Using profiles.yml file at D:\lab2\dbt_project\profiles.yml
[0m11:13:50.937977 [info ] [MainThread]: Using dbt_project.yml file at D:\lab2\dbt_project\dbt_project.yml
[0m11:13:50.940473 [info ] [MainThread]: adapter type: duckdb
[0m11:13:50.940840 [info ] [MainThread]: adapter version: 1.10.0
[0m11:13:51.084802 [info ] [MainThread]: Configuration:
[0m11:13:51.085289 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m11:13:51.085665 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m11:13:51.086030 [info ] [MainThread]: Required dependencies:
[0m11:13:51.086404 [debug] [MainThread]: Executing "git --help"
[0m11:13:51.127220 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--no-lazy-fetch]\n           [--no-optional-locks] [--no-advice] [--bare] [--git-dir=<path>]\n           [--work-tree=<path>] [--namespace=<name>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone      Clone a repository into a new directory\n   init       Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add        Add file contents to the index\n   mv         Move or rename a file, a directory, or a symlink\n   restore    Restore working tree files\n   rm         Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect     Use binary search to find the commit that introduced a bug\n   diff       Show changes between commits, commit and working tree, etc\n   grep       Print lines matching a pattern\n   log        Show commit logs\n   show       Show various types of objects\n   status     Show the working tree status\n\ngrow, mark and tweak your common history\n   backfill   Download missing objects in a partial clone\n   branch     List, create, or delete branches\n   commit     Record changes to the repository\n   merge      Join two or more development histories together\n   rebase     Reapply commits on top of another base tip\n   reset      Reset current HEAD to the specified state\n   switch     Switch branches\n   tag        Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch      Download objects and refs from another repository\n   pull       Fetch from and integrate with another repository or a local branch\n   push       Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m11:13:51.127910 [debug] [MainThread]: STDERR: "b''"
[0m11:13:51.128324 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m11:13:51.128858 [info ] [MainThread]: Connection:
[0m11:13:51.129308 [info ] [MainThread]:   database: memory
[0m11:13:51.129658 [info ] [MainThread]:   schema: main
[0m11:13:51.129998 [info ] [MainThread]:   path: :memory:
[0m11:13:51.130395 [info ] [MainThread]:   config_options: None
[0m11:13:51.130802 [info ] [MainThread]:   extensions: None
[0m11:13:51.131304 [info ] [MainThread]:   settings: {}
[0m11:13:51.131797 [info ] [MainThread]:   external_root: .
[0m11:13:51.132232 [info ] [MainThread]:   use_credential_provider: None
[0m11:13:51.132585 [info ] [MainThread]:   attach: None
[0m11:13:51.132901 [info ] [MainThread]:   filesystems: None
[0m11:13:51.133237 [info ] [MainThread]:   remote: None
[0m11:13:51.133601 [info ] [MainThread]:   plugins: None
[0m11:13:51.133989 [info ] [MainThread]:   disable_transactions: False
[0m11:13:51.134584 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m11:13:51.485626 [debug] [MainThread]: Acquiring new duckdb connection 'debug'
[0m11:13:51.546206 [debug] [MainThread]: Using duckdb connection "debug"
[0m11:13:51.546564 [debug] [MainThread]: On debug: select 1 as id
[0m11:13:51.546824 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:13:51.563745 [debug] [MainThread]: SQL status: OK in 0.017 seconds
[0m11:13:51.564889 [debug] [MainThread]: On debug: Close
[0m11:13:51.565328 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m11:13:51.565812 [info ] [MainThread]: [32mAll checks passed![0m
[0m11:13:51.566840 [debug] [MainThread]: Command `dbt debug` succeeded at 11:13:51.566700 after 1.06 seconds
[0m11:13:51.567197 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m11:13:51.567558 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018EFD97DF30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018EFDA1D5B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018EFD8F4E20>]}
[0m11:13:51.568011 [debug] [MainThread]: Flushing usage events
[0m11:13:53.172906 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:14:10.828640 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B967E59FD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B965B64190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B9659D7890>]}


============================== 11:14:10.840611 | 8449bea2-1457-4d5f-87e4-7383637e2727 ==============================
[0m11:14:10.840611 [info ] [MainThread]: Running with dbt=1.10.13
[0m11:14:10.842061 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'use_experimental_parser': 'False', 'cache_selected_only': 'False', 'warn_error': 'None', 'empty': 'None', 'debug': 'False', 'static_parser': 'True', 'log_cache_events': 'False', 'profiles_dir': 'D:\\lab2\\dbt_project', 'log_format': 'default', 'quiet': 'False', 'use_colors': 'True', 'partial_parse': 'True', 'version_check': 'True', 'log_path': 'D:\\lab2\\dbt_project\\logs', 'introspect': 'True', 'invocation_command': 'dbt debug', 'fail_fast': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'no_print': 'None', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'write_json': 'True', 'target_path': 'None'}
[0m11:14:10.879540 [info ] [MainThread]: dbt version: 1.10.13
[0m11:14:10.880022 [info ] [MainThread]: python version: 3.13.2
[0m11:14:10.880426 [info ] [MainThread]: python path: C:\Users\HP\AppData\Local\Programs\Python\Python313\python.exe
[0m11:14:10.880939 [info ] [MainThread]: os info: Windows-11-10.0.26200-SP0
[0m11:14:11.035995 [info ] [MainThread]: Using profiles dir at D:\lab2\dbt_project
[0m11:14:11.036487 [info ] [MainThread]: Using profiles.yml file at D:\lab2\dbt_project\profiles.yml
[0m11:14:11.036856 [info ] [MainThread]: Using dbt_project.yml file at D:\lab2\dbt_project\dbt_project.yml
[0m11:14:11.042173 [info ] [MainThread]: adapter type: duckdb
[0m11:14:11.043155 [info ] [MainThread]: adapter version: 1.10.0
[0m11:14:11.247803 [info ] [MainThread]: Configuration:
[0m11:14:11.248231 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m11:14:11.248560 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m11:14:11.248859 [info ] [MainThread]: Required dependencies:
[0m11:14:11.249167 [debug] [MainThread]: Executing "git --help"
[0m11:14:11.289829 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--no-lazy-fetch]\n           [--no-optional-locks] [--no-advice] [--bare] [--git-dir=<path>]\n           [--work-tree=<path>] [--namespace=<name>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone      Clone a repository into a new directory\n   init       Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add        Add file contents to the index\n   mv         Move or rename a file, a directory, or a symlink\n   restore    Restore working tree files\n   rm         Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect     Use binary search to find the commit that introduced a bug\n   diff       Show changes between commits, commit and working tree, etc\n   grep       Print lines matching a pattern\n   log        Show commit logs\n   show       Show various types of objects\n   status     Show the working tree status\n\ngrow, mark and tweak your common history\n   backfill   Download missing objects in a partial clone\n   branch     List, create, or delete branches\n   commit     Record changes to the repository\n   merge      Join two or more development histories together\n   rebase     Reapply commits on top of another base tip\n   reset      Reset current HEAD to the specified state\n   switch     Switch branches\n   tag        Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch      Download objects and refs from another repository\n   pull       Fetch from and integrate with another repository or a local branch\n   push       Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m11:14:11.290438 [debug] [MainThread]: STDERR: "b''"
[0m11:14:11.290893 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m11:14:11.291283 [info ] [MainThread]: Connection:
[0m11:14:11.291646 [info ] [MainThread]:   database: memory
[0m11:14:11.291992 [info ] [MainThread]:   schema: main
[0m11:14:11.292340 [info ] [MainThread]:   path: :memory:
[0m11:14:11.292675 [info ] [MainThread]:   config_options: None
[0m11:14:11.293013 [info ] [MainThread]:   extensions: None
[0m11:14:11.293356 [info ] [MainThread]:   settings: {}
[0m11:14:11.293695 [info ] [MainThread]:   external_root: .
[0m11:14:11.294031 [info ] [MainThread]:   use_credential_provider: None
[0m11:14:11.294385 [info ] [MainThread]:   attach: None
[0m11:14:11.294724 [info ] [MainThread]:   filesystems: None
[0m11:14:11.295064 [info ] [MainThread]:   remote: None
[0m11:14:11.295403 [info ] [MainThread]:   plugins: None
[0m11:14:11.295742 [info ] [MainThread]:   disable_transactions: False
[0m11:14:11.296304 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m11:14:11.555882 [debug] [MainThread]: Acquiring new duckdb connection 'debug'
[0m11:14:11.619299 [debug] [MainThread]: Using duckdb connection "debug"
[0m11:14:11.619685 [debug] [MainThread]: On debug: select 1 as id
[0m11:14:11.619967 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:14:11.636811 [debug] [MainThread]: SQL status: OK in 0.017 seconds
[0m11:14:11.637976 [debug] [MainThread]: On debug: Close
[0m11:14:11.638552 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m11:14:11.638962 [info ] [MainThread]: [32mAll checks passed![0m
[0m11:14:11.639904 [debug] [MainThread]: Command `dbt debug` succeeded at 11:14:11.639783 after 1.02 seconds
[0m11:14:11.640235 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m11:14:11.640601 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B96A3A9F30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B96A44D5B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B96A320E20>]}
[0m11:14:11.641002 [debug] [MainThread]: Flushing usage events
[0m11:14:13.061459 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:14:16.502523 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015566725FD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015564424190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015564297890>]}


============================== 11:14:16.507849 | 5dd6369d-0357-4a8d-a46b-46a2b72a3c83 ==============================
[0m11:14:16.507849 [info ] [MainThread]: Running with dbt=1.10.13
[0m11:14:16.508530 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'target_path': 'None', 'cache_selected_only': 'False', 'warn_error': 'None', 'profiles_dir': 'D:\\lab2\\dbt_project', 'debug': 'False', 'static_parser': 'True', 'log_cache_events': 'False', 'empty': 'False', 'quiet': 'False', 'log_format': 'default', 'use_colors': 'True', 'partial_parse': 'True', 'version_check': 'True', 'log_path': 'D:\\lab2\\dbt_project\\logs', 'introspect': 'True', 'invocation_command': 'dbt run', 'fail_fast': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'write_json': 'True'}
[0m11:14:16.989393 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '5dd6369d-0357-4a8d-a46b-46a2b72a3c83', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015566BDF230>]}
[0m11:14:17.076673 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '5dd6369d-0357-4a8d-a46b-46a2b72a3c83', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015566798380>]}
[0m11:14:17.079855 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m11:14:17.486115 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m11:14:17.488615 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m11:14:17.489618 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '5dd6369d-0357-4a8d-a46b-46a2b72a3c83', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015567A04F50>]}
[0m11:14:21.586840 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- seeds
[0m11:14:21.592507 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '5dd6369d-0357-4a8d-a46b-46a2b72a3c83', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015568D772F0>]}
[0m11:14:21.700799 [debug] [MainThread]: Wrote artifact WritableManifest to D:\lab2\dbt_project\target\manifest.json
[0m11:14:21.704637 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\lab2\dbt_project\target\semantic_manifest.json
[0m11:14:21.765054 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '5dd6369d-0357-4a8d-a46b-46a2b72a3c83', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015567A72DD0>]}
[0m11:14:21.765613 [info ] [MainThread]: Found 13 models, 456 macros
[0m11:14:21.766178 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5dd6369d-0357-4a8d-a46b-46a2b72a3c83', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015568E44530>]}
[0m11:14:21.768593 [info ] [MainThread]: 
[0m11:14:21.769092 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m11:14:21.769456 [info ] [MainThread]: 
[0m11:14:21.769991 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m11:14:21.776479 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_memory'
[0m11:14:21.789514 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_memory'
[0m11:14:21.792466 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_memory'
[0m11:14:21.867864 [debug] [ThreadPool]: Using duckdb connection "list_memory"
[0m11:14:21.868279 [debug] [ThreadPool]: On list_memory: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_memory"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"memory"'
    
  
  
[0m11:14:21.868604 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:14:21.868992 [debug] [ThreadPool]: Using duckdb connection "list_memory"
[0m11:14:21.869735 [debug] [ThreadPool]: On list_memory: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_memory"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"memory"'
    
  
  
[0m11:14:21.870062 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:14:21.870709 [debug] [ThreadPool]: Using duckdb connection "list_memory"
[0m11:14:21.871050 [debug] [ThreadPool]: On list_memory: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_memory"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"memory"'
    
  
  
[0m11:14:21.871359 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:14:21.906201 [debug] [ThreadPool]: SQL status: OK in 0.036 seconds
[0m11:14:21.907149 [debug] [ThreadPool]: SQL status: OK in 0.036 seconds
[0m11:14:21.908698 [debug] [ThreadPool]: SQL status: OK in 0.040 seconds
[0m11:14:21.912269 [debug] [ThreadPool]: On list_memory: Close
[0m11:14:21.915580 [debug] [ThreadPool]: On list_memory: Close
[0m11:14:21.919346 [debug] [ThreadPool]: On list_memory: Close
[0m11:14:21.921947 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_memory, now create_memory_main_bronze)
[0m11:14:21.922778 [debug] [ThreadPool]: Creating schema "database: "memory"
schema: "main_bronze"
"
[0m11:14:21.923539 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_memory, now create_memory_main_gold)
[0m11:14:21.932552 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_memory, now create_memory_main_silver)
[0m11:14:21.933211 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_bronze"
[0m11:14:21.933881 [debug] [ThreadPool]: Creating schema "database: "memory"
schema: "main_gold"
"
[0m11:14:21.934543 [debug] [ThreadPool]: Creating schema "database: "memory"
schema: "main_silver"
"
[0m11:14:21.935020 [debug] [ThreadPool]: On create_memory_main_bronze: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_memory_main_bronze"} */

    
        select type from duckdb_databases()
        where lower(database_name)='memory'
        and type='sqlite'
    
  
[0m11:14:21.940232 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_gold"
[0m11:14:21.942424 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_silver"
[0m11:14:21.943028 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:14:21.943513 [debug] [ThreadPool]: On create_memory_main_gold: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_memory_main_gold"} */

    
        select type from duckdb_databases()
        where lower(database_name)='memory'
        and type='sqlite'
    
  
[0m11:14:21.943998 [debug] [ThreadPool]: On create_memory_main_silver: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_memory_main_silver"} */

    
        select type from duckdb_databases()
        where lower(database_name)='memory'
        and type='sqlite'
    
  
[0m11:14:21.944775 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:14:21.945256 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:14:21.946020 [debug] [ThreadPool]: SQL status: OK in 0.003 seconds
[0m11:14:21.946732 [debug] [ThreadPool]: SQL status: OK in 0.002 seconds
[0m11:14:21.948372 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_bronze"
[0m11:14:21.950158 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_gold"
[0m11:14:21.950838 [debug] [ThreadPool]: On create_memory_main_bronze: BEGIN
[0m11:14:21.951356 [debug] [ThreadPool]: SQL status: OK in 0.006 seconds
[0m11:14:21.951870 [debug] [ThreadPool]: On create_memory_main_gold: BEGIN
[0m11:14:21.953776 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_silver"
[0m11:14:21.954544 [debug] [ThreadPool]: SQL status: OK in 0.002 seconds
[0m11:14:21.954968 [debug] [ThreadPool]: On create_memory_main_silver: BEGIN
[0m11:14:21.955448 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_bronze"
[0m11:14:21.955857 [debug] [ThreadPool]: SQL status: OK in 0.002 seconds
[0m11:14:21.956484 [debug] [ThreadPool]: On create_memory_main_bronze: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_memory_main_bronze"} */

    
    
        create schema if not exists "memory"."main_bronze"
    
[0m11:14:21.956966 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m11:14:21.957385 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_gold"
[0m11:14:21.958171 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_silver"
[0m11:14:21.958609 [debug] [ThreadPool]: On create_memory_main_gold: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_memory_main_gold"} */

    
    
        create schema if not exists "memory"."main_gold"
    
[0m11:14:21.959147 [debug] [ThreadPool]: On create_memory_main_silver: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_memory_main_silver"} */

    
    
        create schema if not exists "memory"."main_silver"
    
[0m11:14:21.959612 [debug] [ThreadPool]: SQL status: OK in 0.002 seconds
[0m11:14:21.960972 [debug] [ThreadPool]: On create_memory_main_bronze: COMMIT
[0m11:14:21.961380 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_bronze"
[0m11:14:21.961747 [debug] [ThreadPool]: On create_memory_main_bronze: COMMIT
[0m11:14:21.962274 [debug] [ThreadPool]: SQL status: OK in 0.002 seconds
[0m11:14:21.963312 [debug] [ThreadPool]: On create_memory_main_silver: COMMIT
[0m11:14:21.963714 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_silver"
[0m11:14:21.964086 [debug] [ThreadPool]: On create_memory_main_silver: COMMIT
[0m11:14:21.964594 [debug] [ThreadPool]: SQL status: OK in 0.002 seconds
[0m11:14:21.965017 [debug] [ThreadPool]: On create_memory_main_bronze: Close
[0m11:14:21.965422 [debug] [ThreadPool]: SQL status: OK in 0.005 seconds
[0m11:14:21.966581 [debug] [ThreadPool]: On create_memory_main_gold: COMMIT
[0m11:14:21.967045 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_gold"
[0m11:14:21.967451 [debug] [ThreadPool]: On create_memory_main_gold: COMMIT
[0m11:14:21.968046 [debug] [ThreadPool]: SQL status: OK in 0.004 seconds
[0m11:14:21.968483 [debug] [ThreadPool]: On create_memory_main_silver: Close
[0m11:14:21.968911 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m11:14:21.969321 [debug] [ThreadPool]: On create_memory_main_gold: Close
[0m11:14:21.972637 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_memory_main_bronze'
[0m11:14:21.979168 [debug] [ThreadPool]: Using duckdb connection "list_memory_main_bronze"
[0m11:14:21.979803 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_memory_main_gold'
[0m11:14:21.980239 [debug] [ThreadPool]: On list_memory_main_bronze: BEGIN
[0m11:14:21.982974 [debug] [ThreadPool]: Using duckdb connection "list_memory_main_gold"
[0m11:14:21.983629 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_memory_main_silver'
[0m11:14:21.984076 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:14:21.984710 [debug] [ThreadPool]: On list_memory_main_gold: BEGIN
[0m11:14:21.986658 [debug] [ThreadPool]: Using duckdb connection "list_memory_main_silver"
[0m11:14:21.987459 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:14:21.987974 [debug] [ThreadPool]: SQL status: OK in 0.004 seconds
[0m11:14:21.988406 [debug] [ThreadPool]: On list_memory_main_silver: BEGIN
[0m11:14:21.989070 [debug] [ThreadPool]: Using duckdb connection "list_memory_main_bronze"
[0m11:14:21.989561 [debug] [ThreadPool]: SQL status: OK in 0.002 seconds
[0m11:14:21.989963 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:14:21.990560 [debug] [ThreadPool]: On list_memory_main_bronze: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_memory_main_bronze"} */
select
      'memory' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main_bronze'
    and lower(table_catalog) = 'memory'
  
[0m11:14:21.991029 [debug] [ThreadPool]: Using duckdb connection "list_memory_main_gold"
[0m11:14:21.991965 [debug] [ThreadPool]: On list_memory_main_gold: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_memory_main_gold"} */
select
      'memory' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main_gold'
    and lower(table_catalog) = 'memory'
  
[0m11:14:21.992427 [debug] [ThreadPool]: SQL status: OK in 0.002 seconds
[0m11:14:21.992828 [debug] [ThreadPool]: Using duckdb connection "list_memory_main_silver"
[0m11:14:21.993218 [debug] [ThreadPool]: On list_memory_main_silver: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_memory_main_silver"} */
select
      'memory' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main_silver'
    and lower(table_catalog) = 'memory'
  
[0m11:14:22.030675 [debug] [ThreadPool]: SQL status: OK in 0.038 seconds
[0m11:14:22.031301 [debug] [ThreadPool]: SQL status: OK in 0.038 seconds
[0m11:14:22.032869 [debug] [ThreadPool]: On list_memory_main_gold: ROLLBACK
[0m11:14:22.034202 [debug] [ThreadPool]: On list_memory_main_silver: ROLLBACK
[0m11:14:22.034777 [debug] [ThreadPool]: SQL status: OK in 0.043 seconds
[0m11:14:22.036175 [debug] [ThreadPool]: On list_memory_main_bronze: ROLLBACK
[0m11:14:22.053832 [debug] [ThreadPool]: Failed to rollback 'list_memory_main_bronze'
[0m11:14:22.054270 [debug] [ThreadPool]: On list_memory_main_bronze: Close
[0m11:14:22.055674 [debug] [ThreadPool]: Failed to rollback 'list_memory_main_gold'
[0m11:14:22.056041 [debug] [ThreadPool]: On list_memory_main_gold: Close
[0m11:14:22.057231 [debug] [ThreadPool]: Failed to rollback 'list_memory_main_silver'
[0m11:14:22.057578 [debug] [ThreadPool]: On list_memory_main_silver: Close
[0m11:14:22.058636 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5dd6369d-0357-4a8d-a46b-46a2b72a3c83', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015569012BD0>]}
[0m11:14:22.059145 [debug] [MainThread]: Using duckdb connection "master"
[0m11:14:22.059482 [debug] [MainThread]: On master: BEGIN
[0m11:14:22.059837 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:14:22.060508 [debug] [MainThread]: SQL status: OK in 0.001 seconds
[0m11:14:22.060849 [debug] [MainThread]: On master: COMMIT
[0m11:14:22.061180 [debug] [MainThread]: Using duckdb connection "master"
[0m11:14:22.061493 [debug] [MainThread]: On master: COMMIT
[0m11:14:22.062012 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m11:14:22.062366 [debug] [MainThread]: On master: Close
[0m11:14:22.071102 [debug] [Thread-1 (]: Began running node model.job_intelligent.stg_jobs_raw
[0m11:14:22.071751 [info ] [Thread-1 (]: 1 of 13 START sql view model main_bronze.stg_jobs_raw .......................... [RUN]
[0m11:14:22.072381 [debug] [Thread-1 (]: Acquiring new duckdb connection 'model.job_intelligent.stg_jobs_raw'
[0m11:14:22.072796 [debug] [Thread-1 (]: Began compiling node model.job_intelligent.stg_jobs_raw
[0m11:14:22.081081 [debug] [Thread-1 (]: Writing injected SQL for node "model.job_intelligent.stg_jobs_raw"
[0m11:14:22.083988 [debug] [Thread-1 (]: Began executing node model.job_intelligent.stg_jobs_raw
[0m11:14:22.119169 [debug] [Thread-1 (]: Writing runtime sql for node "model.job_intelligent.stg_jobs_raw"
[0m11:14:22.121427 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.stg_jobs_raw"
[0m11:14:22.121917 [debug] [Thread-1 (]: On model.job_intelligent.stg_jobs_raw: BEGIN
[0m11:14:22.122313 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m11:14:22.123230 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m11:14:22.123642 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.stg_jobs_raw"
[0m11:14:22.124095 [debug] [Thread-1 (]: On model.job_intelligent.stg_jobs_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.stg_jobs_raw"} */

  
  create view "memory"."main_bronze"."stg_jobs_raw__dbt_tmp" as (
    -- models/bronze/stg_jobs_raw.sql
-- Bronze layer: Lecture directe des données brutes depuis final_data.csv
-- Renommage et typage minimal



-- Read directly from CSV file
SELECT
    -- Renommer les colonnes pour cohérence
    title as job_title,
    location,
    postedTime as posted_time,
    publishedAt as published_at,
    jobUrl as job_url,
    companyName as company_name,
    companyUrl as company_url,
    description as job_description,
    contractType as contract_type,
    workType as work_type,
    
    -- Métadonnées de traçabilité
    CURRENT_TIMESTAMP() as ingestion_timestamp,
    '2026-01-08 10:14:16.393416+00:00' as dbt_run_id

FROM read_csv_auto('../data/final_data.csv')

WHERE 1=1
    -- Filtrer les lignes vides
    AND title IS NOT NULL
    AND description IS NOT NULL
  );

[0m11:14:22.154915 [debug] [Thread-1 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.stg_jobs_raw"} */

  
  create view "memory"."main_bronze"."stg_jobs_raw__dbt_tmp" as (
    -- models/bronze/stg_jobs_raw.sql
-- Bronze layer: Lecture directe des données brutes depuis final_data.csv
-- Renommage et typage minimal



-- Read directly from CSV file
SELECT
    -- Renommer les colonnes pour cohérence
    title as job_title,
    location,
    postedTime as posted_time,
    publishedAt as published_at,
    jobUrl as job_url,
    companyName as company_name,
    companyUrl as company_url,
    description as job_description,
    contractType as contract_type,
    workType as work_type,
    
    -- Métadonnées de traçabilité
    CURRENT_TIMESTAMP() as ingestion_timestamp,
    '2026-01-08 10:14:16.393416+00:00' as dbt_run_id

FROM read_csv_auto('../data/final_data.csv')

WHERE 1=1
    -- Filtrer les lignes vides
    AND title IS NOT NULL
    AND description IS NOT NULL
  );

[0m11:14:22.155742 [debug] [Thread-1 (]: DuckDB adapter: Rolling back transaction.
[0m11:14:22.156532 [debug] [Thread-1 (]: On model.job_intelligent.stg_jobs_raw: ROLLBACK
[0m11:14:22.312143 [debug] [Thread-1 (]: Failed to rollback 'model.job_intelligent.stg_jobs_raw'
[0m11:14:22.312664 [debug] [Thread-1 (]: On model.job_intelligent.stg_jobs_raw: Close
[0m11:14:22.318608 [debug] [Thread-1 (]: Runtime Error in model stg_jobs_raw (models\bronze\stg_jobs_raw.sql)
  IO Error: No files found that match the pattern "../data/final_data.csv"
  
  LINE 29: FROM read_csv_auto('../data/final_data.csv')
                ^
[0m11:14:22.322194 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5dd6369d-0357-4a8d-a46b-46a2b72a3c83', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000155690A9E90>]}
[0m11:14:22.323148 [error] [Thread-1 (]: 1 of 13 ERROR creating sql view model main_bronze.stg_jobs_raw ................. [[31mERROR[0m in 0.25s]
[0m11:14:22.323948 [debug] [Thread-1 (]: Finished running node model.job_intelligent.stg_jobs_raw
[0m11:14:22.324705 [debug] [Thread-7 (]: Marking all children of 'model.job_intelligent.stg_jobs_raw' to be skipped because of status 'error'.  Reason: Runtime Error in model stg_jobs_raw (models\bronze\stg_jobs_raw.sql)
  IO Error: No files found that match the pattern "../data/final_data.csv"
  
  LINE 29: FROM read_csv_auto('../data/final_data.csv')
                ^.
[0m11:14:22.326349 [debug] [Thread-2 (]: Began running node model.job_intelligent.int_jobs_cleaned
[0m11:14:22.327333 [info ] [Thread-2 (]: 2 of 13 SKIP relation main_silver.int_jobs_cleaned ............................. [[33mSKIP[0m]
[0m11:14:22.328273 [debug] [Thread-2 (]: Finished running node model.job_intelligent.int_jobs_cleaned
[0m11:14:22.329280 [debug] [Thread-3 (]: Began running node model.job_intelligent.int_job_title_normalization
[0m11:14:22.329972 [info ] [Thread-3 (]: 3 of 13 SKIP relation main_silver.int_job_title_normalization .................. [[33mSKIP[0m]
[0m11:14:22.330815 [debug] [Thread-3 (]: Finished running node model.job_intelligent.int_job_title_normalization
[0m11:14:22.331998 [debug] [Thread-2 (]: Began running node model.job_intelligent.dim_time
[0m11:14:22.333745 [debug] [Thread-3 (]: Began running node model.job_intelligent.int_skills_extraction
[0m11:14:22.332918 [info ] [Thread-2 (]: 6 of 13 SKIP relation main_gold.dim_time ....................................... [[33mSKIP[0m]
[0m11:14:22.334982 [debug] [Thread-1 (]: Began running node model.job_intelligent.dim_location
[0m11:14:22.336347 [debug] [Thread-4 (]: Began running node model.job_intelligent.dim_company
[0m11:14:22.337367 [debug] [Thread-2 (]: Finished running node model.job_intelligent.dim_time
[0m11:14:22.335757 [info ] [Thread-3 (]: 7 of 13 SKIP relation main_silver.int_skills_extraction ........................ [[33mSKIP[0m]
[0m11:14:22.338337 [info ] [Thread-1 (]: 5 of 13 SKIP relation main_gold.dim_location ................................... [[33mSKIP[0m]
[0m11:14:22.338983 [info ] [Thread-4 (]: 4 of 13 SKIP relation main_gold.dim_company .................................... [[33mSKIP[0m]
[0m11:14:22.339825 [debug] [Thread-3 (]: Finished running node model.job_intelligent.int_skills_extraction
[0m11:14:22.340363 [debug] [Thread-1 (]: Finished running node model.job_intelligent.dim_location
[0m11:14:22.340985 [debug] [Thread-4 (]: Finished running node model.job_intelligent.dim_company
[0m11:14:22.342108 [debug] [Thread-2 (]: Began running node model.job_intelligent.dim_skills
[0m11:14:22.343217 [debug] [Thread-3 (]: Began running node model.job_intelligent.fact_job_offers
[0m11:14:22.342767 [info ] [Thread-2 (]: 8 of 13 SKIP relation main_gold.dim_skills ..................................... [[33mSKIP[0m]
[0m11:14:22.344302 [debug] [Thread-2 (]: Finished running node model.job_intelligent.dim_skills
[0m11:14:22.343692 [info ] [Thread-3 (]: 9 of 13 SKIP relation main_gold.fact_job_offers ................................ [[33mSKIP[0m]
[0m11:14:22.345091 [debug] [Thread-3 (]: Finished running node model.job_intelligent.fact_job_offers
[0m11:14:22.346053 [debug] [Thread-4 (]: Began running node model.job_intelligent.agg_location_analysis
[0m11:14:22.346688 [debug] [Thread-1 (]: Began running node model.job_intelligent.agg_job_offers_by_category_time
[0m11:14:22.347724 [debug] [Thread-2 (]: Began running node model.job_intelligent.fact_job_skills
[0m11:14:22.347285 [info ] [Thread-4 (]: 11 of 13 SKIP relation main_gold.agg_location_analysis ......................... [[33mSKIP[0m]
[0m11:14:22.348398 [info ] [Thread-1 (]: 10 of 13 SKIP relation main_gold.agg_job_offers_by_category_time ............... [[33mSKIP[0m]
[0m11:14:22.349108 [info ] [Thread-2 (]: 12 of 13 SKIP relation main_gold.fact_job_skills ............................... [[33mSKIP[0m]
[0m11:14:22.350057 [debug] [Thread-4 (]: Finished running node model.job_intelligent.agg_location_analysis
[0m11:14:22.350863 [debug] [Thread-1 (]: Finished running node model.job_intelligent.agg_job_offers_by_category_time
[0m11:14:22.351442 [debug] [Thread-2 (]: Finished running node model.job_intelligent.fact_job_skills
[0m11:14:22.352624 [debug] [Thread-3 (]: Began running node model.job_intelligent.agg_skills_demand
[0m11:14:22.353133 [info ] [Thread-3 (]: 13 of 13 SKIP relation main_gold.agg_skills_demand ............................. [[33mSKIP[0m]
[0m11:14:22.353711 [debug] [Thread-3 (]: Finished running node model.job_intelligent.agg_skills_demand
[0m11:14:22.355657 [debug] [MainThread]: Using duckdb connection "master"
[0m11:14:22.356077 [debug] [MainThread]: On master: BEGIN
[0m11:14:22.356420 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m11:14:22.357158 [debug] [MainThread]: SQL status: OK in 0.001 seconds
[0m11:14:22.357536 [debug] [MainThread]: On master: COMMIT
[0m11:14:22.357889 [debug] [MainThread]: Using duckdb connection "master"
[0m11:14:22.358247 [debug] [MainThread]: On master: COMMIT
[0m11:14:22.358896 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m11:14:22.359277 [debug] [MainThread]: On master: Close
[0m11:14:22.359825 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:14:22.360168 [debug] [MainThread]: Connection 'create_memory_main_silver' was properly closed.
[0m11:14:22.360472 [debug] [MainThread]: Connection 'create_memory_main_bronze' was properly closed.
[0m11:14:22.360795 [debug] [MainThread]: Connection 'create_memory_main_gold' was properly closed.
[0m11:14:22.361090 [debug] [MainThread]: Connection 'list_memory_main_bronze' was properly closed.
[0m11:14:22.361383 [debug] [MainThread]: Connection 'list_memory_main_gold' was properly closed.
[0m11:14:22.361699 [debug] [MainThread]: Connection 'list_memory_main_silver' was properly closed.
[0m11:14:22.361995 [debug] [MainThread]: Connection 'model.job_intelligent.stg_jobs_raw' was properly closed.
[0m11:14:22.362542 [info ] [MainThread]: 
[0m11:14:22.363076 [info ] [MainThread]: Finished running 12 table models, 1 view model in 0 hours 0 minutes and 0.59 seconds (0.59s).
[0m11:14:22.364236 [debug] [MainThread]: Command end result
[0m11:14:22.391572 [debug] [MainThread]: Wrote artifact WritableManifest to D:\lab2\dbt_project\target\manifest.json
[0m11:14:22.395007 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\lab2\dbt_project\target\semantic_manifest.json
[0m11:14:22.404480 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\lab2\dbt_project\target\run_results.json
[0m11:14:22.405058 [info ] [MainThread]: 
[0m11:14:22.405527 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m11:14:22.405940 [info ] [MainThread]: 
[0m11:14:22.406405 [error] [MainThread]: [31mFailure in model stg_jobs_raw (models\bronze\stg_jobs_raw.sql)[0m
[0m11:14:22.406869 [error] [MainThread]:   Runtime Error in model stg_jobs_raw (models\bronze\stg_jobs_raw.sql)
  IO Error: No files found that match the pattern "../data/final_data.csv"
  
  LINE 29: FROM read_csv_auto('../data/final_data.csv')
                ^
[0m11:14:22.407245 [info ] [MainThread]: 
[0m11:14:22.407707 [info ] [MainThread]:   compiled code at target\compiled\job_intelligent\models\bronze\stg_jobs_raw.sql
[0m11:14:22.408245 [info ] [MainThread]: 
[0m11:14:22.408802 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=12 NO-OP=0 TOTAL=13
[0m11:14:22.410352 [debug] [MainThread]: Command `dbt run` failed at 11:14:22.410155 after 6.09 seconds
[0m11:14:22.410833 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001556912D4F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000155690C2180>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015568E676D0>]}
[0m11:14:22.411244 [debug] [MainThread]: Flushing usage events
[0m11:14:24.223827 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:16:29.409023 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FE56025FD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FE53D20190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FE53B97890>]}


============================== 11:16:29.413355 | 8dc2ea0d-b026-4d18-a960-a901760b7445 ==============================
[0m11:16:29.413355 [info ] [MainThread]: Running with dbt=1.10.13
[0m11:16:29.413838 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'use_experimental_parser': 'False', 'cache_selected_only': 'False', 'warn_error': 'None', 'profiles_dir': 'D:\\lab2\\dbt_project', 'debug': 'True', 'static_parser': 'True', 'log_cache_events': 'False', 'empty': 'False', 'log_format': 'default', 'quiet': 'False', 'use_colors': 'True', 'partial_parse': 'True', 'version_check': 'True', 'log_path': 'D:\\lab2\\dbt_project\\logs', 'introspect': 'True', 'invocation_command': 'dbt run --debug', 'fail_fast': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'write_json': 'True', 'target_path': 'None'}
[0m11:16:29.751827 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '8dc2ea0d-b026-4d18-a960-a901760b7445', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FE564DF230>]}
[0m11:16:29.874199 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '8dc2ea0d-b026-4d18-a960-a901760b7445', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FE56098380>]}
[0m11:16:29.878727 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m11:16:30.161312 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m11:16:30.334218 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m11:16:30.334837 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m11:16:30.342966 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- seeds
[0m11:16:30.377724 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '8dc2ea0d-b026-4d18-a960-a901760b7445', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FE58374850>]}
[0m11:16:30.467930 [debug] [MainThread]: Wrote artifact WritableManifest to D:\lab2\dbt_project\target\manifest.json
[0m11:16:30.470541 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\lab2\dbt_project\target\semantic_manifest.json
[0m11:16:30.504395 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '8dc2ea0d-b026-4d18-a960-a901760b7445', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FE5701F7A0>]}
[0m11:16:30.505027 [info ] [MainThread]: Found 13 models, 456 macros
[0m11:16:30.505433 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8dc2ea0d-b026-4d18-a960-a901760b7445', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FE589252B0>]}
[0m11:16:30.507676 [info ] [MainThread]: 
[0m11:16:30.508090 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m11:16:30.508462 [info ] [MainThread]: 
[0m11:16:30.509008 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m11:16:30.544230 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_memory'
[0m11:16:30.578342 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_memory'
[0m11:16:30.579546 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_memory'
[0m11:16:30.717646 [debug] [ThreadPool]: Using duckdb connection "list_memory"
[0m11:16:30.718454 [debug] [ThreadPool]: Using duckdb connection "list_memory"
[0m11:16:30.719049 [debug] [ThreadPool]: Using duckdb connection "list_memory"
[0m11:16:30.719697 [debug] [ThreadPool]: On list_memory: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_memory"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"memory"'
    
  
  
[0m11:16:30.720437 [debug] [ThreadPool]: On list_memory: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_memory"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"memory"'
    
  
  
[0m11:16:30.721128 [debug] [ThreadPool]: On list_memory: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_memory"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"memory"'
    
  
  
[0m11:16:30.721736 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:16:30.722488 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:16:30.723037 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:16:30.752376 [debug] [ThreadPool]: SQL status: OK in 0.029 seconds
[0m11:16:30.753187 [debug] [ThreadPool]: SQL status: OK in 0.031 seconds
[0m11:16:30.755474 [debug] [ThreadPool]: On list_memory: Close
[0m11:16:30.756170 [debug] [ThreadPool]: SQL status: OK in 0.034 seconds
[0m11:16:30.758439 [debug] [ThreadPool]: On list_memory: Close
[0m11:16:30.760982 [debug] [ThreadPool]: On list_memory: Close
[0m11:16:30.763397 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_memory, now create_memory_main_silver)
[0m11:16:30.764275 [debug] [ThreadPool]: Creating schema "database: "memory"
schema: "main_silver"
"
[0m11:16:30.774555 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_silver"
[0m11:16:30.775441 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_memory, now create_memory_main_bronze)
[0m11:16:30.776142 [debug] [ThreadPool]: On create_memory_main_silver: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_memory_main_silver"} */

    
        select type from duckdb_databases()
        where lower(database_name)='memory'
        and type='sqlite'
    
  
[0m11:16:30.762665 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_memory, now create_memory_main_gold)
[0m11:16:30.777197 [debug] [ThreadPool]: Creating schema "database: "memory"
schema: "main_bronze"
"
[0m11:16:30.778017 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:16:30.778892 [debug] [ThreadPool]: Creating schema "database: "memory"
schema: "main_gold"
"
[0m11:16:30.783386 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_bronze"
[0m11:16:30.786877 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_gold"
[0m11:16:30.787706 [debug] [ThreadPool]: SQL status: OK in 0.010 seconds
[0m11:16:30.788434 [debug] [ThreadPool]: On create_memory_main_bronze: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_memory_main_bronze"} */

    
        select type from duckdb_databases()
        where lower(database_name)='memory'
        and type='sqlite'
    
  
[0m11:16:30.789325 [debug] [ThreadPool]: On create_memory_main_gold: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_memory_main_gold"} */

    
        select type from duckdb_databases()
        where lower(database_name)='memory'
        and type='sqlite'
    
  
[0m11:16:30.791654 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_silver"
[0m11:16:30.792305 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:16:30.793006 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:16:30.793719 [debug] [ThreadPool]: On create_memory_main_silver: BEGIN
[0m11:16:30.795541 [debug] [ThreadPool]: SQL status: OK in 0.003 seconds
[0m11:16:30.796442 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m11:16:30.797085 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_silver"
[0m11:16:30.797695 [debug] [ThreadPool]: On create_memory_main_silver: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_memory_main_silver"} */

    
    
        create schema if not exists "memory"."main_silver"
    
[0m11:16:30.798552 [debug] [ThreadPool]: SQL status: OK in 0.006 seconds
[0m11:16:30.801304 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_bronze"
[0m11:16:30.802041 [debug] [ThreadPool]: SQL status: OK in 0.004 seconds
[0m11:16:30.804202 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_gold"
[0m11:16:30.804862 [debug] [ThreadPool]: On create_memory_main_bronze: BEGIN
[0m11:16:30.806455 [debug] [ThreadPool]: On create_memory_main_silver: COMMIT
[0m11:16:30.807161 [debug] [ThreadPool]: On create_memory_main_gold: BEGIN
[0m11:16:30.808144 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_silver"
[0m11:16:30.809058 [debug] [ThreadPool]: On create_memory_main_silver: COMMIT
[0m11:16:30.809826 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m11:16:30.810474 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_gold"
[0m11:16:30.811088 [debug] [ThreadPool]: On create_memory_main_gold: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_memory_main_gold"} */

    
    
        create schema if not exists "memory"."main_gold"
    
[0m11:16:30.811650 [debug] [ThreadPool]: SQL status: OK in 0.002 seconds
[0m11:16:30.812155 [debug] [ThreadPool]: SQL status: OK in 0.004 seconds
[0m11:16:30.813411 [debug] [ThreadPool]: On create_memory_main_silver: Close
[0m11:16:30.814175 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m11:16:30.815318 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_bronze"
[0m11:16:30.817692 [debug] [ThreadPool]: On create_memory_main_gold: COMMIT
[0m11:16:30.818655 [debug] [ThreadPool]: On create_memory_main_bronze: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_memory_main_bronze"} */

    
    
        create schema if not exists "memory"."main_bronze"
    
[0m11:16:30.819450 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_gold"
[0m11:16:30.820446 [debug] [ThreadPool]: On create_memory_main_gold: COMMIT
[0m11:16:30.821382 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m11:16:30.822921 [debug] [ThreadPool]: On create_memory_main_bronze: COMMIT
[0m11:16:30.823570 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_bronze"
[0m11:16:30.824163 [debug] [ThreadPool]: On create_memory_main_bronze: COMMIT
[0m11:16:30.825009 [debug] [ThreadPool]: SQL status: OK in 0.004 seconds
[0m11:16:30.825673 [debug] [ThreadPool]: On create_memory_main_gold: Close
[0m11:16:30.826325 [debug] [ThreadPool]: SQL status: OK in 0.002 seconds
[0m11:16:30.826956 [debug] [ThreadPool]: On create_memory_main_bronze: Close
[0m11:16:30.831064 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_memory_main_bronze'
[0m11:16:30.839978 [debug] [ThreadPool]: Using duckdb connection "list_memory_main_bronze"
[0m11:16:30.840901 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_memory_main_silver'
[0m11:16:30.841782 [debug] [ThreadPool]: On list_memory_main_bronze: BEGIN
[0m11:16:30.842583 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_memory_main_gold'
[0m11:16:30.845973 [debug] [ThreadPool]: Using duckdb connection "list_memory_main_silver"
[0m11:16:30.846704 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:16:30.849350 [debug] [ThreadPool]: Using duckdb connection "list_memory_main_gold"
[0m11:16:30.850266 [debug] [ThreadPool]: On list_memory_main_silver: BEGIN
[0m11:16:30.851435 [debug] [ThreadPool]: On list_memory_main_gold: BEGIN
[0m11:16:30.852156 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:16:30.852855 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:16:30.853803 [debug] [ThreadPool]: SQL status: OK in 0.007 seconds
[0m11:16:30.854804 [debug] [ThreadPool]: SQL status: OK in 0.003 seconds
[0m11:16:30.855583 [debug] [ThreadPool]: SQL status: OK in 0.003 seconds
[0m11:16:30.856241 [debug] [ThreadPool]: Using duckdb connection "list_memory_main_bronze"
[0m11:16:30.856938 [debug] [ThreadPool]: Using duckdb connection "list_memory_main_silver"
[0m11:16:30.857644 [debug] [ThreadPool]: Using duckdb connection "list_memory_main_gold"
[0m11:16:30.858369 [debug] [ThreadPool]: On list_memory_main_bronze: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_memory_main_bronze"} */
select
      'memory' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main_bronze'
    and lower(table_catalog) = 'memory'
  
[0m11:16:30.859178 [debug] [ThreadPool]: On list_memory_main_silver: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_memory_main_silver"} */
select
      'memory' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main_silver'
    and lower(table_catalog) = 'memory'
  
[0m11:16:30.859948 [debug] [ThreadPool]: On list_memory_main_gold: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_memory_main_gold"} */
select
      'memory' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main_gold'
    and lower(table_catalog) = 'memory'
  
[0m11:16:30.886616 [debug] [ThreadPool]: SQL status: OK in 0.026 seconds
[0m11:16:30.887351 [debug] [ThreadPool]: SQL status: OK in 0.026 seconds
[0m11:16:30.887853 [debug] [ThreadPool]: SQL status: OK in 0.027 seconds
[0m11:16:30.889144 [debug] [ThreadPool]: On list_memory_main_bronze: ROLLBACK
[0m11:16:30.890436 [debug] [ThreadPool]: On list_memory_main_silver: ROLLBACK
[0m11:16:30.891861 [debug] [ThreadPool]: On list_memory_main_gold: ROLLBACK
[0m11:16:30.894095 [debug] [ThreadPool]: Failed to rollback 'list_memory_main_bronze'
[0m11:16:30.894623 [debug] [ThreadPool]: On list_memory_main_bronze: Close
[0m11:16:30.895989 [debug] [ThreadPool]: Failed to rollback 'list_memory_main_gold'
[0m11:16:30.896378 [debug] [ThreadPool]: On list_memory_main_gold: Close
[0m11:16:30.897472 [debug] [ThreadPool]: Failed to rollback 'list_memory_main_silver'
[0m11:16:30.897874 [debug] [ThreadPool]: On list_memory_main_silver: Close
[0m11:16:30.898908 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8dc2ea0d-b026-4d18-a960-a901760b7445', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FE58B13A00>]}
[0m11:16:30.900293 [debug] [MainThread]: Using duckdb connection "master"
[0m11:16:30.901170 [debug] [MainThread]: On master: BEGIN
[0m11:16:30.901723 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:16:30.902511 [debug] [MainThread]: SQL status: OK in 0.001 seconds
[0m11:16:30.902948 [debug] [MainThread]: On master: COMMIT
[0m11:16:30.903348 [debug] [MainThread]: Using duckdb connection "master"
[0m11:16:30.903730 [debug] [MainThread]: On master: COMMIT
[0m11:16:30.904326 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m11:16:30.904740 [debug] [MainThread]: On master: Close
[0m11:16:30.910545 [debug] [Thread-1 (]: Began running node model.job_intelligent.stg_jobs_raw
[0m11:16:30.911244 [info ] [Thread-1 (]: 1 of 13 START sql view model main_bronze.stg_jobs_raw .......................... [RUN]
[0m11:16:30.911861 [debug] [Thread-1 (]: Acquiring new duckdb connection 'model.job_intelligent.stg_jobs_raw'
[0m11:16:30.912341 [debug] [Thread-1 (]: Began compiling node model.job_intelligent.stg_jobs_raw
[0m11:16:30.921813 [debug] [Thread-1 (]: Writing injected SQL for node "model.job_intelligent.stg_jobs_raw"
[0m11:16:30.922809 [debug] [Thread-1 (]: Began executing node model.job_intelligent.stg_jobs_raw
[0m11:16:30.955085 [debug] [Thread-1 (]: Writing runtime sql for node "model.job_intelligent.stg_jobs_raw"
[0m11:16:30.956095 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.stg_jobs_raw"
[0m11:16:30.956573 [debug] [Thread-1 (]: On model.job_intelligent.stg_jobs_raw: BEGIN
[0m11:16:30.957002 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m11:16:30.958019 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m11:16:30.958487 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.stg_jobs_raw"
[0m11:16:30.958961 [debug] [Thread-1 (]: On model.job_intelligent.stg_jobs_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.stg_jobs_raw"} */

  
  create view "memory"."main_bronze"."stg_jobs_raw__dbt_tmp" as (
    -- models/bronze/stg_jobs_raw.sql
-- Bronze layer: Lecture directe des données brutes depuis final_data.csv
-- Renommage et typage minimal



-- Read directly from CSV file
SELECT
    -- Renommer les colonnes pour cohérence
    title as job_title,
    location,
    postedTime as posted_time,
    publishedAt as published_at,
    jobUrl as job_url,
    companyName as company_name,
    companyUrl as company_url,
    description as job_description,
    contractType as contract_type,
    workType as work_type,
    
    -- Métadonnées de traçabilité
    CURRENT_TIMESTAMP() as ingestion_timestamp,
    '2026-01-08 10:16:29.253534+00:00' as dbt_run_id

FROM read_csv_auto('../data/final_data.csv')

WHERE 1=1
    -- Filtrer les lignes vides
    AND title IS NOT NULL
    AND description IS NOT NULL
  );

[0m11:16:30.960425 [debug] [Thread-1 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.stg_jobs_raw"} */

  
  create view "memory"."main_bronze"."stg_jobs_raw__dbt_tmp" as (
    -- models/bronze/stg_jobs_raw.sql
-- Bronze layer: Lecture directe des données brutes depuis final_data.csv
-- Renommage et typage minimal



-- Read directly from CSV file
SELECT
    -- Renommer les colonnes pour cohérence
    title as job_title,
    location,
    postedTime as posted_time,
    publishedAt as published_at,
    jobUrl as job_url,
    companyName as company_name,
    companyUrl as company_url,
    description as job_description,
    contractType as contract_type,
    workType as work_type,
    
    -- Métadonnées de traçabilité
    CURRENT_TIMESTAMP() as ingestion_timestamp,
    '2026-01-08 10:16:29.253534+00:00' as dbt_run_id

FROM read_csv_auto('../data/final_data.csv')

WHERE 1=1
    -- Filtrer les lignes vides
    AND title IS NOT NULL
    AND description IS NOT NULL
  );

[0m11:16:30.960983 [debug] [Thread-1 (]: DuckDB adapter: Rolling back transaction.
[0m11:16:30.961529 [debug] [Thread-1 (]: On model.job_intelligent.stg_jobs_raw: ROLLBACK
[0m11:16:30.969357 [debug] [Thread-1 (]: Failed to rollback 'model.job_intelligent.stg_jobs_raw'
[0m11:16:30.969878 [debug] [Thread-1 (]: On model.job_intelligent.stg_jobs_raw: Close
[0m11:16:30.974324 [debug] [Thread-1 (]: Runtime Error in model stg_jobs_raw (models\bronze\stg_jobs_raw.sql)
  IO Error: No files found that match the pattern "../data/final_data.csv"
  
  LINE 29: FROM read_csv_auto('../data/final_data.csv')
                ^
[0m11:16:30.976601 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8dc2ea0d-b026-4d18-a960-a901760b7445', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FE5839DA90>]}
[0m11:16:30.977314 [error] [Thread-1 (]: 1 of 13 ERROR creating sql view model main_bronze.stg_jobs_raw ................. [[31mERROR[0m in 0.06s]
[0m11:16:30.978026 [debug] [Thread-1 (]: Finished running node model.job_intelligent.stg_jobs_raw
[0m11:16:30.978849 [debug] [Thread-7 (]: Marking all children of 'model.job_intelligent.stg_jobs_raw' to be skipped because of status 'error'.  Reason: Runtime Error in model stg_jobs_raw (models\bronze\stg_jobs_raw.sql)
  IO Error: No files found that match the pattern "../data/final_data.csv"
  
  LINE 29: FROM read_csv_auto('../data/final_data.csv')
                ^.
[0m11:16:30.980289 [debug] [Thread-2 (]: Began running node model.job_intelligent.int_jobs_cleaned
[0m11:16:30.980796 [info ] [Thread-2 (]: 2 of 13 SKIP relation main_silver.int_jobs_cleaned ............................. [[33mSKIP[0m]
[0m11:16:30.981329 [debug] [Thread-2 (]: Finished running node model.job_intelligent.int_jobs_cleaned
[0m11:16:30.982159 [debug] [Thread-3 (]: Began running node model.job_intelligent.int_job_title_normalization
[0m11:16:30.982793 [info ] [Thread-3 (]: 3 of 13 SKIP relation main_silver.int_job_title_normalization .................. [[33mSKIP[0m]
[0m11:16:30.984173 [debug] [Thread-3 (]: Finished running node model.job_intelligent.int_job_title_normalization
[0m11:16:30.985390 [debug] [Thread-4 (]: Began running node model.job_intelligent.dim_company
[0m11:16:30.985912 [info ] [Thread-4 (]: 4 of 13 SKIP relation main_gold.dim_company .................................... [[33mSKIP[0m]
[0m11:16:30.986480 [debug] [Thread-3 (]: Began running node model.job_intelligent.int_skills_extraction
[0m11:16:30.987067 [debug] [Thread-1 (]: Began running node model.job_intelligent.dim_location
[0m11:16:30.987686 [debug] [Thread-2 (]: Began running node model.job_intelligent.dim_time
[0m11:16:30.988301 [debug] [Thread-4 (]: Finished running node model.job_intelligent.dim_company
[0m11:16:30.988954 [info ] [Thread-3 (]: 7 of 13 SKIP relation main_silver.int_skills_extraction ........................ [[33mSKIP[0m]
[0m11:16:30.989556 [info ] [Thread-1 (]: 5 of 13 SKIP relation main_gold.dim_location ................................... [[33mSKIP[0m]
[0m11:16:30.990219 [info ] [Thread-2 (]: 6 of 13 SKIP relation main_gold.dim_time ....................................... [[33mSKIP[0m]
[0m11:16:30.991019 [debug] [Thread-3 (]: Finished running node model.job_intelligent.int_skills_extraction
[0m11:16:30.991703 [debug] [Thread-1 (]: Finished running node model.job_intelligent.dim_location
[0m11:16:30.992285 [debug] [Thread-2 (]: Finished running node model.job_intelligent.dim_time
[0m11:16:30.993416 [debug] [Thread-3 (]: Began running node model.job_intelligent.fact_job_offers
[0m11:16:30.993977 [info ] [Thread-3 (]: 9 of 13 SKIP relation main_gold.fact_job_offers ................................ [[33mSKIP[0m]
[0m11:16:30.994452 [debug] [Thread-4 (]: Began running node model.job_intelligent.dim_skills
[0m11:16:30.994985 [debug] [Thread-3 (]: Finished running node model.job_intelligent.fact_job_offers
[0m11:16:30.995570 [info ] [Thread-4 (]: 8 of 13 SKIP relation main_gold.dim_skills ..................................... [[33mSKIP[0m]
[0m11:16:30.996354 [debug] [Thread-4 (]: Finished running node model.job_intelligent.dim_skills
[0m11:16:30.997200 [debug] [Thread-2 (]: Began running node model.job_intelligent.agg_location_analysis
[0m11:16:30.997667 [debug] [Thread-1 (]: Began running node model.job_intelligent.agg_job_offers_by_category_time
[0m11:16:30.998144 [debug] [Thread-3 (]: Began running node model.job_intelligent.fact_job_skills
[0m11:16:30.998720 [info ] [Thread-2 (]: 11 of 13 SKIP relation main_gold.agg_location_analysis ......................... [[33mSKIP[0m]
[0m11:16:30.999508 [info ] [Thread-1 (]: 10 of 13 SKIP relation main_gold.agg_job_offers_by_category_time ............... [[33mSKIP[0m]
[0m11:16:31.000366 [info ] [Thread-3 (]: 12 of 13 SKIP relation main_gold.fact_job_skills ............................... [[33mSKIP[0m]
[0m11:16:31.001138 [debug] [Thread-2 (]: Finished running node model.job_intelligent.agg_location_analysis
[0m11:16:31.001825 [debug] [Thread-1 (]: Finished running node model.job_intelligent.agg_job_offers_by_category_time
[0m11:16:31.002526 [debug] [Thread-3 (]: Finished running node model.job_intelligent.fact_job_skills
[0m11:16:31.003722 [debug] [Thread-4 (]: Began running node model.job_intelligent.agg_skills_demand
[0m11:16:31.004277 [info ] [Thread-4 (]: 13 of 13 SKIP relation main_gold.agg_skills_demand ............................. [[33mSKIP[0m]
[0m11:16:31.004835 [debug] [Thread-4 (]: Finished running node model.job_intelligent.agg_skills_demand
[0m11:16:31.006562 [debug] [MainThread]: Using duckdb connection "master"
[0m11:16:31.007028 [debug] [MainThread]: On master: BEGIN
[0m11:16:31.007436 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m11:16:31.008236 [debug] [MainThread]: SQL status: OK in 0.001 seconds
[0m11:16:31.008669 [debug] [MainThread]: On master: COMMIT
[0m11:16:31.009090 [debug] [MainThread]: Using duckdb connection "master"
[0m11:16:31.009511 [debug] [MainThread]: On master: COMMIT
[0m11:16:31.010156 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m11:16:31.010626 [debug] [MainThread]: On master: Close
[0m11:16:31.011148 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:16:31.011544 [debug] [MainThread]: Connection 'create_memory_main_gold' was properly closed.
[0m11:16:31.011936 [debug] [MainThread]: Connection 'create_memory_main_silver' was properly closed.
[0m11:16:31.012312 [debug] [MainThread]: Connection 'create_memory_main_bronze' was properly closed.
[0m11:16:31.012686 [debug] [MainThread]: Connection 'list_memory_main_bronze' was properly closed.
[0m11:16:31.013067 [debug] [MainThread]: Connection 'list_memory_main_silver' was properly closed.
[0m11:16:31.013448 [debug] [MainThread]: Connection 'list_memory_main_gold' was properly closed.
[0m11:16:31.013819 [debug] [MainThread]: Connection 'model.job_intelligent.stg_jobs_raw' was properly closed.
[0m11:16:31.014343 [info ] [MainThread]: 
[0m11:16:31.014754 [info ] [MainThread]: Finished running 12 table models, 1 view model in 0 hours 0 minutes and 0.51 seconds (0.51s).
[0m11:16:31.016024 [debug] [MainThread]: Command end result
[0m11:16:31.038945 [debug] [MainThread]: Wrote artifact WritableManifest to D:\lab2\dbt_project\target\manifest.json
[0m11:16:31.041022 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\lab2\dbt_project\target\semantic_manifest.json
[0m11:16:31.046584 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\lab2\dbt_project\target\run_results.json
[0m11:16:31.046998 [info ] [MainThread]: 
[0m11:16:31.047326 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m11:16:31.047640 [info ] [MainThread]: 
[0m11:16:31.048016 [error] [MainThread]: [31mFailure in model stg_jobs_raw (models\bronze\stg_jobs_raw.sql)[0m
[0m11:16:31.048387 [error] [MainThread]:   Runtime Error in model stg_jobs_raw (models\bronze\stg_jobs_raw.sql)
  IO Error: No files found that match the pattern "../data/final_data.csv"
  
  LINE 29: FROM read_csv_auto('../data/final_data.csv')
                ^
[0m11:16:31.048682 [info ] [MainThread]: 
[0m11:16:31.049131 [info ] [MainThread]:   compiled code at target\compiled\job_intelligent\models\bronze\stg_jobs_raw.sql
[0m11:16:31.049567 [info ] [MainThread]: 
[0m11:16:31.050262 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=12 NO-OP=0 TOTAL=13
[0m11:16:31.051307 [debug] [MainThread]: Command `dbt run` failed at 11:16:31.051194 after 1.88 seconds
[0m11:16:31.051715 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FE5893AF10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FE58404190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FE58404A50>]}
[0m11:16:31.052109 [debug] [MainThread]: Flushing usage events
[0m11:16:32.385369 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:16:52.332762 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B2294D9FD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B2271F0190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B227067890>]}


============================== 11:16:52.341641 | 413a10e5-c308-4fda-bebf-bda431df127f ==============================
[0m11:16:52.341641 [info ] [MainThread]: Running with dbt=1.10.13
[0m11:16:52.342749 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'use_experimental_parser': 'False', 'cache_selected_only': 'False', 'warn_error': 'None', 'profiles_dir': 'D:\\lab2\\dbt_project', 'debug': 'False', 'static_parser': 'True', 'log_cache_events': 'False', 'empty': 'None', 'quiet': 'False', 'log_format': 'default', 'use_colors': 'True', 'partial_parse': 'True', 'version_check': 'True', 'log_path': 'D:\\lab2\\dbt_project\\logs', 'introspect': 'True', 'no_print': 'None', 'fail_fast': 'False', 'invocation_command': 'dbt debug', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'target_path': 'None', 'write_json': 'True'}
[0m11:16:52.383180 [info ] [MainThread]: dbt version: 1.10.13
[0m11:16:52.383875 [info ] [MainThread]: python version: 3.13.2
[0m11:16:52.384425 [info ] [MainThread]: python path: C:\Users\HP\AppData\Local\Programs\Python\Python313\python.exe
[0m11:16:52.385045 [info ] [MainThread]: os info: Windows-11-10.0.26200-SP0
[0m11:16:52.528140 [info ] [MainThread]: Using profiles dir at D:\lab2\dbt_project
[0m11:16:52.528600 [info ] [MainThread]: Using profiles.yml file at D:\lab2\dbt_project\profiles.yml
[0m11:16:52.528952 [info ] [MainThread]: Using dbt_project.yml file at D:\lab2\dbt_project\dbt_project.yml
[0m11:16:52.531644 [info ] [MainThread]: adapter type: duckdb
[0m11:16:52.531996 [info ] [MainThread]: adapter version: 1.10.0
[0m11:16:52.681214 [info ] [MainThread]: Configuration:
[0m11:16:52.681645 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m11:16:52.681972 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m11:16:52.682288 [info ] [MainThread]: Required dependencies:
[0m11:16:52.682621 [debug] [MainThread]: Executing "git --help"
[0m11:16:52.720529 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--no-lazy-fetch]\n           [--no-optional-locks] [--no-advice] [--bare] [--git-dir=<path>]\n           [--work-tree=<path>] [--namespace=<name>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone      Clone a repository into a new directory\n   init       Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add        Add file contents to the index\n   mv         Move or rename a file, a directory, or a symlink\n   restore    Restore working tree files\n   rm         Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect     Use binary search to find the commit that introduced a bug\n   diff       Show changes between commits, commit and working tree, etc\n   grep       Print lines matching a pattern\n   log        Show commit logs\n   show       Show various types of objects\n   status     Show the working tree status\n\ngrow, mark and tweak your common history\n   backfill   Download missing objects in a partial clone\n   branch     List, create, or delete branches\n   commit     Record changes to the repository\n   merge      Join two or more development histories together\n   rebase     Reapply commits on top of another base tip\n   reset      Reset current HEAD to the specified state\n   switch     Switch branches\n   tag        Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch      Download objects and refs from another repository\n   pull       Fetch from and integrate with another repository or a local branch\n   push       Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m11:16:52.720998 [debug] [MainThread]: STDERR: "b''"
[0m11:16:52.721518 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m11:16:52.721970 [info ] [MainThread]: Connection:
[0m11:16:52.722309 [info ] [MainThread]:   database: memory
[0m11:16:52.722616 [info ] [MainThread]:   schema: main
[0m11:16:52.722933 [info ] [MainThread]:   path: :memory:
[0m11:16:52.723232 [info ] [MainThread]:   config_options: None
[0m11:16:52.723528 [info ] [MainThread]:   extensions: None
[0m11:16:52.723825 [info ] [MainThread]:   settings: {}
[0m11:16:52.724121 [info ] [MainThread]:   external_root: .
[0m11:16:52.724413 [info ] [MainThread]:   use_credential_provider: None
[0m11:16:52.724710 [info ] [MainThread]:   attach: None
[0m11:16:52.725013 [info ] [MainThread]:   filesystems: None
[0m11:16:52.725352 [info ] [MainThread]:   remote: None
[0m11:16:52.725656 [info ] [MainThread]:   plugins: None
[0m11:16:52.725951 [info ] [MainThread]:   disable_transactions: False
[0m11:16:52.726452 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m11:16:53.009177 [debug] [MainThread]: Acquiring new duckdb connection 'debug'
[0m11:16:53.122494 [debug] [MainThread]: Using duckdb connection "debug"
[0m11:16:53.122883 [debug] [MainThread]: On debug: select 1 as id
[0m11:16:53.123151 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:16:53.137497 [debug] [MainThread]: SQL status: OK in 0.014 seconds
[0m11:16:53.138954 [debug] [MainThread]: On debug: Close
[0m11:16:53.139361 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m11:16:53.139687 [info ] [MainThread]: [32mAll checks passed![0m
[0m11:16:53.140505 [debug] [MainThread]: Command `dbt debug` succeeded at 11:16:53.140402 after 1.01 seconds
[0m11:16:53.140785 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m11:16:53.141093 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B22B9E1F30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B22BA8D5B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B22B960E20>]}
[0m11:16:53.141439 [debug] [MainThread]: Flushing usage events
[0m11:16:54.300504 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:16:57.750686 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020819659FD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020817364190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000208171D7890>]}


============================== 11:16:57.761579 | 9e6e83d9-0436-4f2b-bbc1-bb553101b864 ==============================
[0m11:16:57.761579 [info ] [MainThread]: Running with dbt=1.10.13
[0m11:16:57.763124 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'write_json': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'empty': 'False', 'debug': 'False', 'static_parser': 'True', 'log_cache_events': 'False', 'profiles_dir': 'D:\\lab2\\dbt_project', 'quiet': 'False', 'log_format': 'default', 'use_colors': 'True', 'partial_parse': 'True', 'version_check': 'True', 'log_path': 'D:\\lab2\\dbt_project\\logs', 'introspect': 'True', 'invocation_command': 'dbt run', 'fail_fast': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'no_print': 'None', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'target_path': 'None', 'use_experimental_parser': 'False'}
[0m11:16:58.181907 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '9e6e83d9-0436-4f2b-bbc1-bb553101b864', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020819B0F230>]}
[0m11:16:58.260919 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '9e6e83d9-0436-4f2b-bbc1-bb553101b864', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000208196C8380>]}
[0m11:16:58.263713 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m11:16:58.500232 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m11:16:58.628816 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m11:16:58.629427 [debug] [MainThread]: Partial parsing: updated file: job_intelligent://models\bronze\stg_jobs_raw.sql
[0m11:16:58.978940 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- seeds
[0m11:16:58.984177 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '9e6e83d9-0436-4f2b-bbc1-bb553101b864', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002081A984E50>]}
[0m11:16:59.046361 [debug] [MainThread]: Wrote artifact WritableManifest to D:\lab2\dbt_project\target\manifest.json
[0m11:16:59.048750 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\lab2\dbt_project\target\semantic_manifest.json
[0m11:16:59.088023 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '9e6e83d9-0436-4f2b-bbc1-bb553101b864', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002081A64B7A0>]}
[0m11:16:59.088709 [info ] [MainThread]: Found 13 models, 456 macros
[0m11:16:59.089238 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9e6e83d9-0436-4f2b-bbc1-bb553101b864', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002081BA819B0>]}
[0m11:16:59.092019 [info ] [MainThread]: 
[0m11:16:59.092562 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m11:16:59.093017 [info ] [MainThread]: 
[0m11:16:59.093691 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m11:16:59.101064 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_memory'
[0m11:16:59.114823 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_memory'
[0m11:16:59.119581 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_memory'
[0m11:16:59.197162 [debug] [ThreadPool]: Using duckdb connection "list_memory"
[0m11:16:59.197589 [debug] [ThreadPool]: Using duckdb connection "list_memory"
[0m11:16:59.197944 [debug] [ThreadPool]: On list_memory: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_memory"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"memory"'
    
  
  
[0m11:16:59.198308 [debug] [ThreadPool]: Using duckdb connection "list_memory"
[0m11:16:59.198703 [debug] [ThreadPool]: On list_memory: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_memory"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"memory"'
    
  
  
[0m11:16:59.199081 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:16:59.199456 [debug] [ThreadPool]: On list_memory: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_memory"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"memory"'
    
  
  
[0m11:16:59.199822 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:16:59.200331 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:16:59.215996 [debug] [ThreadPool]: SQL status: OK in 0.017 seconds
[0m11:16:59.217353 [debug] [ThreadPool]: On list_memory: Close
[0m11:16:59.217742 [debug] [ThreadPool]: SQL status: OK in 0.018 seconds
[0m11:16:59.219155 [debug] [ThreadPool]: On list_memory: Close
[0m11:16:59.219778 [debug] [ThreadPool]: SQL status: OK in 0.019 seconds
[0m11:16:59.221041 [debug] [ThreadPool]: On list_memory: Close
[0m11:16:59.221724 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_memory, now create_memory_main_gold)
[0m11:16:59.222145 [debug] [ThreadPool]: Creating schema "database: "memory"
schema: "main_gold"
"
[0m11:16:59.227389 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_gold"
[0m11:16:59.227693 [debug] [ThreadPool]: On create_memory_main_gold: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_memory_main_gold"} */

    
        select type from duckdb_databases()
        where lower(database_name)='memory'
        and type='sqlite'
    
  
[0m11:16:59.228101 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_memory, now create_memory_main_silver)
[0m11:16:59.228502 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_memory, now create_memory_main_bronze)
[0m11:16:59.228869 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:16:59.229356 [debug] [ThreadPool]: Creating schema "database: "memory"
schema: "main_silver"
"
[0m11:16:59.229916 [debug] [ThreadPool]: Creating schema "database: "memory"
schema: "main_bronze"
"
[0m11:16:59.232511 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_silver"
[0m11:16:59.233101 [debug] [ThreadPool]: SQL status: OK in 0.004 seconds
[0m11:16:59.235907 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_bronze"
[0m11:16:59.236425 [debug] [ThreadPool]: On create_memory_main_silver: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_memory_main_silver"} */

    
        select type from duckdb_databases()
        where lower(database_name)='memory'
        and type='sqlite'
    
  
[0m11:16:59.236967 [debug] [ThreadPool]: On create_memory_main_bronze: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_memory_main_bronze"} */

    
        select type from duckdb_databases()
        where lower(database_name)='memory'
        and type='sqlite'
    
  
[0m11:16:59.238361 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_gold"
[0m11:16:59.238858 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:16:59.239277 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:16:59.239700 [debug] [ThreadPool]: On create_memory_main_gold: BEGIN
[0m11:16:59.240863 [debug] [ThreadPool]: SQL status: OK in 0.002 seconds
[0m11:16:59.241227 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m11:16:59.241529 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_gold"
[0m11:16:59.241816 [debug] [ThreadPool]: On create_memory_main_gold: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_memory_main_gold"} */

    
    
        create schema if not exists "memory"."main_gold"
    
[0m11:16:59.242240 [debug] [ThreadPool]: SQL status: OK in 0.003 seconds
[0m11:16:59.243483 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_silver"
[0m11:16:59.243804 [debug] [ThreadPool]: On create_memory_main_silver: BEGIN
[0m11:16:59.245004 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_bronze"
[0m11:16:59.245327 [debug] [ThreadPool]: On create_memory_main_bronze: BEGIN
[0m11:16:59.245744 [debug] [ThreadPool]: SQL status: OK in 0.004 seconds
[0m11:16:59.246559 [debug] [ThreadPool]: On create_memory_main_gold: COMMIT
[0m11:16:59.246864 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_gold"
[0m11:16:59.247146 [debug] [ThreadPool]: On create_memory_main_gold: COMMIT
[0m11:16:59.247590 [debug] [ThreadPool]: SQL status: OK in 0.003 seconds
[0m11:16:59.247933 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_silver"
[0m11:16:59.248586 [debug] [ThreadPool]: SQL status: OK in 0.003 seconds
[0m11:16:59.248895 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_bronze"
[0m11:16:59.249185 [debug] [ThreadPool]: On create_memory_main_bronze: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_memory_main_bronze"} */

    
    
        create schema if not exists "memory"."main_bronze"
    
[0m11:16:59.248245 [debug] [ThreadPool]: On create_memory_main_silver: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_memory_main_silver"} */

    
    
        create schema if not exists "memory"."main_silver"
    
[0m11:16:59.249969 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m11:16:59.250969 [debug] [ThreadPool]: On create_memory_main_bronze: COMMIT
[0m11:16:59.251318 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_bronze"
[0m11:16:59.251627 [debug] [ThreadPool]: On create_memory_main_bronze: COMMIT
[0m11:16:59.252079 [debug] [ThreadPool]: SQL status: OK in 0.005 seconds
[0m11:16:59.252455 [debug] [ThreadPool]: On create_memory_main_gold: Close
[0m11:16:59.253060 [debug] [ThreadPool]: SQL status: OK in 0.003 seconds
[0m11:16:59.253916 [debug] [ThreadPool]: On create_memory_main_silver: COMMIT
[0m11:16:59.254296 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_silver"
[0m11:16:59.254631 [debug] [ThreadPool]: On create_memory_main_silver: COMMIT
[0m11:16:59.255044 [debug] [ThreadPool]: SQL status: OK in 0.003 seconds
[0m11:16:59.255366 [debug] [ThreadPool]: On create_memory_main_bronze: Close
[0m11:16:59.255920 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m11:16:59.256268 [debug] [ThreadPool]: On create_memory_main_silver: Close
[0m11:16:59.258663 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_memory_main_gold'
[0m11:16:59.264358 [debug] [ThreadPool]: Using duckdb connection "list_memory_main_gold"
[0m11:16:59.264768 [debug] [ThreadPool]: On list_memory_main_gold: BEGIN
[0m11:16:59.265123 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:16:59.265850 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m11:16:59.266197 [debug] [ThreadPool]: Using duckdb connection "list_memory_main_gold"
[0m11:16:59.266537 [debug] [ThreadPool]: On list_memory_main_gold: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_memory_main_gold"} */
select
      'memory' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main_gold'
    and lower(table_catalog) = 'memory'
  
[0m11:16:59.267041 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_memory_main_silver'
[0m11:16:59.269597 [debug] [ThreadPool]: Using duckdb connection "list_memory_main_silver"
[0m11:16:59.269999 [debug] [ThreadPool]: On list_memory_main_silver: BEGIN
[0m11:16:59.270337 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:16:59.271013 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_memory_main_bronze'
[0m11:16:59.272667 [debug] [ThreadPool]: Using duckdb connection "list_memory_main_bronze"
[0m11:16:59.273025 [debug] [ThreadPool]: On list_memory_main_bronze: BEGIN
[0m11:16:59.273340 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:16:59.273984 [debug] [ThreadPool]: SQL status: OK in 0.004 seconds
[0m11:16:59.274330 [debug] [ThreadPool]: Using duckdb connection "list_memory_main_silver"
[0m11:16:59.274674 [debug] [ThreadPool]: On list_memory_main_silver: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_memory_main_silver"} */
select
      'memory' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main_silver'
    and lower(table_catalog) = 'memory'
  
[0m11:16:59.275059 [debug] [ThreadPool]: SQL status: OK in 0.002 seconds
[0m11:16:59.275640 [debug] [ThreadPool]: Using duckdb connection "list_memory_main_bronze"
[0m11:16:59.275994 [debug] [ThreadPool]: On list_memory_main_bronze: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_memory_main_bronze"} */
select
      'memory' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main_bronze'
    and lower(table_catalog) = 'memory'
  
[0m11:16:59.286760 [debug] [ThreadPool]: SQL status: OK in 0.020 seconds
[0m11:16:59.287300 [debug] [ThreadPool]: SQL status: OK in 0.012 seconds
[0m11:16:59.288773 [debug] [ThreadPool]: On list_memory_main_gold: ROLLBACK
[0m11:16:59.290062 [debug] [ThreadPool]: On list_memory_main_silver: ROLLBACK
[0m11:16:59.290599 [debug] [ThreadPool]: SQL status: OK in 0.014 seconds
[0m11:16:59.292015 [debug] [ThreadPool]: On list_memory_main_bronze: ROLLBACK
[0m11:16:59.293837 [debug] [ThreadPool]: Failed to rollback 'list_memory_main_silver'
[0m11:16:59.294192 [debug] [ThreadPool]: On list_memory_main_silver: Close
[0m11:16:59.295601 [debug] [ThreadPool]: Failed to rollback 'list_memory_main_bronze'
[0m11:16:59.295941 [debug] [ThreadPool]: On list_memory_main_bronze: Close
[0m11:16:59.297057 [debug] [ThreadPool]: Failed to rollback 'list_memory_main_gold'
[0m11:16:59.297403 [debug] [ThreadPool]: On list_memory_main_gold: Close
[0m11:16:59.298486 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9e6e83d9-0436-4f2b-bbc1-bb553101b864', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002081C1E2A90>]}
[0m11:16:59.298979 [debug] [MainThread]: Using duckdb connection "master"
[0m11:16:59.299327 [debug] [MainThread]: On master: BEGIN
[0m11:16:59.299683 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:16:59.300383 [debug] [MainThread]: SQL status: OK in 0.001 seconds
[0m11:16:59.300717 [debug] [MainThread]: On master: COMMIT
[0m11:16:59.301050 [debug] [MainThread]: Using duckdb connection "master"
[0m11:16:59.301373 [debug] [MainThread]: On master: COMMIT
[0m11:16:59.301925 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m11:16:59.302309 [debug] [MainThread]: On master: Close
[0m11:16:59.308369 [debug] [Thread-1 (]: Began running node model.job_intelligent.stg_jobs_raw
[0m11:16:59.308973 [info ] [Thread-1 (]: 1 of 13 START sql view model main_bronze.stg_jobs_raw .......................... [RUN]
[0m11:16:59.309582 [debug] [Thread-1 (]: Acquiring new duckdb connection 'model.job_intelligent.stg_jobs_raw'
[0m11:16:59.309984 [debug] [Thread-1 (]: Began compiling node model.job_intelligent.stg_jobs_raw
[0m11:16:59.318287 [debug] [Thread-1 (]: Writing injected SQL for node "model.job_intelligent.stg_jobs_raw"
[0m11:16:59.319187 [debug] [Thread-1 (]: Began executing node model.job_intelligent.stg_jobs_raw
[0m11:16:59.349692 [debug] [Thread-1 (]: Writing runtime sql for node "model.job_intelligent.stg_jobs_raw"
[0m11:16:59.350570 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.stg_jobs_raw"
[0m11:16:59.350975 [debug] [Thread-1 (]: On model.job_intelligent.stg_jobs_raw: BEGIN
[0m11:16:59.351288 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m11:16:59.352080 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m11:16:59.352407 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.stg_jobs_raw"
[0m11:16:59.352764 [debug] [Thread-1 (]: On model.job_intelligent.stg_jobs_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.stg_jobs_raw"} */

  
  create view "memory"."main_bronze"."stg_jobs_raw__dbt_tmp" as (
    -- models/bronze/stg_jobs_raw.sql
-- Bronze layer: Lecture directe des données brutes depuis final_data.csv
-- Renommage et typage minimal



-- Read directly from CSV file
SELECT
    -- Renommer les colonnes pour cohérence
    title as job_title,
    location,
    postedTime as posted_time,
    publishedAt as published_at,
    jobUrl as job_url,
    companyName as company_name,
    companyUrl as company_url,
    description as job_description,
    contractType as contract_type,
    workType as work_type,
    
    -- Métadonnées de traçabilité
    CURRENT_TIMESTAMP() as ingestion_timestamp,
    '2026-01-08 10:16:57.636492+00:00' as dbt_run_id

FROM read_csv_auto('D:/lab2/data/final_data.csv')

WHERE 1=1
    -- Filtrer les lignes vides
    AND title IS NOT NULL
    AND description IS NOT NULL
  );

[0m11:16:59.353928 [debug] [Thread-1 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.stg_jobs_raw"} */

  
  create view "memory"."main_bronze"."stg_jobs_raw__dbt_tmp" as (
    -- models/bronze/stg_jobs_raw.sql
-- Bronze layer: Lecture directe des données brutes depuis final_data.csv
-- Renommage et typage minimal



-- Read directly from CSV file
SELECT
    -- Renommer les colonnes pour cohérence
    title as job_title,
    location,
    postedTime as posted_time,
    publishedAt as published_at,
    jobUrl as job_url,
    companyName as company_name,
    companyUrl as company_url,
    description as job_description,
    contractType as contract_type,
    workType as work_type,
    
    -- Métadonnées de traçabilité
    CURRENT_TIMESTAMP() as ingestion_timestamp,
    '2026-01-08 10:16:57.636492+00:00' as dbt_run_id

FROM read_csv_auto('D:/lab2/data/final_data.csv')

WHERE 1=1
    -- Filtrer les lignes vides
    AND title IS NOT NULL
    AND description IS NOT NULL
  );

[0m11:16:59.354317 [debug] [Thread-1 (]: DuckDB adapter: Rolling back transaction.
[0m11:16:59.354735 [debug] [Thread-1 (]: On model.job_intelligent.stg_jobs_raw: ROLLBACK
[0m11:16:59.361036 [debug] [Thread-1 (]: Failed to rollback 'model.job_intelligent.stg_jobs_raw'
[0m11:16:59.361385 [debug] [Thread-1 (]: On model.job_intelligent.stg_jobs_raw: Close
[0m11:16:59.365240 [debug] [Thread-1 (]: Runtime Error in model stg_jobs_raw (models\bronze\stg_jobs_raw.sql)
  IO Error: No files found that match the pattern "D:/lab2/data/final_data.csv"
  
  LINE 29: FROM read_csv_auto('D:/lab2/data/final_data.csv')
                ^
[0m11:16:59.367332 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9e6e83d9-0436-4f2b-bbc1-bb553101b864', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002081A9ACD10>]}
[0m11:16:59.368035 [error] [Thread-1 (]: 1 of 13 ERROR creating sql view model main_bronze.stg_jobs_raw ................. [[31mERROR[0m in 0.06s]
[0m11:16:59.368686 [debug] [Thread-1 (]: Finished running node model.job_intelligent.stg_jobs_raw
[0m11:16:59.369291 [debug] [Thread-7 (]: Marking all children of 'model.job_intelligent.stg_jobs_raw' to be skipped because of status 'error'.  Reason: Runtime Error in model stg_jobs_raw (models\bronze\stg_jobs_raw.sql)
  IO Error: No files found that match the pattern "D:/lab2/data/final_data.csv"
  
  LINE 29: FROM read_csv_auto('D:/lab2/data/final_data.csv')
                ^.
[0m11:16:59.370533 [debug] [Thread-2 (]: Began running node model.job_intelligent.int_jobs_cleaned
[0m11:16:59.370928 [info ] [Thread-2 (]: 2 of 13 SKIP relation main_silver.int_jobs_cleaned ............................. [[33mSKIP[0m]
[0m11:16:59.371363 [debug] [Thread-2 (]: Finished running node model.job_intelligent.int_jobs_cleaned
[0m11:16:59.372489 [debug] [Thread-3 (]: Began running node model.job_intelligent.int_job_title_normalization
[0m11:16:59.372858 [info ] [Thread-3 (]: 3 of 13 SKIP relation main_silver.int_job_title_normalization .................. [[33mSKIP[0m]
[0m11:16:59.373354 [debug] [Thread-3 (]: Finished running node model.job_intelligent.int_job_title_normalization
[0m11:16:59.374162 [debug] [Thread-1 (]: Began running node model.job_intelligent.dim_location
[0m11:16:59.374519 [info ] [Thread-1 (]: 5 of 13 SKIP relation main_gold.dim_location ................................... [[33mSKIP[0m]
[0m11:16:59.375019 [debug] [Thread-1 (]: Finished running node model.job_intelligent.dim_location
[0m11:16:59.375432 [debug] [Thread-4 (]: Began running node model.job_intelligent.dim_company
[0m11:16:59.375795 [info ] [Thread-4 (]: 4 of 13 SKIP relation main_gold.dim_company .................................... [[33mSKIP[0m]
[0m11:16:59.376259 [debug] [Thread-4 (]: Finished running node model.job_intelligent.dim_company
[0m11:16:59.376676 [debug] [Thread-2 (]: Began running node model.job_intelligent.dim_time
[0m11:16:59.377033 [info ] [Thread-2 (]: 6 of 13 SKIP relation main_gold.dim_time ....................................... [[33mSKIP[0m]
[0m11:16:59.377485 [debug] [Thread-2 (]: Finished running node model.job_intelligent.dim_time
[0m11:16:59.378091 [debug] [Thread-1 (]: Began running node model.job_intelligent.fact_job_offers
[0m11:16:59.378455 [info ] [Thread-1 (]: 8 of 13 SKIP relation main_gold.fact_job_offers ................................ [[33mSKIP[0m]
[0m11:16:59.378948 [debug] [Thread-1 (]: Finished running node model.job_intelligent.fact_job_offers
[0m11:16:59.379346 [debug] [Thread-3 (]: Began running node model.job_intelligent.int_skills_extraction
[0m11:16:59.379737 [info ] [Thread-3 (]: 7 of 13 SKIP relation main_silver.int_skills_extraction ........................ [[33mSKIP[0m]
[0m11:16:59.380213 [debug] [Thread-3 (]: Finished running node model.job_intelligent.int_skills_extraction
[0m11:16:59.380839 [debug] [Thread-1 (]: Began running node model.job_intelligent.agg_location_analysis
[0m11:16:59.381208 [info ] [Thread-1 (]: 11 of 13 SKIP relation main_gold.agg_location_analysis ......................... [[33mSKIP[0m]
[0m11:16:59.381732 [debug] [Thread-1 (]: Finished running node model.job_intelligent.agg_location_analysis
[0m11:16:59.382144 [debug] [Thread-2 (]: Began running node model.job_intelligent.agg_job_offers_by_category_time
[0m11:16:59.382512 [info ] [Thread-2 (]: 10 of 13 SKIP relation main_gold.agg_job_offers_by_category_time ............... [[33mSKIP[0m]
[0m11:16:59.382964 [debug] [Thread-2 (]: Finished running node model.job_intelligent.agg_job_offers_by_category_time
[0m11:16:59.383476 [debug] [Thread-4 (]: Began running node model.job_intelligent.dim_skills
[0m11:16:59.383857 [info ] [Thread-4 (]: 9 of 13 SKIP relation main_gold.dim_skills ..................................... [[33mSKIP[0m]
[0m11:16:59.384349 [debug] [Thread-4 (]: Finished running node model.job_intelligent.dim_skills
[0m11:16:59.385192 [debug] [Thread-3 (]: Began running node model.job_intelligent.fact_job_skills
[0m11:16:59.385571 [info ] [Thread-3 (]: 12 of 13 SKIP relation main_gold.fact_job_skills ............................... [[33mSKIP[0m]
[0m11:16:59.386034 [debug] [Thread-3 (]: Finished running node model.job_intelligent.fact_job_skills
[0m11:16:59.386796 [debug] [Thread-1 (]: Began running node model.job_intelligent.agg_skills_demand
[0m11:16:59.387176 [info ] [Thread-1 (]: 13 of 13 SKIP relation main_gold.agg_skills_demand ............................. [[33mSKIP[0m]
[0m11:16:59.387709 [debug] [Thread-1 (]: Finished running node model.job_intelligent.agg_skills_demand
[0m11:16:59.389172 [debug] [MainThread]: Using duckdb connection "master"
[0m11:16:59.389518 [debug] [MainThread]: On master: BEGIN
[0m11:16:59.389814 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m11:16:59.390467 [debug] [MainThread]: SQL status: OK in 0.001 seconds
[0m11:16:59.390760 [debug] [MainThread]: On master: COMMIT
[0m11:16:59.391047 [debug] [MainThread]: Using duckdb connection "master"
[0m11:16:59.391323 [debug] [MainThread]: On master: COMMIT
[0m11:16:59.391843 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m11:16:59.392139 [debug] [MainThread]: On master: Close
[0m11:16:59.392506 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:16:59.392773 [debug] [MainThread]: Connection 'create_memory_main_gold' was properly closed.
[0m11:16:59.393059 [debug] [MainThread]: Connection 'create_memory_main_silver' was properly closed.
[0m11:16:59.393329 [debug] [MainThread]: Connection 'create_memory_main_bronze' was properly closed.
[0m11:16:59.393607 [debug] [MainThread]: Connection 'list_memory_main_gold' was properly closed.
[0m11:16:59.393873 [debug] [MainThread]: Connection 'list_memory_main_silver' was properly closed.
[0m11:16:59.394138 [debug] [MainThread]: Connection 'list_memory_main_bronze' was properly closed.
[0m11:16:59.394400 [debug] [MainThread]: Connection 'model.job_intelligent.stg_jobs_raw' was properly closed.
[0m11:16:59.394781 [info ] [MainThread]: 
[0m11:16:59.395172 [info ] [MainThread]: Finished running 12 table models, 1 view model in 0 hours 0 minutes and 0.30 seconds (0.30s).
[0m11:16:59.396178 [debug] [MainThread]: Command end result
[0m11:16:59.416744 [debug] [MainThread]: Wrote artifact WritableManifest to D:\lab2\dbt_project\target\manifest.json
[0m11:16:59.419352 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\lab2\dbt_project\target\semantic_manifest.json
[0m11:16:59.426433 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\lab2\dbt_project\target\run_results.json
[0m11:16:59.426833 [info ] [MainThread]: 
[0m11:16:59.427331 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m11:16:59.427776 [info ] [MainThread]: 
[0m11:16:59.428297 [error] [MainThread]: [31mFailure in model stg_jobs_raw (models\bronze\stg_jobs_raw.sql)[0m
[0m11:16:59.428863 [error] [MainThread]:   Runtime Error in model stg_jobs_raw (models\bronze\stg_jobs_raw.sql)
  IO Error: No files found that match the pattern "D:/lab2/data/final_data.csv"
  
  LINE 29: FROM read_csv_auto('D:/lab2/data/final_data.csv')
                ^
[0m11:16:59.429282 [info ] [MainThread]: 
[0m11:16:59.429769 [info ] [MainThread]:   compiled code at target\compiled\job_intelligent\models\bronze\stg_jobs_raw.sql
[0m11:16:59.430189 [info ] [MainThread]: 
[0m11:16:59.430641 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=12 NO-OP=0 TOTAL=13
[0m11:16:59.431744 [debug] [MainThread]: Command `dbt run` failed at 11:16:59.431604 after 1.87 seconds
[0m11:16:59.432211 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002081BA7E6D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002081BB76CB0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002081BB76F30>]}
[0m11:16:59.432677 [debug] [MainThread]: Flushing usage events
[0m11:17:00.925050 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:19:43.114052 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001917CCA9FD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001917A9B0190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001917A827890>]}


============================== 11:19:43.120078 | 670a07c9-b4ed-400e-b4c9-fc09a043fa5d ==============================
[0m11:19:43.120078 [info ] [MainThread]: Running with dbt=1.10.13
[0m11:19:43.120723 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'use_experimental_parser': 'False', 'cache_selected_only': 'False', 'warn_error': 'None', 'empty': 'False', 'debug': 'False', 'static_parser': 'True', 'log_cache_events': 'False', 'profiles_dir': 'D:\\lab2\\dbt_project', 'log_format': 'default', 'quiet': 'False', 'use_colors': 'True', 'partial_parse': 'True', 'version_check': 'True', 'log_path': 'D:\\lab2\\dbt_project\\logs', 'introspect': 'True', 'no_print': 'None', 'fail_fast': 'False', 'invocation_command': 'dbt run --select stg_jobs_raw', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'target_path': 'None', 'write_json': 'True'}
[0m11:19:43.524410 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '670a07c9-b4ed-400e-b4c9-fc09a043fa5d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001917D173230>]}
[0m11:19:43.611065 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '670a07c9-b4ed-400e-b4c9-fc09a043fa5d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001917CD28380>]}
[0m11:19:43.613817 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m11:19:43.920486 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m11:19:44.128993 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m11:19:44.129429 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m11:19:44.137686 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- seeds
[0m11:19:44.177432 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '670a07c9-b4ed-400e-b4c9-fc09a043fa5d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001917DFC4850>]}
[0m11:19:44.265518 [debug] [MainThread]: Wrote artifact WritableManifest to D:\lab2\dbt_project\target\manifest.json
[0m11:19:44.269152 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\lab2\dbt_project\target\semantic_manifest.json
[0m11:19:44.306442 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '670a07c9-b4ed-400e-b4c9-fc09a043fa5d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001917DCAB7A0>]}
[0m11:19:44.307250 [info ] [MainThread]: Found 13 models, 456 macros
[0m11:19:44.307928 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '670a07c9-b4ed-400e-b4c9-fc09a043fa5d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001917F545470>]}
[0m11:19:44.309969 [info ] [MainThread]: 
[0m11:19:44.310518 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m11:19:44.310948 [info ] [MainThread]: 
[0m11:19:44.311564 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m11:19:44.312672 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_memory'
[0m11:19:44.424091 [debug] [ThreadPool]: Using duckdb connection "list_memory"
[0m11:19:44.424530 [debug] [ThreadPool]: On list_memory: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_memory"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"memory"'
    
  
  
[0m11:19:44.424878 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:19:44.444536 [debug] [ThreadPool]: SQL status: OK in 0.020 seconds
[0m11:19:44.446114 [debug] [ThreadPool]: On list_memory: Close
[0m11:19:44.447019 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_memory, now create_memory_main_bronze)
[0m11:19:44.447827 [debug] [ThreadPool]: Creating schema "database: "memory"
schema: "main_bronze"
"
[0m11:19:44.457423 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_bronze"
[0m11:19:44.458186 [debug] [ThreadPool]: On create_memory_main_bronze: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_memory_main_bronze"} */

    
        select type from duckdb_databases()
        where lower(database_name)='memory'
        and type='sqlite'
    
  
[0m11:19:44.458764 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:19:44.460259 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m11:19:44.461747 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_bronze"
[0m11:19:44.462112 [debug] [ThreadPool]: On create_memory_main_bronze: BEGIN
[0m11:19:44.462698 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m11:19:44.463036 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_bronze"
[0m11:19:44.463367 [debug] [ThreadPool]: On create_memory_main_bronze: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_memory_main_bronze"} */

    
    
        create schema if not exists "memory"."main_bronze"
    
[0m11:19:44.464037 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m11:19:44.464970 [debug] [ThreadPool]: On create_memory_main_bronze: COMMIT
[0m11:19:44.465327 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_bronze"
[0m11:19:44.465733 [debug] [ThreadPool]: On create_memory_main_bronze: COMMIT
[0m11:19:44.466750 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m11:19:44.467259 [debug] [ThreadPool]: On create_memory_main_bronze: Close
[0m11:19:44.473716 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_memory_main_silver'
[0m11:19:44.535405 [debug] [ThreadPool]: Using duckdb connection "list_memory_main_silver"
[0m11:19:44.535908 [debug] [ThreadPool]: On list_memory_main_silver: BEGIN
[0m11:19:44.536301 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:19:44.528888 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_memory_main_bronze'
[0m11:19:44.539181 [debug] [ThreadPool]: Using duckdb connection "list_memory_main_bronze"
[0m11:19:44.539695 [debug] [ThreadPool]: On list_memory_main_bronze: BEGIN
[0m11:19:44.540060 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:19:44.540861 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_memory_main_gold'
[0m11:19:44.543168 [debug] [ThreadPool]: Using duckdb connection "list_memory_main_gold"
[0m11:19:44.543623 [debug] [ThreadPool]: On list_memory_main_gold: BEGIN
[0m11:19:44.543963 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:19:44.544538 [debug] [ThreadPool]: SQL status: OK in 0.008 seconds
[0m11:19:44.545062 [debug] [ThreadPool]: SQL status: OK in 0.005 seconds
[0m11:19:44.545444 [debug] [ThreadPool]: Using duckdb connection "list_memory_main_bronze"
[0m11:19:44.545820 [debug] [ThreadPool]: On list_memory_main_bronze: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_memory_main_bronze"} */
select
      'memory' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main_bronze'
    and lower(table_catalog) = 'memory'
  
[0m11:19:44.544928 [debug] [ThreadPool]: Using duckdb connection "list_memory_main_silver"
[0m11:19:44.546967 [debug] [ThreadPool]: On list_memory_main_silver: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_memory_main_silver"} */
select
      'memory' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main_silver'
    and lower(table_catalog) = 'memory'
  
[0m11:19:44.547603 [debug] [ThreadPool]: SQL status: OK in 0.004 seconds
[0m11:19:44.548004 [debug] [ThreadPool]: Using duckdb connection "list_memory_main_gold"
[0m11:19:44.548385 [debug] [ThreadPool]: On list_memory_main_gold: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_memory_main_gold"} */
select
      'memory' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main_gold'
    and lower(table_catalog) = 'memory'
  
[0m11:19:44.570539 [debug] [ThreadPool]: SQL status: OK in 0.023 seconds
[0m11:19:44.571291 [debug] [ThreadPool]: SQL status: OK in 0.025 seconds
[0m11:19:44.571820 [debug] [ThreadPool]: SQL status: OK in 0.023 seconds
[0m11:19:44.573747 [debug] [ThreadPool]: On list_memory_main_silver: ROLLBACK
[0m11:19:44.575196 [debug] [ThreadPool]: On list_memory_main_bronze: ROLLBACK
[0m11:19:44.577141 [debug] [ThreadPool]: On list_memory_main_gold: ROLLBACK
[0m11:19:44.579761 [debug] [ThreadPool]: Failed to rollback 'list_memory_main_silver'
[0m11:19:44.580317 [debug] [ThreadPool]: On list_memory_main_silver: Close
[0m11:19:44.581755 [debug] [ThreadPool]: Failed to rollback 'list_memory_main_gold'
[0m11:19:44.582161 [debug] [ThreadPool]: On list_memory_main_gold: Close
[0m11:19:44.583912 [debug] [ThreadPool]: Failed to rollback 'list_memory_main_bronze'
[0m11:19:44.584407 [debug] [ThreadPool]: On list_memory_main_bronze: Close
[0m11:19:44.585459 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '670a07c9-b4ed-400e-b4c9-fc09a043fa5d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001917DD58390>]}
[0m11:19:44.586013 [debug] [MainThread]: Using duckdb connection "master"
[0m11:19:44.586402 [debug] [MainThread]: On master: BEGIN
[0m11:19:44.586788 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:19:44.587559 [debug] [MainThread]: SQL status: OK in 0.001 seconds
[0m11:19:44.587971 [debug] [MainThread]: On master: COMMIT
[0m11:19:44.588365 [debug] [MainThread]: Using duckdb connection "master"
[0m11:19:44.588727 [debug] [MainThread]: On master: COMMIT
[0m11:19:44.589335 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m11:19:44.589687 [debug] [MainThread]: On master: Close
[0m11:19:44.595529 [debug] [Thread-1 (]: Began running node model.job_intelligent.stg_jobs_raw
[0m11:19:44.596245 [info ] [Thread-1 (]: 1 of 1 START sql view model main_bronze.stg_jobs_raw ........................... [RUN]
[0m11:19:44.596957 [debug] [Thread-1 (]: Acquiring new duckdb connection 'model.job_intelligent.stg_jobs_raw'
[0m11:19:44.597490 [debug] [Thread-1 (]: Began compiling node model.job_intelligent.stg_jobs_raw
[0m11:19:44.607045 [debug] [Thread-1 (]: Writing injected SQL for node "model.job_intelligent.stg_jobs_raw"
[0m11:19:44.608096 [debug] [Thread-1 (]: Began executing node model.job_intelligent.stg_jobs_raw
[0m11:19:44.646716 [debug] [Thread-1 (]: Writing runtime sql for node "model.job_intelligent.stg_jobs_raw"
[0m11:19:44.647919 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.stg_jobs_raw"
[0m11:19:44.648416 [debug] [Thread-1 (]: On model.job_intelligent.stg_jobs_raw: BEGIN
[0m11:19:44.648947 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m11:19:44.650314 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m11:19:44.650898 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.stg_jobs_raw"
[0m11:19:44.651369 [debug] [Thread-1 (]: On model.job_intelligent.stg_jobs_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.stg_jobs_raw"} */

  
  create view "memory"."main_bronze"."stg_jobs_raw__dbt_tmp" as (
    -- models/bronze/stg_jobs_raw.sql
-- Bronze layer: Lecture directe des données brutes depuis final_data.csv
-- Renommage et typage minimal



-- Read directly from CSV file
SELECT
    -- Renommer les colonnes pour cohérence
    title as job_title,
    location,
    postedTime as posted_time,
    publishedAt as published_at,
    jobUrl as job_url,
    companyName as company_name,
    companyUrl as company_url,
    description as job_description,
    contractType as contract_type,
    workType as work_type,
    
    -- Métadonnées de traçabilité
    CURRENT_TIMESTAMP() as ingestion_timestamp,
    '2026-01-08 10:19:42.890821+00:00' as dbt_run_id

FROM read_csv_auto('D:/lab2/data/final_data.csv')

WHERE 1=1
    -- Filtrer les lignes vides
    AND title IS NOT NULL
    AND description IS NOT NULL
  );

[0m11:19:44.653147 [debug] [Thread-1 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.stg_jobs_raw"} */

  
  create view "memory"."main_bronze"."stg_jobs_raw__dbt_tmp" as (
    -- models/bronze/stg_jobs_raw.sql
-- Bronze layer: Lecture directe des données brutes depuis final_data.csv
-- Renommage et typage minimal



-- Read directly from CSV file
SELECT
    -- Renommer les colonnes pour cohérence
    title as job_title,
    location,
    postedTime as posted_time,
    publishedAt as published_at,
    jobUrl as job_url,
    companyName as company_name,
    companyUrl as company_url,
    description as job_description,
    contractType as contract_type,
    workType as work_type,
    
    -- Métadonnées de traçabilité
    CURRENT_TIMESTAMP() as ingestion_timestamp,
    '2026-01-08 10:19:42.890821+00:00' as dbt_run_id

FROM read_csv_auto('D:/lab2/data/final_data.csv')

WHERE 1=1
    -- Filtrer les lignes vides
    AND title IS NOT NULL
    AND description IS NOT NULL
  );

[0m11:19:44.653735 [debug] [Thread-1 (]: DuckDB adapter: Rolling back transaction.
[0m11:19:44.654326 [debug] [Thread-1 (]: On model.job_intelligent.stg_jobs_raw: ROLLBACK
[0m11:19:44.663709 [debug] [Thread-1 (]: Failed to rollback 'model.job_intelligent.stg_jobs_raw'
[0m11:19:44.664285 [debug] [Thread-1 (]: On model.job_intelligent.stg_jobs_raw: Close
[0m11:19:44.670940 [debug] [Thread-1 (]: Runtime Error in model stg_jobs_raw (models\bronze\stg_jobs_raw.sql)
  IO Error: No files found that match the pattern "D:/lab2/data/final_data.csv"
  
  LINE 29: FROM read_csv_auto('D:/lab2/data/final_data.csv')
                ^
[0m11:19:44.673816 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '670a07c9-b4ed-400e-b4c9-fc09a043fa5d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001917DFEEBD0>]}
[0m11:19:44.674832 [error] [Thread-1 (]: 1 of 1 ERROR creating sql view model main_bronze.stg_jobs_raw .................. [[31mERROR[0m in 0.07s]
[0m11:19:44.675791 [debug] [Thread-1 (]: Finished running node model.job_intelligent.stg_jobs_raw
[0m11:19:44.676590 [debug] [Thread-7 (]: Marking all children of 'model.job_intelligent.stg_jobs_raw' to be skipped because of status 'error'.  Reason: Runtime Error in model stg_jobs_raw (models\bronze\stg_jobs_raw.sql)
  IO Error: No files found that match the pattern "D:/lab2/data/final_data.csv"
  
  LINE 29: FROM read_csv_auto('D:/lab2/data/final_data.csv')
                ^.
[0m11:19:44.679082 [debug] [MainThread]: Using duckdb connection "master"
[0m11:19:44.679488 [debug] [MainThread]: On master: BEGIN
[0m11:19:44.679817 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m11:19:44.680631 [debug] [MainThread]: SQL status: OK in 0.001 seconds
[0m11:19:44.681048 [debug] [MainThread]: On master: COMMIT
[0m11:19:44.681414 [debug] [MainThread]: Using duckdb connection "master"
[0m11:19:44.681752 [debug] [MainThread]: On master: COMMIT
[0m11:19:44.682352 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m11:19:44.682901 [debug] [MainThread]: On master: Close
[0m11:19:44.683754 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:19:44.684143 [debug] [MainThread]: Connection 'create_memory_main_bronze' was properly closed.
[0m11:19:44.684452 [debug] [MainThread]: Connection 'list_memory_main_silver' was properly closed.
[0m11:19:44.684789 [debug] [MainThread]: Connection 'list_memory_main_bronze' was properly closed.
[0m11:19:44.685116 [debug] [MainThread]: Connection 'list_memory_main_gold' was properly closed.
[0m11:19:44.685441 [debug] [MainThread]: Connection 'model.job_intelligent.stg_jobs_raw' was properly closed.
[0m11:19:44.685839 [info ] [MainThread]: 
[0m11:19:44.686369 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 0.37 seconds (0.37s).
[0m11:19:44.687257 [debug] [MainThread]: Command end result
[0m11:19:44.714857 [debug] [MainThread]: Wrote artifact WritableManifest to D:\lab2\dbt_project\target\manifest.json
[0m11:19:44.718458 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\lab2\dbt_project\target\semantic_manifest.json
[0m11:19:44.726471 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\lab2\dbt_project\target\run_results.json
[0m11:19:44.726910 [info ] [MainThread]: 
[0m11:19:44.727469 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m11:19:44.728015 [info ] [MainThread]: 
[0m11:19:44.728857 [error] [MainThread]: [31mFailure in model stg_jobs_raw (models\bronze\stg_jobs_raw.sql)[0m
[0m11:19:44.729691 [error] [MainThread]:   Runtime Error in model stg_jobs_raw (models\bronze\stg_jobs_raw.sql)
  IO Error: No files found that match the pattern "D:/lab2/data/final_data.csv"
  
  LINE 29: FROM read_csv_auto('D:/lab2/data/final_data.csv')
                ^
[0m11:19:44.730187 [info ] [MainThread]: 
[0m11:19:44.730702 [info ] [MainThread]:   compiled code at target\compiled\job_intelligent\models\bronze\stg_jobs_raw.sql
[0m11:19:44.731146 [info ] [MainThread]: 
[0m11:19:44.731541 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=1
[0m11:19:44.732602 [debug] [MainThread]: Command `dbt run` failed at 11:19:44.732427 after 1.98 seconds
[0m11:19:44.733195 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001917F55AD00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001917F0F6030>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001917F0F4D70>]}
[0m11:19:44.733669 [debug] [MainThread]: Flushing usage events
[0m11:19:47.217355 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:20:16.937708 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B38E519FD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B38C220190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B38C097890>]}


============================== 11:20:16.948691 | 5230da88-023a-4026-bdcb-672e8567e650 ==============================
[0m11:20:16.948691 [info ] [MainThread]: Running with dbt=1.10.13
[0m11:20:16.950799 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'target_path': 'None', 'cache_selected_only': 'False', 'warn_error': 'None', 'profiles_dir': 'D:\\lab2\\dbt_project', 'debug': 'False', 'static_parser': 'True', 'log_cache_events': 'False', 'empty': 'False', 'log_format': 'default', 'quiet': 'False', 'use_colors': 'True', 'partial_parse': 'True', 'version_check': 'True', 'log_path': 'D:\\lab2\\dbt_project\\logs', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'fail_fast': 'False', 'invocation_command': 'dbt run --select stg_jobs_raw', 'no_print': 'None', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'write_json': 'True', 'use_experimental_parser': 'False'}
[0m11:20:17.378851 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '5230da88-023a-4026-bdcb-672e8567e650', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B38E9CF230>]}
[0m11:20:17.475843 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '5230da88-023a-4026-bdcb-672e8567e650', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B38E588380>]}
[0m11:20:17.479091 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m11:20:17.746563 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m11:20:17.897543 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m11:20:17.898163 [debug] [MainThread]: Partial parsing: updated file: job_intelligent://models\bronze\stg_jobs_raw.sql
[0m11:20:18.286061 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- seeds
[0m11:20:18.293265 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '5230da88-023a-4026-bdcb-672e8567e650', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B38F7E4E50>]}
[0m11:20:18.363255 [debug] [MainThread]: Wrote artifact WritableManifest to D:\lab2\dbt_project\target\manifest.json
[0m11:20:18.366288 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\lab2\dbt_project\target\semantic_manifest.json
[0m11:20:18.402947 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '5230da88-023a-4026-bdcb-672e8567e650', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B38F50F7A0>]}
[0m11:20:18.403549 [info ] [MainThread]: Found 13 models, 456 macros
[0m11:20:18.404051 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5230da88-023a-4026-bdcb-672e8567e650', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B3909019B0>]}
[0m11:20:18.406051 [info ] [MainThread]: 
[0m11:20:18.406619 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m11:20:18.407100 [info ] [MainThread]: 
[0m11:20:18.407649 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m11:20:18.408888 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_memory'
[0m11:20:18.508899 [debug] [ThreadPool]: Using duckdb connection "list_memory"
[0m11:20:18.509370 [debug] [ThreadPool]: On list_memory: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_memory"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"memory"'
    
  
  
[0m11:20:18.509660 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:20:18.526360 [debug] [ThreadPool]: SQL status: OK in 0.017 seconds
[0m11:20:18.527889 [debug] [ThreadPool]: On list_memory: Close
[0m11:20:18.528776 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_memory, now create_memory_main_bronze)
[0m11:20:18.529243 [debug] [ThreadPool]: Creating schema "database: "memory"
schema: "main_bronze"
"
[0m11:20:18.535466 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_bronze"
[0m11:20:18.535831 [debug] [ThreadPool]: On create_memory_main_bronze: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_memory_main_bronze"} */

    
        select type from duckdb_databases()
        where lower(database_name)='memory'
        and type='sqlite'
    
  
[0m11:20:18.536156 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:20:18.537266 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m11:20:18.539296 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_bronze"
[0m11:20:18.539863 [debug] [ThreadPool]: On create_memory_main_bronze: BEGIN
[0m11:20:18.540489 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m11:20:18.540763 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_bronze"
[0m11:20:18.541026 [debug] [ThreadPool]: On create_memory_main_bronze: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_memory_main_bronze"} */

    
    
        create schema if not exists "memory"."main_bronze"
    
[0m11:20:18.541563 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m11:20:18.542341 [debug] [ThreadPool]: On create_memory_main_bronze: COMMIT
[0m11:20:18.542635 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_bronze"
[0m11:20:18.542895 [debug] [ThreadPool]: On create_memory_main_bronze: COMMIT
[0m11:20:18.543366 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m11:20:18.543648 [debug] [ThreadPool]: On create_memory_main_bronze: Close
[0m11:20:18.549868 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_memory_main_silver'
[0m11:20:18.554923 [debug] [ThreadPool]: Using duckdb connection "list_memory_main_silver"
[0m11:20:18.555253 [debug] [ThreadPool]: On list_memory_main_silver: BEGIN
[0m11:20:18.555513 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:20:18.556333 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m11:20:18.556734 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_memory_main_gold'
[0m11:20:18.557057 [debug] [ThreadPool]: Using duckdb connection "list_memory_main_silver"
[0m11:20:18.558737 [debug] [ThreadPool]: Using duckdb connection "list_memory_main_gold"
[0m11:20:18.559207 [debug] [ThreadPool]: On list_memory_main_silver: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_memory_main_silver"} */
select
      'memory' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main_silver'
    and lower(table_catalog) = 'memory'
  
[0m11:20:18.559633 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_memory_main_bronze'
[0m11:20:18.559986 [debug] [ThreadPool]: On list_memory_main_gold: BEGIN
[0m11:20:18.561655 [debug] [ThreadPool]: Using duckdb connection "list_memory_main_bronze"
[0m11:20:18.562014 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:20:18.562369 [debug] [ThreadPool]: On list_memory_main_bronze: BEGIN
[0m11:20:18.562935 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:20:18.563483 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m11:20:18.563766 [debug] [ThreadPool]: Using duckdb connection "list_memory_main_gold"
[0m11:20:18.564059 [debug] [ThreadPool]: On list_memory_main_gold: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_memory_main_gold"} */
select
      'memory' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main_gold'
    and lower(table_catalog) = 'memory'
  
[0m11:20:18.564568 [debug] [ThreadPool]: SQL status: OK in 0.002 seconds
[0m11:20:18.564951 [debug] [ThreadPool]: Using duckdb connection "list_memory_main_bronze"
[0m11:20:18.565520 [debug] [ThreadPool]: On list_memory_main_bronze: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_memory_main_bronze"} */
select
      'memory' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main_bronze'
    and lower(table_catalog) = 'memory'
  
[0m11:20:18.577650 [debug] [ThreadPool]: SQL status: OK in 0.011 seconds
[0m11:20:18.578336 [debug] [ThreadPool]: SQL status: OK in 0.018 seconds
[0m11:20:18.578702 [debug] [ThreadPool]: SQL status: OK in 0.014 seconds
[0m11:20:18.580222 [debug] [ThreadPool]: On list_memory_main_silver: ROLLBACK
[0m11:20:18.581393 [debug] [ThreadPool]: On list_memory_main_bronze: ROLLBACK
[0m11:20:18.582935 [debug] [ThreadPool]: On list_memory_main_gold: ROLLBACK
[0m11:20:18.585087 [debug] [ThreadPool]: Failed to rollback 'list_memory_main_silver'
[0m11:20:18.585418 [debug] [ThreadPool]: On list_memory_main_silver: Close
[0m11:20:18.586564 [debug] [ThreadPool]: Failed to rollback 'list_memory_main_bronze'
[0m11:20:18.586839 [debug] [ThreadPool]: On list_memory_main_bronze: Close
[0m11:20:18.587759 [debug] [ThreadPool]: Failed to rollback 'list_memory_main_gold'
[0m11:20:18.588031 [debug] [ThreadPool]: On list_memory_main_gold: Close
[0m11:20:18.588954 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5230da88-023a-4026-bdcb-672e8567e650', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B39104E5B0>]}
[0m11:20:18.589353 [debug] [MainThread]: Using duckdb connection "master"
[0m11:20:18.589618 [debug] [MainThread]: On master: BEGIN
[0m11:20:18.589879 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:20:18.590466 [debug] [MainThread]: SQL status: OK in 0.001 seconds
[0m11:20:18.590740 [debug] [MainThread]: On master: COMMIT
[0m11:20:18.591004 [debug] [MainThread]: Using duckdb connection "master"
[0m11:20:18.591255 [debug] [MainThread]: On master: COMMIT
[0m11:20:18.591715 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m11:20:18.591997 [debug] [MainThread]: On master: Close
[0m11:20:18.597352 [debug] [Thread-1 (]: Began running node model.job_intelligent.stg_jobs_raw
[0m11:20:18.597891 [info ] [Thread-1 (]: 1 of 1 START sql view model main_bronze.stg_jobs_raw ........................... [RUN]
[0m11:20:18.598527 [debug] [Thread-1 (]: Acquiring new duckdb connection 'model.job_intelligent.stg_jobs_raw'
[0m11:20:18.598963 [debug] [Thread-1 (]: Began compiling node model.job_intelligent.stg_jobs_raw
[0m11:20:18.605714 [debug] [Thread-1 (]: Writing injected SQL for node "model.job_intelligent.stg_jobs_raw"
[0m11:20:18.606499 [debug] [Thread-1 (]: Began executing node model.job_intelligent.stg_jobs_raw
[0m11:20:18.634237 [debug] [Thread-1 (]: Writing runtime sql for node "model.job_intelligent.stg_jobs_raw"
[0m11:20:18.635197 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.stg_jobs_raw"
[0m11:20:18.635600 [debug] [Thread-1 (]: On model.job_intelligent.stg_jobs_raw: BEGIN
[0m11:20:18.635923 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m11:20:18.636792 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m11:20:18.637113 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.stg_jobs_raw"
[0m11:20:18.637458 [debug] [Thread-1 (]: On model.job_intelligent.stg_jobs_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.stg_jobs_raw"} */

  
  create view "memory"."main_bronze"."stg_jobs_raw__dbt_tmp" as (
    -- models/bronze/stg_jobs_raw.sql
-- Bronze layer: Lecture directe des données brutes depuis final_data.csv
-- Renommage et typage minimal



-- Read directly from CSV file
SELECT
    -- Renommer les colonnes pour cohérence
    title as job_title,
    location,
    postedTime as posted_time,
    publishedAt as published_at,
    jobUrl as job_url,
    companyName as company_name,
    companyUrl as company_url,
    description as job_description,
    contractType as contract_type,
    workType as work_type,
    
    -- Métadonnées de traçabilité
    CURRENT_TIMESTAMP() as ingestion_timestamp,
    '2026-01-08 10:20:16.801593+00:00' as dbt_run_id

FROM read_csv_auto('D:/lab2/final_data.csv')

WHERE 1=1
    -- Filtrer les lignes vides
    AND title IS NOT NULL
    AND description IS NOT NULL
  );

[0m11:20:18.727792 [debug] [Thread-1 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.stg_jobs_raw"} */

  
  create view "memory"."main_bronze"."stg_jobs_raw__dbt_tmp" as (
    -- models/bronze/stg_jobs_raw.sql
-- Bronze layer: Lecture directe des données brutes depuis final_data.csv
-- Renommage et typage minimal



-- Read directly from CSV file
SELECT
    -- Renommer les colonnes pour cohérence
    title as job_title,
    location,
    postedTime as posted_time,
    publishedAt as published_at,
    jobUrl as job_url,
    companyName as company_name,
    companyUrl as company_url,
    description as job_description,
    contractType as contract_type,
    workType as work_type,
    
    -- Métadonnées de traçabilité
    CURRENT_TIMESTAMP() as ingestion_timestamp,
    '2026-01-08 10:20:16.801593+00:00' as dbt_run_id

FROM read_csv_auto('D:/lab2/final_data.csv')

WHERE 1=1
    -- Filtrer les lignes vides
    AND title IS NOT NULL
    AND description IS NOT NULL
  );

[0m11:20:18.728411 [debug] [Thread-1 (]: DuckDB adapter: Rolling back transaction.
[0m11:20:18.728971 [debug] [Thread-1 (]: On model.job_intelligent.stg_jobs_raw: ROLLBACK
[0m11:20:18.737586 [debug] [Thread-1 (]: Failed to rollback 'model.job_intelligent.stg_jobs_raw'
[0m11:20:18.738060 [debug] [Thread-1 (]: On model.job_intelligent.stg_jobs_raw: Close
[0m11:20:18.742728 [debug] [Thread-1 (]: Runtime Error in model stg_jobs_raw (models\bronze\stg_jobs_raw.sql)
  Catalog Error: Scalar Function with name current_timestamp does not exist!
  Did you mean "current_localtimestamp"?
  
  LINE 26:     CURRENT_TIMESTAMP() as ingestion_timestamp,
               ^
[0m11:20:18.745265 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5230da88-023a-4026-bdcb-672e8567e650', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B3909893D0>]}
[0m11:20:18.745992 [error] [Thread-1 (]: 1 of 1 ERROR creating sql view model main_bronze.stg_jobs_raw .................. [[31mERROR[0m in 0.14s]
[0m11:20:18.746765 [debug] [Thread-1 (]: Finished running node model.job_intelligent.stg_jobs_raw
[0m11:20:18.747540 [debug] [Thread-7 (]: Marking all children of 'model.job_intelligent.stg_jobs_raw' to be skipped because of status 'error'.  Reason: Runtime Error in model stg_jobs_raw (models\bronze\stg_jobs_raw.sql)
  Catalog Error: Scalar Function with name current_timestamp does not exist!
  Did you mean "current_localtimestamp"?
  
  LINE 26:     CURRENT_TIMESTAMP() as ingestion_timestamp,
               ^.
[0m11:20:18.750127 [debug] [MainThread]: Using duckdb connection "master"
[0m11:20:18.750618 [debug] [MainThread]: On master: BEGIN
[0m11:20:18.751003 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m11:20:18.751832 [debug] [MainThread]: SQL status: OK in 0.001 seconds
[0m11:20:18.752187 [debug] [MainThread]: On master: COMMIT
[0m11:20:18.752526 [debug] [MainThread]: Using duckdb connection "master"
[0m11:20:18.752881 [debug] [MainThread]: On master: COMMIT
[0m11:20:18.753559 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m11:20:18.753983 [debug] [MainThread]: On master: Close
[0m11:20:18.754499 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:20:18.754879 [debug] [MainThread]: Connection 'create_memory_main_bronze' was properly closed.
[0m11:20:18.755249 [debug] [MainThread]: Connection 'list_memory_main_silver' was properly closed.
[0m11:20:18.755625 [debug] [MainThread]: Connection 'list_memory_main_gold' was properly closed.
[0m11:20:18.755981 [debug] [MainThread]: Connection 'list_memory_main_bronze' was properly closed.
[0m11:20:18.756352 [debug] [MainThread]: Connection 'model.job_intelligent.stg_jobs_raw' was properly closed.
[0m11:20:18.756818 [info ] [MainThread]: 
[0m11:20:18.757339 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 0.35 seconds (0.35s).
[0m11:20:18.758400 [debug] [MainThread]: Command end result
[0m11:20:18.792187 [debug] [MainThread]: Wrote artifact WritableManifest to D:\lab2\dbt_project\target\manifest.json
[0m11:20:18.794709 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\lab2\dbt_project\target\semantic_manifest.json
[0m11:20:18.801269 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\lab2\dbt_project\target\run_results.json
[0m11:20:18.801678 [info ] [MainThread]: 
[0m11:20:18.802122 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m11:20:18.802524 [info ] [MainThread]: 
[0m11:20:18.802997 [error] [MainThread]: [31mFailure in model stg_jobs_raw (models\bronze\stg_jobs_raw.sql)[0m
[0m11:20:18.803472 [error] [MainThread]:   Runtime Error in model stg_jobs_raw (models\bronze\stg_jobs_raw.sql)
  Catalog Error: Scalar Function with name current_timestamp does not exist!
  Did you mean "current_localtimestamp"?
  
  LINE 26:     CURRENT_TIMESTAMP() as ingestion_timestamp,
               ^
[0m11:20:18.803868 [info ] [MainThread]: 
[0m11:20:18.804309 [info ] [MainThread]:   compiled code at target\compiled\job_intelligent\models\bronze\stg_jobs_raw.sql
[0m11:20:18.804676 [info ] [MainThread]: 
[0m11:20:18.805074 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=1
[0m11:20:18.806079 [debug] [MainThread]: Command `dbt run` failed at 11:20:18.805953 after 2.08 seconds
[0m11:20:18.806497 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B390D4EDB0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B38F699590>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B38F6991D0>]}
[0m11:20:18.806903 [debug] [MainThread]: Flushing usage events
[0m11:20:20.088321 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:20:42.469769 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024D77855FD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024D75554190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024D753C7890>]}


============================== 11:20:42.478897 | f4d1d373-6ad3-4c7c-9253-8826ed96f191 ==============================
[0m11:20:42.478897 [info ] [MainThread]: Running with dbt=1.10.13
[0m11:20:42.479997 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'write_json': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'empty': 'False', 'debug': 'False', 'static_parser': 'True', 'log_cache_events': 'False', 'profiles_dir': 'D:\\lab2\\dbt_project', 'log_format': 'default', 'quiet': 'False', 'use_colors': 'True', 'partial_parse': 'True', 'version_check': 'True', 'log_path': 'D:\\lab2\\dbt_project\\logs', 'introspect': 'True', 'no_print': 'None', 'fail_fast': 'False', 'invocation_command': 'dbt run --select stg_jobs_raw', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'target_path': 'None', 'use_experimental_parser': 'False'}
[0m11:20:42.842593 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f4d1d373-6ad3-4c7c-9253-8826ed96f191', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024D77D13230>]}
[0m11:20:42.928627 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'f4d1d373-6ad3-4c7c-9253-8826ed96f191', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024D778C8380>]}
[0m11:20:42.931930 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m11:20:43.204248 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m11:20:43.382545 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m11:20:43.383146 [debug] [MainThread]: Partial parsing: updated file: job_intelligent://models\bronze\stg_jobs_raw.sql
[0m11:20:43.736488 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- seeds
[0m11:20:43.743601 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f4d1d373-6ad3-4c7c-9253-8826ed96f191', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024D78BB4E50>]}
[0m11:20:43.829801 [debug] [MainThread]: Wrote artifact WritableManifest to D:\lab2\dbt_project\target\manifest.json
[0m11:20:43.832396 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\lab2\dbt_project\target\semantic_manifest.json
[0m11:20:43.875955 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f4d1d373-6ad3-4c7c-9253-8826ed96f191', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024D7884B7A0>]}
[0m11:20:43.876545 [info ] [MainThread]: Found 13 models, 456 macros
[0m11:20:43.876972 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f4d1d373-6ad3-4c7c-9253-8826ed96f191', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024D79CB19B0>]}
[0m11:20:43.878825 [info ] [MainThread]: 
[0m11:20:43.879359 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m11:20:43.879777 [info ] [MainThread]: 
[0m11:20:43.880482 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m11:20:43.881724 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_memory'
[0m11:20:43.968966 [debug] [ThreadPool]: Using duckdb connection "list_memory"
[0m11:20:43.969667 [debug] [ThreadPool]: On list_memory: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_memory"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"memory"'
    
  
  
[0m11:20:43.970034 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:20:43.988230 [debug] [ThreadPool]: SQL status: OK in 0.018 seconds
[0m11:20:43.989665 [debug] [ThreadPool]: On list_memory: Close
[0m11:20:43.990531 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_memory, now create_memory_main_bronze)
[0m11:20:43.991020 [debug] [ThreadPool]: Creating schema "database: "memory"
schema: "main_bronze"
"
[0m11:20:43.997402 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_bronze"
[0m11:20:43.997775 [debug] [ThreadPool]: On create_memory_main_bronze: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_memory_main_bronze"} */

    
        select type from duckdb_databases()
        where lower(database_name)='memory'
        and type='sqlite'
    
  
[0m11:20:43.998159 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:20:43.999202 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m11:20:44.000542 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_bronze"
[0m11:20:44.000870 [debug] [ThreadPool]: On create_memory_main_bronze: BEGIN
[0m11:20:44.001390 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m11:20:44.001709 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_bronze"
[0m11:20:44.002024 [debug] [ThreadPool]: On create_memory_main_bronze: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_memory_main_bronze"} */

    
    
        create schema if not exists "memory"."main_bronze"
    
[0m11:20:44.002715 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m11:20:44.003739 [debug] [ThreadPool]: On create_memory_main_bronze: COMMIT
[0m11:20:44.004109 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_bronze"
[0m11:20:44.004427 [debug] [ThreadPool]: On create_memory_main_bronze: COMMIT
[0m11:20:44.005004 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m11:20:44.005342 [debug] [ThreadPool]: On create_memory_main_bronze: Close
[0m11:20:44.012225 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_memory_main_silver'
[0m11:20:44.019218 [debug] [ThreadPool]: Using duckdb connection "list_memory_main_silver"
[0m11:20:44.019953 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_memory_main_gold'
[0m11:20:44.020449 [debug] [ThreadPool]: On list_memory_main_silver: BEGIN
[0m11:20:44.022743 [debug] [ThreadPool]: Using duckdb connection "list_memory_main_gold"
[0m11:20:44.023442 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_memory_main_bronze'
[0m11:20:44.023873 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:20:44.024449 [debug] [ThreadPool]: On list_memory_main_gold: BEGIN
[0m11:20:44.026594 [debug] [ThreadPool]: Using duckdb connection "list_memory_main_bronze"
[0m11:20:44.027457 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:20:44.028024 [debug] [ThreadPool]: On list_memory_main_bronze: BEGIN
[0m11:20:44.028546 [debug] [ThreadPool]: SQL status: OK in 0.005 seconds
[0m11:20:44.029315 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:20:44.029856 [debug] [ThreadPool]: Using duckdb connection "list_memory_main_silver"
[0m11:20:44.030594 [debug] [ThreadPool]: SQL status: OK in 0.003 seconds
[0m11:20:44.031151 [debug] [ThreadPool]: On list_memory_main_silver: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_memory_main_silver"} */
select
      'memory' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main_silver'
    and lower(table_catalog) = 'memory'
  
[0m11:20:44.031725 [debug] [ThreadPool]: Using duckdb connection "list_memory_main_gold"
[0m11:20:44.032189 [debug] [ThreadPool]: SQL status: OK in 0.003 seconds
[0m11:20:44.033071 [debug] [ThreadPool]: On list_memory_main_gold: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_memory_main_gold"} */
select
      'memory' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main_gold'
    and lower(table_catalog) = 'memory'
  
[0m11:20:44.033628 [debug] [ThreadPool]: Using duckdb connection "list_memory_main_bronze"
[0m11:20:44.034481 [debug] [ThreadPool]: On list_memory_main_bronze: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_memory_main_bronze"} */
select
      'memory' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main_bronze'
    and lower(table_catalog) = 'memory'
  
[0m11:20:44.058463 [debug] [ThreadPool]: SQL status: OK in 0.024 seconds
[0m11:20:44.060714 [debug] [ThreadPool]: On list_memory_main_gold: ROLLBACK
[0m11:20:44.061419 [debug] [ThreadPool]: SQL status: OK in 0.026 seconds
[0m11:20:44.062893 [debug] [ThreadPool]: On list_memory_main_bronze: ROLLBACK
[0m11:20:44.063513 [debug] [ThreadPool]: SQL status: OK in 0.031 seconds
[0m11:20:44.064914 [debug] [ThreadPool]: On list_memory_main_silver: ROLLBACK
[0m11:20:44.067289 [debug] [ThreadPool]: Failed to rollback 'list_memory_main_gold'
[0m11:20:44.067649 [debug] [ThreadPool]: On list_memory_main_gold: Close
[0m11:20:44.069139 [debug] [ThreadPool]: Failed to rollback 'list_memory_main_silver'
[0m11:20:44.069753 [debug] [ThreadPool]: On list_memory_main_silver: Close
[0m11:20:44.071132 [debug] [ThreadPool]: Failed to rollback 'list_memory_main_bronze'
[0m11:20:44.071535 [debug] [ThreadPool]: On list_memory_main_bronze: Close
[0m11:20:44.072730 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f4d1d373-6ad3-4c7c-9253-8826ed96f191', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024D7A3FE270>]}
[0m11:20:44.073264 [debug] [MainThread]: Using duckdb connection "master"
[0m11:20:44.073652 [debug] [MainThread]: On master: BEGIN
[0m11:20:44.073997 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:20:44.074698 [debug] [MainThread]: SQL status: OK in 0.001 seconds
[0m11:20:44.075074 [debug] [MainThread]: On master: COMMIT
[0m11:20:44.075448 [debug] [MainThread]: Using duckdb connection "master"
[0m11:20:44.075798 [debug] [MainThread]: On master: COMMIT
[0m11:20:44.076364 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m11:20:44.076738 [debug] [MainThread]: On master: Close
[0m11:20:44.083209 [debug] [Thread-1 (]: Began running node model.job_intelligent.stg_jobs_raw
[0m11:20:44.083931 [info ] [Thread-1 (]: 1 of 1 START sql view model main_bronze.stg_jobs_raw ........................... [RUN]
[0m11:20:44.084572 [debug] [Thread-1 (]: Acquiring new duckdb connection 'model.job_intelligent.stg_jobs_raw'
[0m11:20:44.085038 [debug] [Thread-1 (]: Began compiling node model.job_intelligent.stg_jobs_raw
[0m11:20:44.093023 [debug] [Thread-1 (]: Writing injected SQL for node "model.job_intelligent.stg_jobs_raw"
[0m11:20:44.093955 [debug] [Thread-1 (]: Began executing node model.job_intelligent.stg_jobs_raw
[0m11:20:44.125734 [debug] [Thread-1 (]: Writing runtime sql for node "model.job_intelligent.stg_jobs_raw"
[0m11:20:44.126651 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.stg_jobs_raw"
[0m11:20:44.127042 [debug] [Thread-1 (]: On model.job_intelligent.stg_jobs_raw: BEGIN
[0m11:20:44.127404 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m11:20:44.128253 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m11:20:44.128629 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.stg_jobs_raw"
[0m11:20:44.129037 [debug] [Thread-1 (]: On model.job_intelligent.stg_jobs_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.stg_jobs_raw"} */

  
  create view "memory"."main_bronze"."stg_jobs_raw__dbt_tmp" as (
    -- models/bronze/stg_jobs_raw.sql
-- Bronze layer: Lecture directe des données brutes depuis final_data.csv
-- Renommage et typage minimal



-- Read directly from CSV file
SELECT
    -- Renommer les colonnes pour cohérence
    title as job_title,
    location,
    postedTime as posted_time,
    publishedAt as published_at,
    jobUrl as job_url,
    companyName as company_name,
    companyUrl as company_url,
    description as job_description,
    contractType as contract_type,
    workType as work_type,
    
    -- Métadonnées de traçabilité
    NOW() as ingestion_timestamp,
    '2026-01-08 10:20:42.342141+00:00' as dbt_run_id

FROM read_csv_auto('D:/lab2/final_data.csv')

WHERE 1=1
    -- Filtrer les lignes vides
    AND title IS NOT NULL
    AND description IS NOT NULL
  );

[0m11:20:44.175398 [debug] [Thread-1 (]: SQL status: OK in 0.046 seconds
[0m11:20:44.181542 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.stg_jobs_raw"
[0m11:20:44.181993 [debug] [Thread-1 (]: On model.job_intelligent.stg_jobs_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.stg_jobs_raw"} */
alter view "memory"."main_bronze"."stg_jobs_raw__dbt_tmp" rename to "stg_jobs_raw"
[0m11:20:44.182788 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m11:20:44.193243 [debug] [Thread-1 (]: On model.job_intelligent.stg_jobs_raw: COMMIT
[0m11:20:44.193698 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.stg_jobs_raw"
[0m11:20:44.194064 [debug] [Thread-1 (]: On model.job_intelligent.stg_jobs_raw: COMMIT
[0m11:20:44.198946 [debug] [Thread-1 (]: SQL status: OK in 0.004 seconds
[0m11:20:44.208643 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.stg_jobs_raw"
[0m11:20:44.209209 [debug] [Thread-1 (]: On model.job_intelligent.stg_jobs_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.stg_jobs_raw"} */

      drop view if exists "memory"."main_bronze"."stg_jobs_raw__dbt_backup" cascade
    
[0m11:20:44.210407 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m11:20:44.212991 [debug] [Thread-1 (]: On model.job_intelligent.stg_jobs_raw: Close
[0m11:20:44.215579 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f4d1d373-6ad3-4c7c-9253-8826ed96f191', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024D79D38B90>]}
[0m11:20:44.216274 [info ] [Thread-1 (]: 1 of 1 OK created sql view model main_bronze.stg_jobs_raw ...................... [[32mOK[0m in 0.13s]
[0m11:20:44.216989 [debug] [Thread-1 (]: Finished running node model.job_intelligent.stg_jobs_raw
[0m11:20:44.219003 [debug] [MainThread]: Using duckdb connection "master"
[0m11:20:44.219550 [debug] [MainThread]: On master: BEGIN
[0m11:20:44.219904 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m11:20:44.220666 [debug] [MainThread]: SQL status: OK in 0.001 seconds
[0m11:20:44.221018 [debug] [MainThread]: On master: COMMIT
[0m11:20:44.221355 [debug] [MainThread]: Using duckdb connection "master"
[0m11:20:44.221694 [debug] [MainThread]: On master: COMMIT
[0m11:20:44.222287 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m11:20:44.222631 [debug] [MainThread]: On master: Close
[0m11:20:44.223068 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:20:44.223411 [debug] [MainThread]: Connection 'create_memory_main_bronze' was properly closed.
[0m11:20:44.223737 [debug] [MainThread]: Connection 'list_memory_main_silver' was properly closed.
[0m11:20:44.224056 [debug] [MainThread]: Connection 'list_memory_main_gold' was properly closed.
[0m11:20:44.224437 [debug] [MainThread]: Connection 'list_memory_main_bronze' was properly closed.
[0m11:20:44.224765 [debug] [MainThread]: Connection 'model.job_intelligent.stg_jobs_raw' was properly closed.
[0m11:20:44.225144 [info ] [MainThread]: 
[0m11:20:44.225579 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 0.34 seconds (0.34s).
[0m11:20:44.226420 [debug] [MainThread]: Command end result
[0m11:20:44.249518 [debug] [MainThread]: Wrote artifact WritableManifest to D:\lab2\dbt_project\target\manifest.json
[0m11:20:44.252563 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\lab2\dbt_project\target\semantic_manifest.json
[0m11:20:44.259419 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\lab2\dbt_project\target\run_results.json
[0m11:20:44.259821 [info ] [MainThread]: 
[0m11:20:44.260283 [info ] [MainThread]: [32mCompleted successfully[0m
[0m11:20:44.260677 [info ] [MainThread]: 
[0m11:20:44.261084 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m11:20:44.262118 [debug] [MainThread]: Command `dbt run` succeeded at 11:20:44.261984 after 2.02 seconds
[0m11:20:44.262540 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024D79CF5A70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024D79D819F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024D79D802D0>]}
[0m11:20:44.262957 [debug] [MainThread]: Flushing usage events
[0m11:20:45.381289 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:21:01.918242 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026425A09FD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026423700190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026423577890>]}


============================== 11:21:01.923453 | 68907942-ac91-4e4d-ae77-bb80fcbc439e ==============================
[0m11:21:01.923453 [info ] [MainThread]: Running with dbt=1.10.13
[0m11:21:01.924061 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'target_path': 'None', 'cache_selected_only': 'False', 'warn_error': 'None', 'empty': 'None', 'debug': 'False', 'static_parser': 'True', 'log_cache_events': 'False', 'profiles_dir': 'D:\\lab2\\dbt_project', 'quiet': 'False', 'log_format': 'default', 'use_colors': 'True', 'partial_parse': 'True', 'version_check': 'True', 'log_path': 'D:\\lab2\\dbt_project\\logs', 'introspect': 'True', 'invocation_command': 'dbt debug', 'fail_fast': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'no_print': 'None', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'write_json': 'True'}
[0m11:21:01.951297 [info ] [MainThread]: dbt version: 1.10.13
[0m11:21:01.951765 [info ] [MainThread]: python version: 3.13.2
[0m11:21:01.952132 [info ] [MainThread]: python path: C:\Users\HP\AppData\Local\Programs\Python\Python313\python.exe
[0m11:21:01.952501 [info ] [MainThread]: os info: Windows-11-10.0.26200-SP0
[0m11:21:02.110809 [info ] [MainThread]: Using profiles dir at D:\lab2\dbt_project
[0m11:21:02.111340 [info ] [MainThread]: Using profiles.yml file at D:\lab2\dbt_project\profiles.yml
[0m11:21:02.111704 [info ] [MainThread]: Using dbt_project.yml file at D:\lab2\dbt_project\dbt_project.yml
[0m11:21:02.114705 [info ] [MainThread]: adapter type: duckdb
[0m11:21:02.115271 [info ] [MainThread]: adapter version: 1.10.0
[0m11:21:02.292415 [info ] [MainThread]: Configuration:
[0m11:21:02.292990 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m11:21:02.293585 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m11:21:02.294348 [info ] [MainThread]: Required dependencies:
[0m11:21:02.294820 [debug] [MainThread]: Executing "git --help"
[0m11:21:02.363502 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--no-lazy-fetch]\n           [--no-optional-locks] [--no-advice] [--bare] [--git-dir=<path>]\n           [--work-tree=<path>] [--namespace=<name>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone      Clone a repository into a new directory\n   init       Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add        Add file contents to the index\n   mv         Move or rename a file, a directory, or a symlink\n   restore    Restore working tree files\n   rm         Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect     Use binary search to find the commit that introduced a bug\n   diff       Show changes between commits, commit and working tree, etc\n   grep       Print lines matching a pattern\n   log        Show commit logs\n   show       Show various types of objects\n   status     Show the working tree status\n\ngrow, mark and tweak your common history\n   backfill   Download missing objects in a partial clone\n   branch     List, create, or delete branches\n   commit     Record changes to the repository\n   merge      Join two or more development histories together\n   rebase     Reapply commits on top of another base tip\n   reset      Reset current HEAD to the specified state\n   switch     Switch branches\n   tag        Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch      Download objects and refs from another repository\n   pull       Fetch from and integrate with another repository or a local branch\n   push       Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m11:21:02.364151 [debug] [MainThread]: STDERR: "b''"
[0m11:21:02.364730 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m11:21:02.365168 [info ] [MainThread]: Connection:
[0m11:21:02.365590 [info ] [MainThread]:   database: memory
[0m11:21:02.365976 [info ] [MainThread]:   schema: main
[0m11:21:02.366358 [info ] [MainThread]:   path: :memory:
[0m11:21:02.366737 [info ] [MainThread]:   config_options: None
[0m11:21:02.367119 [info ] [MainThread]:   extensions: None
[0m11:21:02.367511 [info ] [MainThread]:   settings: {}
[0m11:21:02.367892 [info ] [MainThread]:   external_root: .
[0m11:21:02.368269 [info ] [MainThread]:   use_credential_provider: None
[0m11:21:02.368651 [info ] [MainThread]:   attach: None
[0m11:21:02.369028 [info ] [MainThread]:   filesystems: None
[0m11:21:02.369405 [info ] [MainThread]:   remote: None
[0m11:21:02.369794 [info ] [MainThread]:   plugins: None
[0m11:21:02.370139 [info ] [MainThread]:   disable_transactions: False
[0m11:21:02.370707 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m11:21:02.627491 [debug] [MainThread]: Acquiring new duckdb connection 'debug'
[0m11:21:02.698854 [debug] [MainThread]: Using duckdb connection "debug"
[0m11:21:02.699257 [debug] [MainThread]: On debug: select 1 as id
[0m11:21:02.699577 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:21:02.715578 [debug] [MainThread]: SQL status: OK in 0.016 seconds
[0m11:21:02.716453 [debug] [MainThread]: On debug: Close
[0m11:21:02.716794 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m11:21:02.717157 [info ] [MainThread]: [32mAll checks passed![0m
[0m11:21:02.717968 [debug] [MainThread]: Command `dbt debug` succeeded at 11:21:02.717868 after 0.98 seconds
[0m11:21:02.718248 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m11:21:02.718550 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026427F29F30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026427FCD5B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026427EA0E20>]}
[0m11:21:02.718905 [debug] [MainThread]: Flushing usage events
[0m11:21:03.981398 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:21:07.428346 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DD01375FD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DD7F054190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DD7EEC7890>]}


============================== 11:21:07.435619 | 18a2aaad-b799-431b-8eb9-eb0ab248d18b ==============================
[0m11:21:07.435619 [info ] [MainThread]: Running with dbt=1.10.13
[0m11:21:07.436529 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'write_json': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'profiles_dir': 'D:\\lab2\\dbt_project', 'debug': 'False', 'static_parser': 'True', 'log_cache_events': 'False', 'empty': 'False', 'quiet': 'False', 'log_format': 'default', 'use_colors': 'True', 'partial_parse': 'True', 'version_check': 'True', 'log_path': 'D:\\lab2\\dbt_project\\logs', 'introspect': 'True', 'no_print': 'None', 'fail_fast': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'invocation_command': 'dbt run', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'target_path': 'None', 'use_experimental_parser': 'False'}
[0m11:21:07.806911 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '18a2aaad-b799-431b-8eb9-eb0ab248d18b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DD01833230>]}
[0m11:21:07.887178 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '18a2aaad-b799-431b-8eb9-eb0ab248d18b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DD013E8380>]}
[0m11:21:07.890465 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m11:21:08.168652 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m11:21:08.327162 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m11:21:08.327588 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m11:21:08.334955 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- seeds
[0m11:21:08.360046 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '18a2aaad-b799-431b-8eb9-eb0ab248d18b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DD02654850>]}
[0m11:21:08.425755 [debug] [MainThread]: Wrote artifact WritableManifest to D:\lab2\dbt_project\target\manifest.json
[0m11:21:08.428257 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\lab2\dbt_project\target\semantic_manifest.json
[0m11:21:08.462799 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '18a2aaad-b799-431b-8eb9-eb0ab248d18b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DD0236B7A0>]}
[0m11:21:08.463362 [info ] [MainThread]: Found 13 models, 456 macros
[0m11:21:08.463782 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '18a2aaad-b799-431b-8eb9-eb0ab248d18b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DD03C6D470>]}
[0m11:21:08.465916 [info ] [MainThread]: 
[0m11:21:08.466327 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m11:21:08.466688 [info ] [MainThread]: 
[0m11:21:08.467243 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m11:21:08.473325 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_memory'
[0m11:21:08.490202 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_memory'
[0m11:21:08.494289 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_memory'
[0m11:21:08.636375 [debug] [ThreadPool]: Using duckdb connection "list_memory"
[0m11:21:08.637083 [debug] [ThreadPool]: Using duckdb connection "list_memory"
[0m11:21:08.637677 [debug] [ThreadPool]: On list_memory: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_memory"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"memory"'
    
  
  
[0m11:21:08.638321 [debug] [ThreadPool]: Using duckdb connection "list_memory"
[0m11:21:08.639096 [debug] [ThreadPool]: On list_memory: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_memory"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"memory"'
    
  
  
[0m11:21:08.639711 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:21:08.640313 [debug] [ThreadPool]: On list_memory: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_memory"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"memory"'
    
  
  
[0m11:21:08.641041 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:21:08.642190 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:21:08.671611 [debug] [ThreadPool]: SQL status: OK in 0.032 seconds
[0m11:21:08.673910 [debug] [ThreadPool]: On list_memory: Close
[0m11:21:08.674670 [debug] [ThreadPool]: SQL status: OK in 0.034 seconds
[0m11:21:08.675311 [debug] [ThreadPool]: SQL status: OK in 0.033 seconds
[0m11:21:08.677806 [debug] [ThreadPool]: On list_memory: Close
[0m11:21:08.679648 [debug] [ThreadPool]: On list_memory: Close
[0m11:21:08.681015 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_memory, now create_memory_main_gold)
[0m11:21:08.681687 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_memory, now create_memory_main_silver)
[0m11:21:08.682379 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_memory, now create_memory_main_bronze)
[0m11:21:08.683143 [debug] [ThreadPool]: Creating schema "database: "memory"
schema: "main_gold"
"
[0m11:21:08.683958 [debug] [ThreadPool]: Creating schema "database: "memory"
schema: "main_silver"
"
[0m11:21:08.684674 [debug] [ThreadPool]: Creating schema "database: "memory"
schema: "main_bronze"
"
[0m11:21:08.702626 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_silver"
[0m11:21:08.703803 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_gold"
[0m11:21:08.707074 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_bronze"
[0m11:21:08.707942 [debug] [ThreadPool]: On create_memory_main_silver: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_memory_main_silver"} */

    
        select type from duckdb_databases()
        where lower(database_name)='memory'
        and type='sqlite'
    
  
[0m11:21:08.709216 [debug] [ThreadPool]: On create_memory_main_gold: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_memory_main_gold"} */

    
        select type from duckdb_databases()
        where lower(database_name)='memory'
        and type='sqlite'
    
  
[0m11:21:08.710481 [debug] [ThreadPool]: On create_memory_main_bronze: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_memory_main_bronze"} */

    
        select type from duckdb_databases()
        where lower(database_name)='memory'
        and type='sqlite'
    
  
[0m11:21:08.711371 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:21:08.712204 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:21:08.713102 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:21:08.715267 [debug] [ThreadPool]: SQL status: OK in 0.004 seconds
[0m11:21:08.716057 [debug] [ThreadPool]: SQL status: OK in 0.004 seconds
[0m11:21:08.718610 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_gold"
[0m11:21:08.719254 [debug] [ThreadPool]: On create_memory_main_gold: BEGIN
[0m11:21:08.721720 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_silver"
[0m11:21:08.722392 [debug] [ThreadPool]: On create_memory_main_silver: BEGIN
[0m11:21:08.723347 [debug] [ThreadPool]: SQL status: OK in 0.010 seconds
[0m11:21:08.724152 [debug] [ThreadPool]: SQL status: OK in 0.004 seconds
[0m11:21:08.724842 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_gold"
[0m11:21:08.725481 [debug] [ThreadPool]: On create_memory_main_gold: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_memory_main_gold"} */

    
    
        create schema if not exists "memory"."main_gold"
    
[0m11:21:08.728226 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_bronze"
[0m11:21:08.729041 [debug] [ThreadPool]: On create_memory_main_bronze: BEGIN
[0m11:21:08.729927 [debug] [ThreadPool]: SQL status: OK in 0.004 seconds
[0m11:21:08.731608 [debug] [ThreadPool]: On create_memory_main_gold: COMMIT
[0m11:21:08.732261 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_gold"
[0m11:21:08.732865 [debug] [ThreadPool]: On create_memory_main_gold: COMMIT
[0m11:21:08.733685 [debug] [ThreadPool]: SQL status: OK in 0.004 seconds
[0m11:21:08.734340 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_bronze"
[0m11:21:08.734966 [debug] [ThreadPool]: On create_memory_main_bronze: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_memory_main_bronze"} */

    
    
        create schema if not exists "memory"."main_bronze"
    
[0m11:21:08.735784 [debug] [ThreadPool]: SQL status: OK in 0.002 seconds
[0m11:21:08.736444 [debug] [ThreadPool]: On create_memory_main_gold: Close
[0m11:21:08.737590 [debug] [ThreadPool]: SQL status: OK in 0.015 seconds
[0m11:21:08.738298 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_silver"
[0m11:21:08.738916 [debug] [ThreadPool]: On create_memory_main_silver: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_memory_main_silver"} */

    
    
        create schema if not exists "memory"."main_silver"
    
[0m11:21:08.740020 [debug] [ThreadPool]: SQL status: OK in 0.004 seconds
[0m11:21:08.741882 [debug] [ThreadPool]: On create_memory_main_bronze: COMMIT
[0m11:21:08.742710 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_bronze"
[0m11:21:08.743423 [debug] [ThreadPool]: On create_memory_main_bronze: COMMIT
[0m11:21:08.744265 [debug] [ThreadPool]: SQL status: OK in 0.005 seconds
[0m11:21:08.746620 [debug] [ThreadPool]: On create_memory_main_silver: COMMIT
[0m11:21:08.747394 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_silver"
[0m11:21:08.748136 [debug] [ThreadPool]: On create_memory_main_silver: COMMIT
[0m11:21:08.749219 [debug] [ThreadPool]: SQL status: OK in 0.004 seconds
[0m11:21:08.750016 [debug] [ThreadPool]: On create_memory_main_bronze: Close
[0m11:21:08.751466 [debug] [ThreadPool]: SQL status: OK in 0.003 seconds
[0m11:21:08.752250 [debug] [ThreadPool]: On create_memory_main_silver: Close
[0m11:21:08.757642 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_memory_main_silver'
[0m11:21:08.770998 [debug] [ThreadPool]: Using duckdb connection "list_memory_main_silver"
[0m11:21:08.771836 [debug] [ThreadPool]: On list_memory_main_silver: BEGIN
[0m11:21:08.773465 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_memory_main_gold'
[0m11:21:08.774412 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:21:08.779497 [debug] [ThreadPool]: Using duckdb connection "list_memory_main_gold"
[0m11:21:08.780676 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_memory_main_bronze'
[0m11:21:08.782169 [debug] [ThreadPool]: On list_memory_main_gold: BEGIN
[0m11:21:08.785852 [debug] [ThreadPool]: Using duckdb connection "list_memory_main_bronze"
[0m11:21:08.786712 [debug] [ThreadPool]: SQL status: OK in 0.012 seconds
[0m11:21:08.787797 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:21:08.788751 [debug] [ThreadPool]: On list_memory_main_bronze: BEGIN
[0m11:21:08.789805 [debug] [ThreadPool]: Using duckdb connection "list_memory_main_silver"
[0m11:21:08.791340 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:21:08.792403 [debug] [ThreadPool]: On list_memory_main_silver: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_memory_main_silver"} */
select
      'memory' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main_silver'
    and lower(table_catalog) = 'memory'
  
[0m11:21:08.793522 [debug] [ThreadPool]: SQL status: OK in 0.006 seconds
[0m11:21:08.795297 [debug] [ThreadPool]: Using duckdb connection "list_memory_main_gold"
[0m11:21:08.796127 [debug] [ThreadPool]: SQL status: OK in 0.005 seconds
[0m11:21:08.796993 [debug] [ThreadPool]: On list_memory_main_gold: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_memory_main_gold"} */
select
      'memory' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main_gold'
    and lower(table_catalog) = 'memory'
  
[0m11:21:08.797934 [debug] [ThreadPool]: Using duckdb connection "list_memory_main_bronze"
[0m11:21:08.799231 [debug] [ThreadPool]: On list_memory_main_bronze: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_memory_main_bronze"} */
select
      'memory' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main_bronze'
    and lower(table_catalog) = 'memory'
  
[0m11:21:08.829463 [debug] [ThreadPool]: SQL status: OK in 0.031 seconds
[0m11:21:08.830059 [debug] [ThreadPool]: SQL status: OK in 0.030 seconds
[0m11:21:08.830985 [debug] [ThreadPool]: SQL status: OK in 0.036 seconds
[0m11:21:08.832731 [debug] [ThreadPool]: On list_memory_main_silver: ROLLBACK
[0m11:21:08.834206 [debug] [ThreadPool]: On list_memory_main_gold: ROLLBACK
[0m11:21:08.835604 [debug] [ThreadPool]: On list_memory_main_bronze: ROLLBACK
[0m11:21:08.837811 [debug] [ThreadPool]: Failed to rollback 'list_memory_main_silver'
[0m11:21:08.838204 [debug] [ThreadPool]: On list_memory_main_silver: Close
[0m11:21:08.839534 [debug] [ThreadPool]: Failed to rollback 'list_memory_main_gold'
[0m11:21:08.839876 [debug] [ThreadPool]: On list_memory_main_gold: Close
[0m11:21:08.841075 [debug] [ThreadPool]: Failed to rollback 'list_memory_main_bronze'
[0m11:21:08.841406 [debug] [ThreadPool]: On list_memory_main_bronze: Close
[0m11:21:08.842342 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '18a2aaad-b799-431b-8eb9-eb0ab248d18b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DD03E63E10>]}
[0m11:21:08.842811 [debug] [MainThread]: Using duckdb connection "master"
[0m11:21:08.843124 [debug] [MainThread]: On master: BEGIN
[0m11:21:08.843448 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:21:08.844151 [debug] [MainThread]: SQL status: OK in 0.001 seconds
[0m11:21:08.844470 [debug] [MainThread]: On master: COMMIT
[0m11:21:08.844777 [debug] [MainThread]: Using duckdb connection "master"
[0m11:21:08.845069 [debug] [MainThread]: On master: COMMIT
[0m11:21:08.845609 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m11:21:08.845941 [debug] [MainThread]: On master: Close
[0m11:21:08.851456 [debug] [Thread-1 (]: Began running node model.job_intelligent.stg_jobs_raw
[0m11:21:08.852069 [info ] [Thread-1 (]: 1 of 13 START sql view model main_bronze.stg_jobs_raw .......................... [RUN]
[0m11:21:08.852814 [debug] [Thread-1 (]: Acquiring new duckdb connection 'model.job_intelligent.stg_jobs_raw'
[0m11:21:08.853254 [debug] [Thread-1 (]: Began compiling node model.job_intelligent.stg_jobs_raw
[0m11:21:08.861336 [debug] [Thread-1 (]: Writing injected SQL for node "model.job_intelligent.stg_jobs_raw"
[0m11:21:08.862232 [debug] [Thread-1 (]: Began executing node model.job_intelligent.stg_jobs_raw
[0m11:21:08.896111 [debug] [Thread-1 (]: Writing runtime sql for node "model.job_intelligent.stg_jobs_raw"
[0m11:21:08.897086 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.stg_jobs_raw"
[0m11:21:08.897521 [debug] [Thread-1 (]: On model.job_intelligent.stg_jobs_raw: BEGIN
[0m11:21:08.897903 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m11:21:08.898789 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m11:21:08.899200 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.stg_jobs_raw"
[0m11:21:08.899636 [debug] [Thread-1 (]: On model.job_intelligent.stg_jobs_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.stg_jobs_raw"} */

  
  create view "memory"."main_bronze"."stg_jobs_raw__dbt_tmp" as (
    -- models/bronze/stg_jobs_raw.sql
-- Bronze layer: Lecture directe des données brutes depuis final_data.csv
-- Renommage et typage minimal



-- Read directly from CSV file
SELECT
    -- Renommer les colonnes pour cohérence
    title as job_title,
    location,
    postedTime as posted_time,
    publishedAt as published_at,
    jobUrl as job_url,
    companyName as company_name,
    companyUrl as company_url,
    description as job_description,
    contractType as contract_type,
    workType as work_type,
    
    -- Métadonnées de traçabilité
    NOW() as ingestion_timestamp,
    '2026-01-08 10:21:07.303718+00:00' as dbt_run_id

FROM read_csv_auto('D:/lab2/final_data.csv')

WHERE 1=1
    -- Filtrer les lignes vides
    AND title IS NOT NULL
    AND description IS NOT NULL
  );

[0m11:21:08.946857 [debug] [Thread-1 (]: SQL status: OK in 0.047 seconds
[0m11:21:08.952212 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.stg_jobs_raw"
[0m11:21:08.952563 [debug] [Thread-1 (]: On model.job_intelligent.stg_jobs_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.stg_jobs_raw"} */
alter view "memory"."main_bronze"."stg_jobs_raw__dbt_tmp" rename to "stg_jobs_raw"
[0m11:21:08.953216 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m11:21:08.962579 [debug] [Thread-1 (]: On model.job_intelligent.stg_jobs_raw: COMMIT
[0m11:21:08.962987 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.stg_jobs_raw"
[0m11:21:08.963304 [debug] [Thread-1 (]: On model.job_intelligent.stg_jobs_raw: COMMIT
[0m11:21:08.963925 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m11:21:08.968609 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.stg_jobs_raw"
[0m11:21:08.968953 [debug] [Thread-1 (]: On model.job_intelligent.stg_jobs_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.stg_jobs_raw"} */

      drop view if exists "memory"."main_bronze"."stg_jobs_raw__dbt_backup" cascade
    
[0m11:21:08.969640 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m11:21:08.971909 [debug] [Thread-1 (]: On model.job_intelligent.stg_jobs_raw: Close
[0m11:21:08.974453 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '18a2aaad-b799-431b-8eb9-eb0ab248d18b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DD0267DF10>]}
[0m11:21:08.975338 [info ] [Thread-1 (]: 1 of 13 OK created sql view model main_bronze.stg_jobs_raw ..................... [[32mOK[0m in 0.12s]
[0m11:21:08.975979 [debug] [Thread-1 (]: Finished running node model.job_intelligent.stg_jobs_raw
[0m11:21:08.976813 [debug] [Thread-2 (]: Began running node model.job_intelligent.int_jobs_cleaned
[0m11:21:08.977247 [info ] [Thread-2 (]: 2 of 13 START sql table model main_silver.int_jobs_cleaned ..................... [RUN]
[0m11:21:08.977851 [debug] [Thread-2 (]: Acquiring new duckdb connection 'model.job_intelligent.int_jobs_cleaned'
[0m11:21:08.978191 [debug] [Thread-2 (]: Began compiling node model.job_intelligent.int_jobs_cleaned
[0m11:21:08.980990 [debug] [Thread-2 (]: Writing injected SQL for node "model.job_intelligent.int_jobs_cleaned"
[0m11:21:08.982099 [debug] [Thread-2 (]: Began executing node model.job_intelligent.int_jobs_cleaned
[0m11:21:09.000190 [debug] [Thread-2 (]: Writing runtime sql for node "model.job_intelligent.int_jobs_cleaned"
[0m11:21:09.002299 [debug] [Thread-2 (]: Using duckdb connection "model.job_intelligent.int_jobs_cleaned"
[0m11:21:09.003276 [debug] [Thread-2 (]: On model.job_intelligent.int_jobs_cleaned: BEGIN
[0m11:21:09.004184 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m11:21:09.005931 [debug] [Thread-2 (]: SQL status: OK in 0.002 seconds
[0m11:21:09.006704 [debug] [Thread-2 (]: Using duckdb connection "model.job_intelligent.int_jobs_cleaned"
[0m11:21:09.007426 [debug] [Thread-2 (]: On model.job_intelligent.int_jobs_cleaned: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.int_jobs_cleaned"} */

  
    
    

    create  table
      "memory"."main_silver"."int_jobs_cleaned__dbt_tmp"
  
    as (
      -- models/silver/int_jobs_cleaned.sql
-- Silver layer: Nettoyage et normalisation des données brutes



WITH raw_jobs AS (
    SELECT * FROM "memory"."main_bronze"."stg_jobs_raw"
),

cleaned_jobs AS (
    SELECT
        -- Texte: lowercase, trim, remove special characters
        LOWER(TRIM(job_title)) as job_title_cleaned,
        LOWER(TRIM(location)) as location_cleaned,
        LOWER(TRIM(company_name)) as company_name_cleaned,
        LOWER(TRIM(job_description)) as job_description_cleaned,
        LOWER(TRIM(contract_type)) as contract_type_cleaned,
        LOWER(TRIM(work_type)) as work_type_cleaned,
        
        -- URLs as-is
        job_url,
        company_url,
        
        -- Dates
        TRY_CAST(published_at AS DATE) as published_date,
        posted_time,
        
        -- Extract year-month for time-based analysis
        DATE_TRUNC('month', TRY_CAST(published_at AS DATE)) as published_year_month,
        EXTRACT(YEAR FROM TRY_CAST(published_at AS DATE)) as published_year,
        EXTRACT(MONTH FROM TRY_CAST(published_at AS DATE)) as published_month,
        
        -- Métadonnées
        ingestion_timestamp
    FROM raw_jobs
)

SELECT
    *,
    -- Deduplication flag
    ROW_NUMBER() OVER (
        PARTITION BY job_title_cleaned, company_name_cleaned, location_cleaned 
        ORDER BY published_date DESC
    ) as dedup_rank
FROM cleaned_jobs
    );
  
  
[0m11:21:09.198018 [debug] [Thread-2 (]: SQL status: OK in 0.190 seconds
[0m11:21:09.201070 [debug] [Thread-2 (]: Using duckdb connection "model.job_intelligent.int_jobs_cleaned"
[0m11:21:09.201495 [debug] [Thread-2 (]: On model.job_intelligent.int_jobs_cleaned: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.int_jobs_cleaned"} */
alter table "memory"."main_silver"."int_jobs_cleaned__dbt_tmp" rename to "int_jobs_cleaned"
[0m11:21:09.203714 [debug] [Thread-2 (]: SQL status: OK in 0.002 seconds
[0m11:21:09.209100 [debug] [Thread-2 (]: On model.job_intelligent.int_jobs_cleaned: COMMIT
[0m11:21:09.209558 [debug] [Thread-2 (]: Using duckdb connection "model.job_intelligent.int_jobs_cleaned"
[0m11:21:09.209961 [debug] [Thread-2 (]: On model.job_intelligent.int_jobs_cleaned: COMMIT
[0m11:21:09.214993 [debug] [Thread-2 (]: SQL status: OK in 0.005 seconds
[0m11:21:09.217430 [debug] [Thread-2 (]: Using duckdb connection "model.job_intelligent.int_jobs_cleaned"
[0m11:21:09.217851 [debug] [Thread-2 (]: On model.job_intelligent.int_jobs_cleaned: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.int_jobs_cleaned"} */

      drop table if exists "memory"."main_silver"."int_jobs_cleaned__dbt_backup" cascade
    
[0m11:21:09.218593 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m11:21:09.220005 [debug] [Thread-2 (]: On model.job_intelligent.int_jobs_cleaned: Close
[0m11:21:09.220595 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '18a2aaad-b799-431b-8eb9-eb0ab248d18b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DD03C8EFC0>]}
[0m11:21:09.221247 [info ] [Thread-2 (]: 2 of 13 OK created sql table model main_silver.int_jobs_cleaned ................ [[32mOK[0m in 0.24s]
[0m11:21:09.221947 [debug] [Thread-2 (]: Finished running node model.job_intelligent.int_jobs_cleaned
[0m11:21:09.222768 [debug] [Thread-3 (]: Began running node model.job_intelligent.int_job_title_normalization
[0m11:21:09.223317 [info ] [Thread-3 (]: 3 of 13 START sql table model main_silver.int_job_title_normalization .......... [RUN]
[0m11:21:09.223966 [debug] [Thread-3 (]: Acquiring new duckdb connection 'model.job_intelligent.int_job_title_normalization'
[0m11:21:09.224405 [debug] [Thread-3 (]: Began compiling node model.job_intelligent.int_job_title_normalization
[0m11:21:09.228827 [debug] [Thread-3 (]: Writing injected SQL for node "model.job_intelligent.int_job_title_normalization"
[0m11:21:09.229966 [debug] [Thread-3 (]: Began executing node model.job_intelligent.int_job_title_normalization
[0m11:21:09.233646 [debug] [Thread-3 (]: Writing runtime sql for node "model.job_intelligent.int_job_title_normalization"
[0m11:21:09.234748 [debug] [Thread-3 (]: Using duckdb connection "model.job_intelligent.int_job_title_normalization"
[0m11:21:09.235276 [debug] [Thread-3 (]: On model.job_intelligent.int_job_title_normalization: BEGIN
[0m11:21:09.235772 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m11:21:09.236698 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m11:21:09.237133 [debug] [Thread-3 (]: Using duckdb connection "model.job_intelligent.int_job_title_normalization"
[0m11:21:09.237677 [debug] [Thread-3 (]: On model.job_intelligent.int_job_title_normalization: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.int_job_title_normalization"} */

  
    
    

    create  table
      "memory"."main_silver"."int_job_title_normalization__dbt_tmp"
  
    as (
      -- models/silver/int_job_title_normalization.sql
-- Silver layer: Normaliser les intitulés de postes



WITH cleaned_jobs AS (
    SELECT * FROM "memory"."main_silver"."int_jobs_cleaned"
),

title_normalized AS (
    SELECT
        *,
        CASE
            WHEN job_title_cleaned LIKE '%data engineer%' THEN 'Data Engineer'
            WHEN job_title_cleaned LIKE '%data scientist%' THEN 'Data Scientist'
            WHEN job_title_cleaned LIKE '%data analyst%' THEN 'Data Analyst'
            WHEN job_title_cleaned LIKE '%analytics engineer%' THEN 'Analytics Engineer'
            WHEN job_title_cleaned LIKE '%ml engineer%' OR job_title_cleaned LIKE '%machine learning%' THEN 'ML Engineer'
            WHEN job_title_cleaned LIKE '%data architect%' THEN 'Data Architect'
            WHEN job_title_cleaned LIKE '%bi developer%' OR job_title_cleaned LIKE '%business intelligence%' THEN 'BI Developer'
            WHEN job_title_cleaned LIKE '%etl%' OR job_title_cleaned LIKE '%pipeline%' THEN 'ETL/Pipeline Engineer'
            WHEN job_title_cleaned LIKE '%data engineer%' THEN 'Data Engineer'
            ELSE 'Other Data Role'
        END as job_category,
        
        CASE
            WHEN contract_type_cleaned LIKE '%cdi%' OR contract_type_cleaned LIKE '%permanent%' THEN 'Permanent'
            WHEN contract_type_cleaned LIKE '%cdd%' OR contract_type_cleaned LIKE '%contract%' THEN 'Contract'
            WHEN contract_type_cleaned LIKE '%stage%' OR contract_type_cleaned LIKE '%internship%' THEN 'Internship'
            WHEN contract_type_cleaned LIKE '%freelance%' THEN 'Freelance'
            ELSE 'Not Specified'
        END as contract_type_normalized,
        
        CASE
            WHEN work_type_cleaned LIKE '%remote%' THEN 'Remote'
            WHEN work_type_cleaned LIKE '%hybrid%' THEN 'Hybrid'
            WHEN work_type_cleaned LIKE '%onsite%' OR work_type_cleaned LIKE '%on-site%' THEN 'On-site'
            ELSE 'Not Specified'
        END as work_type_normalized
    
    FROM cleaned_jobs
    WHERE dedup_rank = 1  -- Keep only first occurrence (most recent)
)

SELECT * FROM title_normalized
    );
  
  
[0m11:21:09.290566 [debug] [Thread-3 (]: SQL status: OK in 0.052 seconds
[0m11:21:09.293672 [debug] [Thread-3 (]: Using duckdb connection "model.job_intelligent.int_job_title_normalization"
[0m11:21:09.294107 [debug] [Thread-3 (]: On model.job_intelligent.int_job_title_normalization: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.int_job_title_normalization"} */
alter table "memory"."main_silver"."int_job_title_normalization__dbt_tmp" rename to "int_job_title_normalization"
[0m11:21:09.295267 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m11:21:09.296855 [debug] [Thread-3 (]: On model.job_intelligent.int_job_title_normalization: COMMIT
[0m11:21:09.297273 [debug] [Thread-3 (]: Using duckdb connection "model.job_intelligent.int_job_title_normalization"
[0m11:21:09.297671 [debug] [Thread-3 (]: On model.job_intelligent.int_job_title_normalization: COMMIT
[0m11:21:09.298356 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m11:21:09.300580 [debug] [Thread-3 (]: Using duckdb connection "model.job_intelligent.int_job_title_normalization"
[0m11:21:09.301007 [debug] [Thread-3 (]: On model.job_intelligent.int_job_title_normalization: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.int_job_title_normalization"} */

      drop table if exists "memory"."main_silver"."int_job_title_normalization__dbt_backup" cascade
    
[0m11:21:09.301686 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m11:21:09.303075 [debug] [Thread-3 (]: On model.job_intelligent.int_job_title_normalization: Close
[0m11:21:09.303646 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '18a2aaad-b799-431b-8eb9-eb0ab248d18b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DD050A8AF0>]}
[0m11:21:09.304324 [info ] [Thread-3 (]: 3 of 13 OK created sql table model main_silver.int_job_title_normalization ..... [[32mOK[0m in 0.08s]
[0m11:21:09.305035 [debug] [Thread-3 (]: Finished running node model.job_intelligent.int_job_title_normalization
[0m11:21:09.306094 [debug] [Thread-4 (]: Began running node model.job_intelligent.dim_company
[0m11:21:09.306774 [info ] [Thread-4 (]: 4 of 13 START sql table model main_gold.dim_company ............................ [RUN]
[0m11:21:09.307503 [debug] [Thread-4 (]: Acquiring new duckdb connection 'model.job_intelligent.dim_company'
[0m11:21:09.307943 [debug] [Thread-4 (]: Began compiling node model.job_intelligent.dim_company
[0m11:21:09.311565 [debug] [Thread-4 (]: Writing injected SQL for node "model.job_intelligent.dim_company"
[0m11:21:09.312167 [debug] [Thread-3 (]: Began running node model.job_intelligent.int_skills_extraction
[0m11:21:09.312822 [info ] [Thread-3 (]: 7 of 13 START sql table model main_silver.int_skills_extraction ................ [RUN]
[0m11:21:09.313469 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly model.job_intelligent.int_job_title_normalization, now model.job_intelligent.int_skills_extraction)
[0m11:21:09.313926 [debug] [Thread-3 (]: Began compiling node model.job_intelligent.int_skills_extraction
[0m11:21:09.318107 [debug] [Thread-3 (]: Writing injected SQL for node "model.job_intelligent.int_skills_extraction"
[0m11:21:09.318785 [debug] [Thread-1 (]: Began running node model.job_intelligent.dim_location
[0m11:21:09.319385 [info ] [Thread-1 (]: 5 of 13 START sql table model main_gold.dim_location ........................... [RUN]
[0m11:21:09.319942 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.job_intelligent.stg_jobs_raw, now model.job_intelligent.dim_location)
[0m11:21:09.320631 [debug] [Thread-1 (]: Began compiling node model.job_intelligent.dim_location
[0m11:21:09.324286 [debug] [Thread-1 (]: Writing injected SQL for node "model.job_intelligent.dim_location"
[0m11:21:09.324909 [debug] [Thread-2 (]: Began running node model.job_intelligent.dim_time
[0m11:21:09.325465 [info ] [Thread-2 (]: 6 of 13 START sql table model main_gold.dim_time ............................... [RUN]
[0m11:21:09.326038 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly model.job_intelligent.int_jobs_cleaned, now model.job_intelligent.dim_time)
[0m11:21:09.326476 [debug] [Thread-2 (]: Began compiling node model.job_intelligent.dim_time
[0m11:21:09.329909 [debug] [Thread-2 (]: Writing injected SQL for node "model.job_intelligent.dim_time"
[0m11:21:09.331042 [debug] [Thread-3 (]: Began executing node model.job_intelligent.int_skills_extraction
[0m11:21:09.335163 [debug] [Thread-3 (]: Writing runtime sql for node "model.job_intelligent.int_skills_extraction"
[0m11:21:09.335942 [debug] [Thread-4 (]: Began executing node model.job_intelligent.dim_company
[0m11:21:09.339668 [debug] [Thread-4 (]: Writing runtime sql for node "model.job_intelligent.dim_company"
[0m11:21:09.340344 [debug] [Thread-2 (]: Began executing node model.job_intelligent.dim_time
[0m11:21:09.340901 [debug] [Thread-1 (]: Began executing node model.job_intelligent.dim_location
[0m11:21:09.348001 [debug] [Thread-2 (]: Writing runtime sql for node "model.job_intelligent.dim_time"
[0m11:21:09.351411 [debug] [Thread-1 (]: Writing runtime sql for node "model.job_intelligent.dim_location"
[0m11:21:09.352176 [debug] [Thread-4 (]: Using duckdb connection "model.job_intelligent.dim_company"
[0m11:21:09.352912 [debug] [Thread-3 (]: Using duckdb connection "model.job_intelligent.int_skills_extraction"
[0m11:21:09.353706 [debug] [Thread-4 (]: On model.job_intelligent.dim_company: BEGIN
[0m11:21:09.354543 [debug] [Thread-3 (]: On model.job_intelligent.int_skills_extraction: BEGIN
[0m11:21:09.355168 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m11:21:09.355805 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.dim_location"
[0m11:21:09.356427 [debug] [Thread-2 (]: Using duckdb connection "model.job_intelligent.dim_time"
[0m11:21:09.357015 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m11:21:09.357854 [debug] [Thread-1 (]: On model.job_intelligent.dim_location: BEGIN
[0m11:21:09.358457 [debug] [Thread-2 (]: On model.job_intelligent.dim_time: BEGIN
[0m11:21:09.358985 [debug] [Thread-4 (]: SQL status: OK in 0.004 seconds
[0m11:21:09.359808 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:21:09.360390 [debug] [Thread-3 (]: SQL status: OK in 0.003 seconds
[0m11:21:09.360893 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m11:21:09.361518 [debug] [Thread-4 (]: Using duckdb connection "model.job_intelligent.dim_company"
[0m11:21:09.362279 [debug] [Thread-3 (]: Using duckdb connection "model.job_intelligent.int_skills_extraction"
[0m11:21:09.362873 [debug] [Thread-1 (]: SQL status: OK in 0.003 seconds
[0m11:21:09.363691 [debug] [Thread-4 (]: On model.job_intelligent.dim_company: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_company"} */

  
    
    

    create  table
      "memory"."main_gold"."dim_company__dbt_tmp"
  
    as (
      -- models/gold/dim_company.sql
-- Gold layer: Dimension Company



WITH jobs AS (
    SELECT DISTINCT
        company_name_cleaned,
        company_url
    FROM "memory"."main_silver"."int_job_title_normalization"
    WHERE company_name_cleaned IS NOT NULL
),

ranked_companies AS (
    SELECT
        ROW_NUMBER() OVER (ORDER BY company_name_cleaned) as company_id,
        company_name_cleaned as company_name,
        company_url,
        CURRENT_TIMESTAMP() as created_at
    FROM jobs
)

SELECT
    company_id,
    company_name,
    company_url,
    created_at
FROM ranked_companies
ORDER BY company_id
    );
  
  
[0m11:21:09.364346 [debug] [Thread-2 (]: SQL status: OK in 0.003 seconds
[0m11:21:09.365108 [debug] [Thread-3 (]: On model.job_intelligent.int_skills_extraction: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.int_skills_extraction"} */

  
    
    

    create  table
      "memory"."main_silver"."int_skills_extraction__dbt_tmp"
  
    as (
      -- models/silver/int_skills_extraction.sql
-- Silver layer: Extraction des compétences depuis la description



WITH jobs_with_titles AS (
    SELECT * FROM "memory"."main_silver"."int_job_title_normalization"
),

skills_mapping AS (
    -- Définir un mapping de compétences communes Data/Tech
    SELECT
        'Python' as skill_name,
        'python|py |\.py' as skill_pattern
    UNION ALL SELECT 'SQL', 'sql|sql|sql server|postgres|oracle'
    UNION ALL SELECT 'Spark', 'spark|pyspark'
    UNION ALL SELECT 'Hadoop', 'hadoop|hdfs'
    UNION ALL SELECT 'Scala', 'scala'
    UNION ALL SELECT 'Java', '\bjava\b'
    UNION ALL SELECT 'R', '\br\b|r programming'
    UNION ALL SELECT 'Tableau', 'tableau'
    UNION ALL SELECT 'Power BI', 'power bi|powerbi'
    UNION ALL SELECT 'Looker', 'looker'
    UNION ALL SELECT 'AWS', 'aws|amazon web|s3 |ec2|redshift'
    UNION ALL SELECT 'Azure', 'azure|microsoft azure|synapse|cosmos'
    UNION ALL SELECT 'GCP', 'gcp|google cloud|bigquery'
    UNION ALL SELECT 'Airflow', 'airflow'
    UNION ALL SELECT 'DBT', '\bdbt\b|dbt'
    UNION ALL SELECT 'Kubernetes', 'kubernetes|k8s'
    UNION ALL SELECT 'Docker', 'docker'
    UNION ALL SELECT 'Git', 'git|github|gitlab'
    UNION ALL SELECT 'TensorFlow', 'tensorflow'
    UNION ALL SELECT 'PyTorch', 'pytorch'
    UNION ALL SELECT 'Scikit-learn', 'scikit|sklearn'
    UNION ALL SELECT 'Pandas', 'pandas'
    UNION ALL SELECT 'NumPy', 'numpy'
    UNION ALL SELECT 'Machine Learning', 'machine learning|deep learning|ml|artificial intelligence'
    UNION ALL SELECT 'Statistics', 'statistics|statistical|probability'
    UNION ALL SELECT 'Data Visualization', 'data visualization|visualization|charts|graphs'
),

jobs_exploded AS (
    SELECT
        j.*,
        s.skill_name,
        CASE 
            WHEN job_description_cleaned ~* s.skill_pattern THEN 1 
            ELSE 0 
        END as has_skill
    FROM jobs_with_titles j
    CROSS JOIN skills_mapping s
)

SELECT
    *
FROM jobs_exploded
WHERE has_skill = 1

ORDER BY job_title_cleaned, published_date DESC
    );
  
  
[0m11:21:09.365800 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.dim_location"
[0m11:21:09.366674 [debug] [Thread-2 (]: Using duckdb connection "model.job_intelligent.dim_time"
[0m11:21:09.368078 [debug] [Thread-1 (]: On model.job_intelligent.dim_location: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_location"} */

  
    
    

    create  table
      "memory"."main_gold"."dim_location__dbt_tmp"
  
    as (
      -- models/gold/dim_location.sql
-- Gold layer: Dimension Location



WITH jobs AS (
    SELECT DISTINCT
        location_cleaned as location_raw
    FROM "memory"."main_silver"."int_job_title_normalization"
    WHERE location_cleaned IS NOT NULL
),

location_parsed AS (
    SELECT
        location_raw,
        -- Extract city (before comma)
        CASE 
            WHEN POSITION(',' IN location_raw) > 0 
            THEN TRIM(SUBSTRING(location_raw, 1, POSITION(',' IN location_raw) - 1))
            ELSE location_raw
        END as city,
        
        -- Extract country (after comma)
        CASE 
            WHEN POSITION(',' IN location_raw) > 0 
            THEN TRIM(SUBSTRING(location_raw, POSITION(',' IN location_raw) + 1))
            ELSE 'Not Specified'
        END as country,
        
        -- Detect if remote
        CASE 
            WHEN location_raw LIKE '%remote%' 
            THEN 'Remote'
            ELSE 'On-site'
        END as work_location_type
    FROM jobs
),

ranked_locations AS (
    SELECT
        ROW_NUMBER() OVER (ORDER BY location_raw) as location_id,
        location_raw,
        city,
        country,
        work_location_type,
        CURRENT_TIMESTAMP() as created_at
    FROM location_parsed
)

SELECT
    location_id,
    location_raw,
    city,
    country,
    work_location_type,
    created_at
FROM ranked_locations
ORDER BY location_id
    );
  
  
[0m11:21:09.368939 [debug] [Thread-2 (]: On model.job_intelligent.dim_time: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_time"} */

  
    
    

    create  table
      "memory"."main_gold"."dim_time__dbt_tmp"
  
    as (
      -- models/gold/dim_time.sql
-- Gold layer: Dimension Time



-- Generate a date dimension for time-based analysis
WITH date_spine AS (
    SELECT
        published_date,
        EXTRACT(YEAR FROM published_date) as year,
        EXTRACT(MONTH FROM published_date) as month,
        EXTRACT(QUARTER FROM published_date) as quarter,
        EXTRACT(WEEK FROM published_date) as week,
        EXTRACT(DAYOFWEEK FROM published_date) as day_of_week,
        DATE_TRUNC('month', published_date) as month_start,
        DATE_TRUNC('quarter', published_date) as quarter_start,
        DATE_TRUNC('year', published_date) as year_start,
        
        CASE 
            WHEN EXTRACT(MONTH FROM published_date) IN (1,2,3) THEN 'Q1'
            WHEN EXTRACT(MONTH FROM published_date) IN (4,5,6) THEN 'Q2'
            WHEN EXTRACT(MONTH FROM published_date) IN (7,8,9) THEN 'Q3'
            ELSE 'Q4'
        END as quarter_name,
        
        CASE EXTRACT(DAYOFWEEK FROM published_date)
            WHEN 0 THEN 'Sunday'
            WHEN 1 THEN 'Monday'
            WHEN 2 THEN 'Tuesday'
            WHEN 3 THEN 'Wednesday'
            WHEN 4 THEN 'Thursday'
            WHEN 5 THEN 'Friday'
            WHEN 6 THEN 'Saturday'
        END as day_name,
        
        CASE EXTRACT(MONTH FROM published_date)
            WHEN 1 THEN 'January'
            WHEN 2 THEN 'February'
            WHEN 3 THEN 'March'
            WHEN 4 THEN 'April'
            WHEN 5 THEN 'May'
            WHEN 6 THEN 'June'
            WHEN 7 THEN 'July'
            WHEN 8 THEN 'August'
            WHEN 9 THEN 'September'
            WHEN 10 THEN 'October'
            WHEN 11 THEN 'November'
            WHEN 12 THEN 'December'
        END as month_name
        
    FROM (
        SELECT DISTINCT published_date
        FROM "memory"."main_silver"."int_job_title_normalization"
        WHERE published_date IS NOT NULL
    )
)

SELECT
    published_date as date_id,
    year,
    month,
    quarter,
    week,
    day_of_week,
    day_name,
    month_name,
    quarter_name,
    month_start,
    quarter_start,
    year_start,
    CURRENT_TIMESTAMP() as created_at
FROM date_spine
ORDER BY published_date
    );
  
  
[0m11:21:09.371305 [debug] [Thread-4 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_company"} */

  
    
    

    create  table
      "memory"."main_gold"."dim_company__dbt_tmp"
  
    as (
      -- models/gold/dim_company.sql
-- Gold layer: Dimension Company



WITH jobs AS (
    SELECT DISTINCT
        company_name_cleaned,
        company_url
    FROM "memory"."main_silver"."int_job_title_normalization"
    WHERE company_name_cleaned IS NOT NULL
),

ranked_companies AS (
    SELECT
        ROW_NUMBER() OVER (ORDER BY company_name_cleaned) as company_id,
        company_name_cleaned as company_name,
        company_url,
        CURRENT_TIMESTAMP() as created_at
    FROM jobs
)

SELECT
    company_id,
    company_name,
    company_url,
    created_at
FROM ranked_companies
ORDER BY company_id
    );
  
  
[0m11:21:09.372263 [debug] [Thread-3 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.int_skills_extraction"} */

  
    
    

    create  table
      "memory"."main_silver"."int_skills_extraction__dbt_tmp"
  
    as (
      -- models/silver/int_skills_extraction.sql
-- Silver layer: Extraction des compétences depuis la description



WITH jobs_with_titles AS (
    SELECT * FROM "memory"."main_silver"."int_job_title_normalization"
),

skills_mapping AS (
    -- Définir un mapping de compétences communes Data/Tech
    SELECT
        'Python' as skill_name,
        'python|py |\.py' as skill_pattern
    UNION ALL SELECT 'SQL', 'sql|sql|sql server|postgres|oracle'
    UNION ALL SELECT 'Spark', 'spark|pyspark'
    UNION ALL SELECT 'Hadoop', 'hadoop|hdfs'
    UNION ALL SELECT 'Scala', 'scala'
    UNION ALL SELECT 'Java', '\bjava\b'
    UNION ALL SELECT 'R', '\br\b|r programming'
    UNION ALL SELECT 'Tableau', 'tableau'
    UNION ALL SELECT 'Power BI', 'power bi|powerbi'
    UNION ALL SELECT 'Looker', 'looker'
    UNION ALL SELECT 'AWS', 'aws|amazon web|s3 |ec2|redshift'
    UNION ALL SELECT 'Azure', 'azure|microsoft azure|synapse|cosmos'
    UNION ALL SELECT 'GCP', 'gcp|google cloud|bigquery'
    UNION ALL SELECT 'Airflow', 'airflow'
    UNION ALL SELECT 'DBT', '\bdbt\b|dbt'
    UNION ALL SELECT 'Kubernetes', 'kubernetes|k8s'
    UNION ALL SELECT 'Docker', 'docker'
    UNION ALL SELECT 'Git', 'git|github|gitlab'
    UNION ALL SELECT 'TensorFlow', 'tensorflow'
    UNION ALL SELECT 'PyTorch', 'pytorch'
    UNION ALL SELECT 'Scikit-learn', 'scikit|sklearn'
    UNION ALL SELECT 'Pandas', 'pandas'
    UNION ALL SELECT 'NumPy', 'numpy'
    UNION ALL SELECT 'Machine Learning', 'machine learning|deep learning|ml|artificial intelligence'
    UNION ALL SELECT 'Statistics', 'statistics|statistical|probability'
    UNION ALL SELECT 'Data Visualization', 'data visualization|visualization|charts|graphs'
),

jobs_exploded AS (
    SELECT
        j.*,
        s.skill_name,
        CASE 
            WHEN job_description_cleaned ~* s.skill_pattern THEN 1 
            ELSE 0 
        END as has_skill
    FROM jobs_with_titles j
    CROSS JOIN skills_mapping s
)

SELECT
    *
FROM jobs_exploded
WHERE has_skill = 1

ORDER BY job_title_cleaned, published_date DESC
    );
  
  
[0m11:21:09.373009 [debug] [Thread-4 (]: DuckDB adapter: Rolling back transaction.
[0m11:21:09.373710 [debug] [Thread-1 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_location"} */

  
    
    

    create  table
      "memory"."main_gold"."dim_location__dbt_tmp"
  
    as (
      -- models/gold/dim_location.sql
-- Gold layer: Dimension Location



WITH jobs AS (
    SELECT DISTINCT
        location_cleaned as location_raw
    FROM "memory"."main_silver"."int_job_title_normalization"
    WHERE location_cleaned IS NOT NULL
),

location_parsed AS (
    SELECT
        location_raw,
        -- Extract city (before comma)
        CASE 
            WHEN POSITION(',' IN location_raw) > 0 
            THEN TRIM(SUBSTRING(location_raw, 1, POSITION(',' IN location_raw) - 1))
            ELSE location_raw
        END as city,
        
        -- Extract country (after comma)
        CASE 
            WHEN POSITION(',' IN location_raw) > 0 
            THEN TRIM(SUBSTRING(location_raw, POSITION(',' IN location_raw) + 1))
            ELSE 'Not Specified'
        END as country,
        
        -- Detect if remote
        CASE 
            WHEN location_raw LIKE '%remote%' 
            THEN 'Remote'
            ELSE 'On-site'
        END as work_location_type
    FROM jobs
),

ranked_locations AS (
    SELECT
        ROW_NUMBER() OVER (ORDER BY location_raw) as location_id,
        location_raw,
        city,
        country,
        work_location_type,
        CURRENT_TIMESTAMP() as created_at
    FROM location_parsed
)

SELECT
    location_id,
    location_raw,
    city,
    country,
    work_location_type,
    created_at
FROM ranked_locations
ORDER BY location_id
    );
  
  
[0m11:21:09.374487 [debug] [Thread-2 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_time"} */

  
    
    

    create  table
      "memory"."main_gold"."dim_time__dbt_tmp"
  
    as (
      -- models/gold/dim_time.sql
-- Gold layer: Dimension Time



-- Generate a date dimension for time-based analysis
WITH date_spine AS (
    SELECT
        published_date,
        EXTRACT(YEAR FROM published_date) as year,
        EXTRACT(MONTH FROM published_date) as month,
        EXTRACT(QUARTER FROM published_date) as quarter,
        EXTRACT(WEEK FROM published_date) as week,
        EXTRACT(DAYOFWEEK FROM published_date) as day_of_week,
        DATE_TRUNC('month', published_date) as month_start,
        DATE_TRUNC('quarter', published_date) as quarter_start,
        DATE_TRUNC('year', published_date) as year_start,
        
        CASE 
            WHEN EXTRACT(MONTH FROM published_date) IN (1,2,3) THEN 'Q1'
            WHEN EXTRACT(MONTH FROM published_date) IN (4,5,6) THEN 'Q2'
            WHEN EXTRACT(MONTH FROM published_date) IN (7,8,9) THEN 'Q3'
            ELSE 'Q4'
        END as quarter_name,
        
        CASE EXTRACT(DAYOFWEEK FROM published_date)
            WHEN 0 THEN 'Sunday'
            WHEN 1 THEN 'Monday'
            WHEN 2 THEN 'Tuesday'
            WHEN 3 THEN 'Wednesday'
            WHEN 4 THEN 'Thursday'
            WHEN 5 THEN 'Friday'
            WHEN 6 THEN 'Saturday'
        END as day_name,
        
        CASE EXTRACT(MONTH FROM published_date)
            WHEN 1 THEN 'January'
            WHEN 2 THEN 'February'
            WHEN 3 THEN 'March'
            WHEN 4 THEN 'April'
            WHEN 5 THEN 'May'
            WHEN 6 THEN 'June'
            WHEN 7 THEN 'July'
            WHEN 8 THEN 'August'
            WHEN 9 THEN 'September'
            WHEN 10 THEN 'October'
            WHEN 11 THEN 'November'
            WHEN 12 THEN 'December'
        END as month_name
        
    FROM (
        SELECT DISTINCT published_date
        FROM "memory"."main_silver"."int_job_title_normalization"
        WHERE published_date IS NOT NULL
    )
)

SELECT
    published_date as date_id,
    year,
    month,
    quarter,
    week,
    day_of_week,
    day_name,
    month_name,
    quarter_name,
    month_start,
    quarter_start,
    year_start,
    CURRENT_TIMESTAMP() as created_at
FROM date_spine
ORDER BY published_date
    );
  
  
[0m11:21:09.375230 [debug] [Thread-3 (]: DuckDB adapter: Rolling back transaction.
[0m11:21:09.375997 [debug] [Thread-4 (]: On model.job_intelligent.dim_company: ROLLBACK
[0m11:21:09.376890 [debug] [Thread-1 (]: DuckDB adapter: Rolling back transaction.
[0m11:21:09.378007 [debug] [Thread-2 (]: DuckDB adapter: Rolling back transaction.
[0m11:21:09.379293 [debug] [Thread-3 (]: On model.job_intelligent.int_skills_extraction: ROLLBACK
[0m11:21:09.380784 [debug] [Thread-1 (]: On model.job_intelligent.dim_location: ROLLBACK
[0m11:21:09.382307 [debug] [Thread-2 (]: On model.job_intelligent.dim_time: ROLLBACK
[0m11:21:09.425779 [debug] [Thread-1 (]: Failed to rollback 'model.job_intelligent.dim_location'
[0m11:21:09.432868 [debug] [Thread-1 (]: On model.job_intelligent.dim_location: Close
[0m11:21:09.441003 [debug] [Thread-3 (]: Failed to rollback 'model.job_intelligent.int_skills_extraction'
[0m11:21:09.441933 [debug] [Thread-3 (]: On model.job_intelligent.int_skills_extraction: Close
[0m11:21:09.445636 [debug] [Thread-4 (]: Failed to rollback 'model.job_intelligent.dim_company'
[0m11:21:09.446668 [debug] [Thread-4 (]: On model.job_intelligent.dim_company: Close
[0m11:21:09.459136 [debug] [Thread-2 (]: Failed to rollback 'model.job_intelligent.dim_time'
[0m11:21:09.460185 [debug] [Thread-2 (]: On model.job_intelligent.dim_time: Close
[0m11:21:09.472140 [debug] [Thread-1 (]: Runtime Error in model dim_location (models\gold\dim_location.sql)
  Catalog Error: Scalar Function with name current_timestamp does not exist!
  Did you mean "current_localtimestamp"?
  
  LINE 56:         CURRENT_TIMESTAMP() as created_at
                   ^
[0m11:21:09.480550 [debug] [Thread-4 (]: Runtime Error in model dim_company (models\gold\dim_company.sql)
  Catalog Error: Scalar Function with name current_timestamp does not exist!
  Did you mean "current_localtimestamp"?
  
  LINE 29:         CURRENT_TIMESTAMP() as created_at
                   ^
[0m11:21:09.481808 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '18a2aaad-b799-431b-8eb9-eb0ab248d18b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DD03F822D0>]}
[0m11:21:09.489676 [debug] [Thread-3 (]: Runtime Error in model int_skills_extraction (models\silver\int_skills_extraction.sql)
  Catalog Error: Scalar Function with name ~* does not exist!
  Did you mean "~"?
  
  LINE 57:             WHEN job_description_cleaned ~* s.skill_pattern THEN 1 
                                                    ^
[0m11:21:09.491017 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '18a2aaad-b799-431b-8eb9-eb0ab248d18b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DD03F51550>]}
[0m11:21:09.498643 [debug] [Thread-2 (]: Runtime Error in model dim_time (models\gold\dim_time.sql)
  Catalog Error: Scalar Function with name current_timestamp does not exist!
  Did you mean "current_localtimestamp"?
  
  LINE 81:     CURRENT_TIMESTAMP() as created_at
               ^
[0m11:21:09.500851 [error] [Thread-1 (]: 5 of 13 ERROR creating sql table model main_gold.dim_location .................. [[31mERROR[0m in 0.16s]
[0m11:21:09.503079 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '18a2aaad-b799-431b-8eb9-eb0ab248d18b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DD04088050>]}
[0m11:21:09.504721 [error] [Thread-4 (]: 4 of 13 ERROR creating sql table model main_gold.dim_company ................... [[31mERROR[0m in 0.18s]
[0m11:21:09.506352 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '18a2aaad-b799-431b-8eb9-eb0ab248d18b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DD026E7950>]}
[0m11:21:09.508002 [debug] [Thread-1 (]: Finished running node model.job_intelligent.dim_location
[0m11:21:09.509592 [error] [Thread-3 (]: 7 of 13 ERROR creating sql table model main_silver.int_skills_extraction ....... [[31mERROR[0m in 0.19s]
[0m11:21:09.511406 [debug] [Thread-4 (]: Finished running node model.job_intelligent.dim_company
[0m11:21:09.513208 [error] [Thread-2 (]: 6 of 13 ERROR creating sql table model main_gold.dim_time ...................... [[31mERROR[0m in 0.18s]
[0m11:21:09.515444 [debug] [Thread-7 (]: Marking all children of 'model.job_intelligent.dim_location' to be skipped because of status 'error'.  Reason: Runtime Error in model dim_location (models\gold\dim_location.sql)
  Catalog Error: Scalar Function with name current_timestamp does not exist!
  Did you mean "current_localtimestamp"?
  
  LINE 56:         CURRENT_TIMESTAMP() as created_at
                   ^.
[0m11:21:09.517152 [debug] [Thread-3 (]: Finished running node model.job_intelligent.int_skills_extraction
[0m11:21:09.518823 [debug] [Thread-2 (]: Finished running node model.job_intelligent.dim_time
[0m11:21:09.521644 [debug] [Thread-7 (]: Marking all children of 'model.job_intelligent.dim_company' to be skipped because of status 'error'.  Reason: Runtime Error in model dim_company (models\gold\dim_company.sql)
  Catalog Error: Scalar Function with name current_timestamp does not exist!
  Did you mean "current_localtimestamp"?
  
  LINE 29:         CURRENT_TIMESTAMP() as created_at
                   ^.
[0m11:21:09.523648 [debug] [Thread-7 (]: Marking all children of 'model.job_intelligent.int_skills_extraction' to be skipped because of status 'error'.  Reason: Runtime Error in model int_skills_extraction (models\silver\int_skills_extraction.sql)
  Catalog Error: Scalar Function with name ~* does not exist!
  Did you mean "~"?
  
  LINE 57:             WHEN job_description_cleaned ~* s.skill_pattern THEN 1 
                                                    ^.
[0m11:21:09.525173 [debug] [Thread-7 (]: Marking all children of 'model.job_intelligent.dim_time' to be skipped because of status 'error'.  Reason: Runtime Error in model dim_time (models\gold\dim_time.sql)
  Catalog Error: Scalar Function with name current_timestamp does not exist!
  Did you mean "current_localtimestamp"?
  
  LINE 81:     CURRENT_TIMESTAMP() as created_at
               ^.
[0m11:21:09.526700 [debug] [Thread-1 (]: Began running node model.job_intelligent.dim_skills
[0m11:21:09.527784 [debug] [Thread-4 (]: Began running node model.job_intelligent.fact_job_offers
[0m11:21:09.529083 [info ] [Thread-1 (]: 8 of 13 SKIP relation main_gold.dim_skills ..................................... [[33mSKIP[0m]
[0m11:21:09.530595 [info ] [Thread-4 (]: 9 of 13 SKIP relation main_gold.fact_job_offers ................................ [[33mSKIP[0m]
[0m11:21:09.531965 [debug] [Thread-1 (]: Finished running node model.job_intelligent.dim_skills
[0m11:21:09.533138 [debug] [Thread-4 (]: Finished running node model.job_intelligent.fact_job_offers
[0m11:21:09.535364 [debug] [Thread-2 (]: Began running node model.job_intelligent.agg_location_analysis
[0m11:21:09.536429 [debug] [Thread-3 (]: Began running node model.job_intelligent.agg_job_offers_by_category_time
[0m11:21:09.537468 [debug] [Thread-1 (]: Began running node model.job_intelligent.fact_job_skills
[0m11:21:09.538562 [info ] [Thread-2 (]: 11 of 13 SKIP relation main_gold.agg_location_analysis ......................... [[33mSKIP[0m]
[0m11:21:09.540089 [info ] [Thread-3 (]: 10 of 13 SKIP relation main_gold.agg_job_offers_by_category_time ............... [[33mSKIP[0m]
[0m11:21:09.542228 [info ] [Thread-1 (]: 12 of 13 SKIP relation main_gold.fact_job_skills ............................... [[33mSKIP[0m]
[0m11:21:09.543574 [debug] [Thread-2 (]: Finished running node model.job_intelligent.agg_location_analysis
[0m11:21:09.544845 [debug] [Thread-3 (]: Finished running node model.job_intelligent.agg_job_offers_by_category_time
[0m11:21:09.546001 [debug] [Thread-1 (]: Finished running node model.job_intelligent.fact_job_skills
[0m11:21:09.548386 [debug] [Thread-4 (]: Began running node model.job_intelligent.agg_skills_demand
[0m11:21:09.549437 [info ] [Thread-4 (]: 13 of 13 SKIP relation main_gold.agg_skills_demand ............................. [[33mSKIP[0m]
[0m11:21:09.550657 [debug] [Thread-4 (]: Finished running node model.job_intelligent.agg_skills_demand
[0m11:21:09.553992 [debug] [MainThread]: Using duckdb connection "master"
[0m11:21:09.554872 [debug] [MainThread]: On master: BEGIN
[0m11:21:09.555627 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m11:21:09.557182 [debug] [MainThread]: SQL status: OK in 0.002 seconds
[0m11:21:09.557979 [debug] [MainThread]: On master: COMMIT
[0m11:21:09.558859 [debug] [MainThread]: Using duckdb connection "master"
[0m11:21:09.559674 [debug] [MainThread]: On master: COMMIT
[0m11:21:09.560976 [debug] [MainThread]: SQL status: OK in 0.001 seconds
[0m11:21:09.561786 [debug] [MainThread]: On master: Close
[0m11:21:09.562762 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:21:09.563478 [debug] [MainThread]: Connection 'create_memory_main_silver' was properly closed.
[0m11:21:09.564191 [debug] [MainThread]: Connection 'create_memory_main_bronze' was properly closed.
[0m11:21:09.564865 [debug] [MainThread]: Connection 'create_memory_main_gold' was properly closed.
[0m11:21:09.565529 [debug] [MainThread]: Connection 'list_memory_main_silver' was properly closed.
[0m11:21:09.566219 [debug] [MainThread]: Connection 'list_memory_main_gold' was properly closed.
[0m11:21:09.566880 [debug] [MainThread]: Connection 'list_memory_main_bronze' was properly closed.
[0m11:21:09.567540 [debug] [MainThread]: Connection 'model.job_intelligent.dim_location' was properly closed.
[0m11:21:09.568227 [debug] [MainThread]: Connection 'model.job_intelligent.dim_time' was properly closed.
[0m11:21:09.568890 [debug] [MainThread]: Connection 'model.job_intelligent.int_skills_extraction' was properly closed.
[0m11:21:09.569555 [debug] [MainThread]: Connection 'model.job_intelligent.dim_company' was properly closed.
[0m11:21:09.570509 [info ] [MainThread]: 
[0m11:21:09.571407 [info ] [MainThread]: Finished running 12 table models, 1 view model in 0 hours 0 minutes and 1.10 seconds (1.10s).
[0m11:21:09.575996 [debug] [MainThread]: Command end result
[0m11:21:09.607777 [debug] [MainThread]: Wrote artifact WritableManifest to D:\lab2\dbt_project\target\manifest.json
[0m11:21:09.610032 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\lab2\dbt_project\target\semantic_manifest.json
[0m11:21:09.615571 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\lab2\dbt_project\target\run_results.json
[0m11:21:09.615919 [info ] [MainThread]: 
[0m11:21:09.616272 [info ] [MainThread]: [31mCompleted with 4 errors, 0 partial successes, and 0 warnings:[0m
[0m11:21:09.616584 [info ] [MainThread]: 
[0m11:21:09.616945 [error] [MainThread]: [31mFailure in model dim_location (models\gold\dim_location.sql)[0m
[0m11:21:09.617308 [error] [MainThread]:   Runtime Error in model dim_location (models\gold\dim_location.sql)
  Catalog Error: Scalar Function with name current_timestamp does not exist!
  Did you mean "current_localtimestamp"?
  
  LINE 56:         CURRENT_TIMESTAMP() as created_at
                   ^
[0m11:21:09.617602 [info ] [MainThread]: 
[0m11:21:09.617936 [info ] [MainThread]:   compiled code at target\compiled\job_intelligent\models\gold\dim_location.sql
[0m11:21:09.618234 [info ] [MainThread]: 
[0m11:21:09.618604 [error] [MainThread]: [31mFailure in model dim_company (models\gold\dim_company.sql)[0m
[0m11:21:09.618963 [error] [MainThread]:   Runtime Error in model dim_company (models\gold\dim_company.sql)
  Catalog Error: Scalar Function with name current_timestamp does not exist!
  Did you mean "current_localtimestamp"?
  
  LINE 29:         CURRENT_TIMESTAMP() as created_at
                   ^
[0m11:21:09.619294 [info ] [MainThread]: 
[0m11:21:09.619753 [info ] [MainThread]:   compiled code at target\compiled\job_intelligent\models\gold\dim_company.sql
[0m11:21:09.620046 [info ] [MainThread]: 
[0m11:21:09.620402 [error] [MainThread]: [31mFailure in model int_skills_extraction (models\silver\int_skills_extraction.sql)[0m
[0m11:21:09.620767 [error] [MainThread]:   Runtime Error in model int_skills_extraction (models\silver\int_skills_extraction.sql)
  Catalog Error: Scalar Function with name ~* does not exist!
  Did you mean "~"?
  
  LINE 57:             WHEN job_description_cleaned ~* s.skill_pattern THEN 1 
                                                    ^
[0m11:21:09.621063 [info ] [MainThread]: 
[0m11:21:09.621398 [info ] [MainThread]:   compiled code at target\compiled\job_intelligent\models\silver\int_skills_extraction.sql
[0m11:21:09.621684 [info ] [MainThread]: 
[0m11:21:09.622034 [error] [MainThread]: [31mFailure in model dim_time (models\gold\dim_time.sql)[0m
[0m11:21:09.622467 [error] [MainThread]:   Runtime Error in model dim_time (models\gold\dim_time.sql)
  Catalog Error: Scalar Function with name current_timestamp does not exist!
  Did you mean "current_localtimestamp"?
  
  LINE 81:     CURRENT_TIMESTAMP() as created_at
               ^
[0m11:21:09.622759 [info ] [MainThread]: 
[0m11:21:09.623092 [info ] [MainThread]:   compiled code at target\compiled\job_intelligent\models\gold\dim_time.sql
[0m11:21:09.623384 [info ] [MainThread]: 
[0m11:21:09.623693 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=4 SKIP=6 NO-OP=0 TOTAL=13
[0m11:21:09.624520 [debug] [MainThread]: Command `dbt run` failed at 11:21:09.624418 after 2.40 seconds
[0m11:21:09.624865 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DD7F2B23F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DD03B87FB0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DD026E4590>]}
[0m11:21:09.625193 [debug] [MainThread]: Flushing usage events
[0m11:21:10.882415 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:24:29.581046 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E4DA735FD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E4D8444190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E4D82B7890>]}


============================== 11:24:29.591937 | 72bca6db-7fe3-464c-856e-a87dd623f887 ==============================
[0m11:24:29.591937 [info ] [MainThread]: Running with dbt=1.10.13
[0m11:24:29.593193 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'target_path': 'None', 'cache_selected_only': 'False', 'warn_error': 'None', 'profiles_dir': 'D:\\lab2\\dbt_project', 'debug': 'False', 'static_parser': 'True', 'log_cache_events': 'False', 'empty': 'False', 'log_format': 'default', 'quiet': 'False', 'use_colors': 'True', 'partial_parse': 'True', 'version_check': 'True', 'log_path': 'D:\\lab2\\dbt_project\\logs', 'introspect': 'True', 'no_print': 'None', 'fail_fast': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'invocation_command': 'dbt run', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'write_json': 'True'}
[0m11:24:29.952040 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '72bca6db-7fe3-464c-856e-a87dd623f887', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E4DABEF230>]}
[0m11:24:30.033477 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '72bca6db-7fe3-464c-856e-a87dd623f887', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E4DA7A8380>]}
[0m11:24:30.036828 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m11:24:30.306274 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m11:24:30.398961 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m11:24:30.399314 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m11:24:30.405621 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- seeds
[0m11:24:30.424606 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '72bca6db-7fe3-464c-856e-a87dd623f887', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E4DC9E4850>]}
[0m11:24:30.494779 [debug] [MainThread]: Wrote artifact WritableManifest to D:\lab2\dbt_project\target\manifest.json
[0m11:24:30.498360 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\lab2\dbt_project\target\semantic_manifest.json
[0m11:24:30.547706 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '72bca6db-7fe3-464c-856e-a87dd623f887', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E4DB72B7A0>]}
[0m11:24:30.548417 [info ] [MainThread]: Found 13 models, 456 macros
[0m11:24:30.548995 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '72bca6db-7fe3-464c-856e-a87dd623f887', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E4DCF95470>]}
[0m11:24:30.552032 [info ] [MainThread]: 
[0m11:24:30.552706 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m11:24:30.553391 [info ] [MainThread]: 
[0m11:24:30.554611 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m11:24:30.563187 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_memory'
[0m11:24:30.568365 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_memory'
[0m11:24:30.584008 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_memory'
[0m11:24:30.728450 [debug] [ThreadPool]: Using duckdb connection "list_memory"
[0m11:24:30.729145 [debug] [ThreadPool]: Using duckdb connection "list_memory"
[0m11:24:30.729704 [debug] [ThreadPool]: On list_memory: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_memory"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"memory"'
    
  
  
[0m11:24:30.730345 [debug] [ThreadPool]: Using duckdb connection "list_memory"
[0m11:24:30.731094 [debug] [ThreadPool]: On list_memory: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_memory"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"memory"'
    
  
  
[0m11:24:30.731738 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:24:30.732422 [debug] [ThreadPool]: On list_memory: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_memory"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"memory"'
    
  
  
[0m11:24:30.733093 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:24:30.734051 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:24:30.756937 [debug] [ThreadPool]: SQL status: OK in 0.024 seconds
[0m11:24:30.757429 [debug] [ThreadPool]: SQL status: OK in 0.026 seconds
[0m11:24:30.758021 [debug] [ThreadPool]: SQL status: OK in 0.024 seconds
[0m11:24:30.759627 [debug] [ThreadPool]: On list_memory: Close
[0m11:24:30.761707 [debug] [ThreadPool]: On list_memory: Close
[0m11:24:30.764522 [debug] [ThreadPool]: On list_memory: Close
[0m11:24:30.767314 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_memory, now create_memory_main_bronze)
[0m11:24:30.768350 [debug] [ThreadPool]: Creating schema "database: "memory"
schema: "main_bronze"
"
[0m11:24:30.769585 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_memory, now create_memory_main_silver)
[0m11:24:30.783170 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_memory, now create_memory_main_gold)
[0m11:24:30.784489 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_bronze"
[0m11:24:30.785848 [debug] [ThreadPool]: Creating schema "database: "memory"
schema: "main_silver"
"
[0m11:24:30.787212 [debug] [ThreadPool]: Creating schema "database: "memory"
schema: "main_gold"
"
[0m11:24:30.788193 [debug] [ThreadPool]: On create_memory_main_bronze: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_memory_main_bronze"} */

    
        select type from duckdb_databases()
        where lower(database_name)='memory'
        and type='sqlite'
    
  
[0m11:24:30.793161 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_silver"
[0m11:24:30.796972 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_gold"
[0m11:24:30.797988 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:24:30.798976 [debug] [ThreadPool]: On create_memory_main_silver: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_memory_main_silver"} */

    
        select type from duckdb_databases()
        where lower(database_name)='memory'
        and type='sqlite'
    
  
[0m11:24:30.799998 [debug] [ThreadPool]: On create_memory_main_gold: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_memory_main_gold"} */

    
        select type from duckdb_databases()
        where lower(database_name)='memory'
        and type='sqlite'
    
  
[0m11:24:30.801552 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:24:30.802683 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:24:30.804308 [debug] [ThreadPool]: SQL status: OK in 0.006 seconds
[0m11:24:30.806003 [debug] [ThreadPool]: SQL status: OK in 0.004 seconds
[0m11:24:30.809121 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_bronze"
[0m11:24:30.812225 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_silver"
[0m11:24:30.813186 [debug] [ThreadPool]: On create_memory_main_bronze: BEGIN
[0m11:24:30.814184 [debug] [ThreadPool]: SQL status: OK in 0.011 seconds
[0m11:24:30.815203 [debug] [ThreadPool]: On create_memory_main_silver: BEGIN
[0m11:24:30.818931 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_gold"
[0m11:24:30.820434 [debug] [ThreadPool]: SQL status: OK in 0.004 seconds
[0m11:24:30.821331 [debug] [ThreadPool]: On create_memory_main_gold: BEGIN
[0m11:24:30.822320 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_bronze"
[0m11:24:30.823166 [debug] [ThreadPool]: SQL status: OK in 0.003 seconds
[0m11:24:30.824530 [debug] [ThreadPool]: On create_memory_main_bronze: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_memory_main_bronze"} */

    
    
        create schema if not exists "memory"."main_bronze"
    
[0m11:24:30.825538 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m11:24:30.826413 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_silver"
[0m11:24:30.827772 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_gold"
[0m11:24:30.828762 [debug] [ThreadPool]: On create_memory_main_silver: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_memory_main_silver"} */

    
    
        create schema if not exists "memory"."main_silver"
    
[0m11:24:30.829767 [debug] [ThreadPool]: On create_memory_main_gold: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_memory_main_gold"} */

    
    
        create schema if not exists "memory"."main_gold"
    
[0m11:24:30.830670 [debug] [ThreadPool]: SQL status: OK in 0.003 seconds
[0m11:24:30.833769 [debug] [ThreadPool]: On create_memory_main_bronze: COMMIT
[0m11:24:30.834620 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_bronze"
[0m11:24:30.835668 [debug] [ThreadPool]: On create_memory_main_bronze: COMMIT
[0m11:24:30.836575 [debug] [ThreadPool]: SQL status: OK in 0.004 seconds
[0m11:24:30.837583 [debug] [ThreadPool]: SQL status: OK in 0.006 seconds
[0m11:24:30.840348 [debug] [ThreadPool]: On create_memory_main_silver: COMMIT
[0m11:24:30.841368 [debug] [ThreadPool]: SQL status: OK in 0.003 seconds
[0m11:24:30.843525 [debug] [ThreadPool]: On create_memory_main_gold: COMMIT
[0m11:24:30.844477 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_silver"
[0m11:24:30.845431 [debug] [ThreadPool]: On create_memory_main_bronze: Close
[0m11:24:30.846397 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_gold"
[0m11:24:30.847322 [debug] [ThreadPool]: On create_memory_main_silver: COMMIT
[0m11:24:30.849012 [debug] [ThreadPool]: On create_memory_main_gold: COMMIT
[0m11:24:30.850761 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m11:24:30.851799 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m11:24:30.852835 [debug] [ThreadPool]: On create_memory_main_silver: Close
[0m11:24:30.853892 [debug] [ThreadPool]: On create_memory_main_gold: Close
[0m11:24:30.860017 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_memory_main_bronze'
[0m11:24:30.867608 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_memory_main_silver'
[0m11:24:30.875391 [debug] [ThreadPool]: Using duckdb connection "list_memory_main_bronze"
[0m11:24:30.881943 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_memory_main_gold'
[0m11:24:30.880655 [debug] [ThreadPool]: Using duckdb connection "list_memory_main_silver"
[0m11:24:30.882932 [debug] [ThreadPool]: On list_memory_main_bronze: BEGIN
[0m11:24:30.888577 [debug] [ThreadPool]: Using duckdb connection "list_memory_main_gold"
[0m11:24:30.889873 [debug] [ThreadPool]: On list_memory_main_silver: BEGIN
[0m11:24:30.891011 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:24:30.892066 [debug] [ThreadPool]: On list_memory_main_gold: BEGIN
[0m11:24:30.892952 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:24:30.894404 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:24:30.895437 [debug] [ThreadPool]: SQL status: OK in 0.004 seconds
[0m11:24:30.897161 [debug] [ThreadPool]: Using duckdb connection "list_memory_main_bronze"
[0m11:24:30.898023 [debug] [ThreadPool]: On list_memory_main_bronze: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_memory_main_bronze"} */
select
      'memory' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main_bronze'
    and lower(table_catalog) = 'memory'
  
[0m11:24:30.899215 [debug] [ThreadPool]: SQL status: OK in 0.006 seconds
[0m11:24:30.900284 [debug] [ThreadPool]: SQL status: OK in 0.006 seconds
[0m11:24:30.901967 [debug] [ThreadPool]: Using duckdb connection "list_memory_main_silver"
[0m11:24:30.903551 [debug] [ThreadPool]: Using duckdb connection "list_memory_main_gold"
[0m11:24:30.904672 [debug] [ThreadPool]: On list_memory_main_silver: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_memory_main_silver"} */
select
      'memory' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main_silver'
    and lower(table_catalog) = 'memory'
  
[0m11:24:30.905639 [debug] [ThreadPool]: On list_memory_main_gold: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_memory_main_gold"} */
select
      'memory' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main_gold'
    and lower(table_catalog) = 'memory'
  
[0m11:24:30.933677 [debug] [ThreadPool]: SQL status: OK in 0.034 seconds
[0m11:24:30.934279 [debug] [ThreadPool]: SQL status: OK in 0.028 seconds
[0m11:24:30.934726 [debug] [ThreadPool]: SQL status: OK in 0.028 seconds
[0m11:24:30.936417 [debug] [ThreadPool]: On list_memory_main_silver: ROLLBACK
[0m11:24:30.937874 [debug] [ThreadPool]: On list_memory_main_bronze: ROLLBACK
[0m11:24:30.939222 [debug] [ThreadPool]: On list_memory_main_gold: ROLLBACK
[0m11:24:30.941348 [debug] [ThreadPool]: Failed to rollback 'list_memory_main_silver'
[0m11:24:30.941702 [debug] [ThreadPool]: On list_memory_main_silver: Close
[0m11:24:30.943016 [debug] [ThreadPool]: Failed to rollback 'list_memory_main_gold'
[0m11:24:30.943387 [debug] [ThreadPool]: On list_memory_main_gold: Close
[0m11:24:30.944601 [debug] [ThreadPool]: Failed to rollback 'list_memory_main_bronze'
[0m11:24:30.944927 [debug] [ThreadPool]: On list_memory_main_bronze: Close
[0m11:24:30.945905 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '72bca6db-7fe3-464c-856e-a87dd623f887', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E4DD1A3930>]}
[0m11:24:30.946369 [debug] [MainThread]: Using duckdb connection "master"
[0m11:24:30.946716 [debug] [MainThread]: On master: BEGIN
[0m11:24:30.947016 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:24:30.947736 [debug] [MainThread]: SQL status: OK in 0.001 seconds
[0m11:24:30.948058 [debug] [MainThread]: On master: COMMIT
[0m11:24:30.948375 [debug] [MainThread]: Using duckdb connection "master"
[0m11:24:30.948678 [debug] [MainThread]: On master: COMMIT
[0m11:24:30.949189 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m11:24:30.949523 [debug] [MainThread]: On master: Close
[0m11:24:30.955710 [debug] [Thread-1 (]: Began running node model.job_intelligent.stg_jobs_raw
[0m11:24:30.956344 [info ] [Thread-1 (]: 1 of 13 START sql view model main_bronze.stg_jobs_raw .......................... [RUN]
[0m11:24:30.956970 [debug] [Thread-1 (]: Acquiring new duckdb connection 'model.job_intelligent.stg_jobs_raw'
[0m11:24:30.957376 [debug] [Thread-1 (]: Began compiling node model.job_intelligent.stg_jobs_raw
[0m11:24:30.965812 [debug] [Thread-1 (]: Writing injected SQL for node "model.job_intelligent.stg_jobs_raw"
[0m11:24:30.966721 [debug] [Thread-1 (]: Began executing node model.job_intelligent.stg_jobs_raw
[0m11:24:30.999012 [debug] [Thread-1 (]: Writing runtime sql for node "model.job_intelligent.stg_jobs_raw"
[0m11:24:30.999991 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.stg_jobs_raw"
[0m11:24:31.000394 [debug] [Thread-1 (]: On model.job_intelligent.stg_jobs_raw: BEGIN
[0m11:24:31.000768 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m11:24:31.001923 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m11:24:31.002486 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.stg_jobs_raw"
[0m11:24:31.003013 [debug] [Thread-1 (]: On model.job_intelligent.stg_jobs_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.stg_jobs_raw"} */

  
  create view "memory"."main_bronze"."stg_jobs_raw__dbt_tmp" as (
    -- models/bronze/stg_jobs_raw.sql
-- Bronze layer: Lecture directe des données brutes depuis final_data.csv
-- Renommage et typage minimal



-- Read directly from CSV file
SELECT
    -- Renommer les colonnes pour cohérence
    title as job_title,
    location,
    postedTime as posted_time,
    publishedAt as published_at,
    jobUrl as job_url,
    companyName as company_name,
    companyUrl as company_url,
    description as job_description,
    contractType as contract_type,
    workType as work_type,
    
    -- Métadonnées de traçabilité
    NOW() as ingestion_timestamp,
    '2026-01-08 10:24:29.340416+00:00' as dbt_run_id

FROM read_csv_auto('D:/lab2/final_data.csv')

WHERE 1=1
    -- Filtrer les lignes vides
    AND title IS NOT NULL
    AND description IS NOT NULL
  );

[0m11:24:31.067977 [debug] [Thread-1 (]: SQL status: OK in 0.064 seconds
[0m11:24:31.079941 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.stg_jobs_raw"
[0m11:24:31.080754 [debug] [Thread-1 (]: On model.job_intelligent.stg_jobs_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.stg_jobs_raw"} */
alter view "memory"."main_bronze"."stg_jobs_raw__dbt_tmp" rename to "stg_jobs_raw"
[0m11:24:31.082260 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m11:24:31.097503 [debug] [Thread-1 (]: On model.job_intelligent.stg_jobs_raw: COMMIT
[0m11:24:31.098050 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.stg_jobs_raw"
[0m11:24:31.098479 [debug] [Thread-1 (]: On model.job_intelligent.stg_jobs_raw: COMMIT
[0m11:24:31.099320 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m11:24:31.105797 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.stg_jobs_raw"
[0m11:24:31.106293 [debug] [Thread-1 (]: On model.job_intelligent.stg_jobs_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.stg_jobs_raw"} */

      drop view if exists "memory"."main_bronze"."stg_jobs_raw__dbt_backup" cascade
    
[0m11:24:31.107080 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m11:24:31.109593 [debug] [Thread-1 (]: On model.job_intelligent.stg_jobs_raw: Close
[0m11:24:31.112137 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '72bca6db-7fe3-464c-856e-a87dd623f887', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E4DCA0E810>]}
[0m11:24:31.112897 [info ] [Thread-1 (]: 1 of 13 OK created sql view model main_bronze.stg_jobs_raw ..................... [[32mOK[0m in 0.15s]
[0m11:24:31.113739 [debug] [Thread-1 (]: Finished running node model.job_intelligent.stg_jobs_raw
[0m11:24:31.114760 [debug] [Thread-2 (]: Began running node model.job_intelligent.int_jobs_cleaned
[0m11:24:31.115603 [info ] [Thread-2 (]: 2 of 13 START sql table model main_silver.int_jobs_cleaned ..................... [RUN]
[0m11:24:31.116260 [debug] [Thread-2 (]: Acquiring new duckdb connection 'model.job_intelligent.int_jobs_cleaned'
[0m11:24:31.116739 [debug] [Thread-2 (]: Began compiling node model.job_intelligent.int_jobs_cleaned
[0m11:24:31.121015 [debug] [Thread-2 (]: Writing injected SQL for node "model.job_intelligent.int_jobs_cleaned"
[0m11:24:31.121961 [debug] [Thread-2 (]: Began executing node model.job_intelligent.int_jobs_cleaned
[0m11:24:31.143838 [debug] [Thread-2 (]: Writing runtime sql for node "model.job_intelligent.int_jobs_cleaned"
[0m11:24:31.144882 [debug] [Thread-2 (]: Using duckdb connection "model.job_intelligent.int_jobs_cleaned"
[0m11:24:31.145345 [debug] [Thread-2 (]: On model.job_intelligent.int_jobs_cleaned: BEGIN
[0m11:24:31.145737 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m11:24:31.146501 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m11:24:31.146896 [debug] [Thread-2 (]: Using duckdb connection "model.job_intelligent.int_jobs_cleaned"
[0m11:24:31.147417 [debug] [Thread-2 (]: On model.job_intelligent.int_jobs_cleaned: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.int_jobs_cleaned"} */

  
    
    

    create  table
      "memory"."main_silver"."int_jobs_cleaned__dbt_tmp"
  
    as (
      -- models/silver/int_jobs_cleaned.sql
-- Silver layer: Nettoyage et normalisation des données brutes



WITH raw_jobs AS (
    SELECT * FROM "memory"."main_bronze"."stg_jobs_raw"
),

cleaned_jobs AS (
    SELECT
        -- Texte: lowercase, trim, remove special characters
        LOWER(TRIM(job_title)) as job_title_cleaned,
        LOWER(TRIM(location)) as location_cleaned,
        LOWER(TRIM(company_name)) as company_name_cleaned,
        LOWER(TRIM(job_description)) as job_description_cleaned,
        LOWER(TRIM(contract_type)) as contract_type_cleaned,
        LOWER(TRIM(work_type)) as work_type_cleaned,
        
        -- URLs as-is
        job_url,
        company_url,
        
        -- Dates
        TRY_CAST(published_at AS DATE) as published_date,
        posted_time,
        
        -- Extract year-month for time-based analysis
        DATE_TRUNC('month', TRY_CAST(published_at AS DATE)) as published_year_month,
        EXTRACT(YEAR FROM TRY_CAST(published_at AS DATE)) as published_year,
        EXTRACT(MONTH FROM TRY_CAST(published_at AS DATE)) as published_month,
        
        -- Métadonnées
        ingestion_timestamp
    FROM raw_jobs
)

SELECT
    *,
    -- Deduplication flag
    ROW_NUMBER() OVER (
        PARTITION BY job_title_cleaned, company_name_cleaned, location_cleaned 
        ORDER BY published_date DESC
    ) as dedup_rank
FROM cleaned_jobs
    );
  
  
[0m11:24:31.278972 [debug] [Thread-2 (]: SQL status: OK in 0.131 seconds
[0m11:24:31.281498 [debug] [Thread-2 (]: Using duckdb connection "model.job_intelligent.int_jobs_cleaned"
[0m11:24:31.281830 [debug] [Thread-2 (]: On model.job_intelligent.int_jobs_cleaned: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.int_jobs_cleaned"} */
alter table "memory"."main_silver"."int_jobs_cleaned__dbt_tmp" rename to "int_jobs_cleaned"
[0m11:24:31.282583 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m11:24:31.287282 [debug] [Thread-2 (]: On model.job_intelligent.int_jobs_cleaned: COMMIT
[0m11:24:31.287693 [debug] [Thread-2 (]: Using duckdb connection "model.job_intelligent.int_jobs_cleaned"
[0m11:24:31.288006 [debug] [Thread-2 (]: On model.job_intelligent.int_jobs_cleaned: COMMIT
[0m11:24:31.291470 [debug] [Thread-2 (]: SQL status: OK in 0.003 seconds
[0m11:24:31.293326 [debug] [Thread-2 (]: Using duckdb connection "model.job_intelligent.int_jobs_cleaned"
[0m11:24:31.293665 [debug] [Thread-2 (]: On model.job_intelligent.int_jobs_cleaned: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.int_jobs_cleaned"} */

      drop table if exists "memory"."main_silver"."int_jobs_cleaned__dbt_backup" cascade
    
[0m11:24:31.294272 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m11:24:31.295412 [debug] [Thread-2 (]: On model.job_intelligent.int_jobs_cleaned: Close
[0m11:24:31.295881 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '72bca6db-7fe3-464c-856e-a87dd623f887', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E4DCFAD230>]}
[0m11:24:31.296400 [info ] [Thread-2 (]: 2 of 13 OK created sql table model main_silver.int_jobs_cleaned ................ [[32mOK[0m in 0.18s]
[0m11:24:31.296948 [debug] [Thread-2 (]: Finished running node model.job_intelligent.int_jobs_cleaned
[0m11:24:31.297654 [debug] [Thread-3 (]: Began running node model.job_intelligent.int_job_title_normalization
[0m11:24:31.298219 [info ] [Thread-3 (]: 3 of 13 START sql table model main_silver.int_job_title_normalization .......... [RUN]
[0m11:24:31.299124 [debug] [Thread-3 (]: Acquiring new duckdb connection 'model.job_intelligent.int_job_title_normalization'
[0m11:24:31.299571 [debug] [Thread-3 (]: Began compiling node model.job_intelligent.int_job_title_normalization
[0m11:24:31.303499 [debug] [Thread-3 (]: Writing injected SQL for node "model.job_intelligent.int_job_title_normalization"
[0m11:24:31.304415 [debug] [Thread-3 (]: Began executing node model.job_intelligent.int_job_title_normalization
[0m11:24:31.306982 [debug] [Thread-3 (]: Writing runtime sql for node "model.job_intelligent.int_job_title_normalization"
[0m11:24:31.307694 [debug] [Thread-3 (]: Using duckdb connection "model.job_intelligent.int_job_title_normalization"
[0m11:24:31.308081 [debug] [Thread-3 (]: On model.job_intelligent.int_job_title_normalization: BEGIN
[0m11:24:31.308397 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m11:24:31.309061 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m11:24:31.309389 [debug] [Thread-3 (]: Using duckdb connection "model.job_intelligent.int_job_title_normalization"
[0m11:24:31.309799 [debug] [Thread-3 (]: On model.job_intelligent.int_job_title_normalization: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.int_job_title_normalization"} */

  
    
    

    create  table
      "memory"."main_silver"."int_job_title_normalization__dbt_tmp"
  
    as (
      -- models/silver/int_job_title_normalization.sql
-- Silver layer: Normaliser les intitulés de postes



WITH cleaned_jobs AS (
    SELECT * FROM "memory"."main_silver"."int_jobs_cleaned"
),

title_normalized AS (
    SELECT
        *,
        CASE
            WHEN job_title_cleaned LIKE '%data engineer%' THEN 'Data Engineer'
            WHEN job_title_cleaned LIKE '%data scientist%' THEN 'Data Scientist'
            WHEN job_title_cleaned LIKE '%data analyst%' THEN 'Data Analyst'
            WHEN job_title_cleaned LIKE '%analytics engineer%' THEN 'Analytics Engineer'
            WHEN job_title_cleaned LIKE '%ml engineer%' OR job_title_cleaned LIKE '%machine learning%' THEN 'ML Engineer'
            WHEN job_title_cleaned LIKE '%data architect%' THEN 'Data Architect'
            WHEN job_title_cleaned LIKE '%bi developer%' OR job_title_cleaned LIKE '%business intelligence%' THEN 'BI Developer'
            WHEN job_title_cleaned LIKE '%etl%' OR job_title_cleaned LIKE '%pipeline%' THEN 'ETL/Pipeline Engineer'
            WHEN job_title_cleaned LIKE '%data engineer%' THEN 'Data Engineer'
            ELSE 'Other Data Role'
        END as job_category,
        
        CASE
            WHEN contract_type_cleaned LIKE '%cdi%' OR contract_type_cleaned LIKE '%permanent%' THEN 'Permanent'
            WHEN contract_type_cleaned LIKE '%cdd%' OR contract_type_cleaned LIKE '%contract%' THEN 'Contract'
            WHEN contract_type_cleaned LIKE '%stage%' OR contract_type_cleaned LIKE '%internship%' THEN 'Internship'
            WHEN contract_type_cleaned LIKE '%freelance%' THEN 'Freelance'
            ELSE 'Not Specified'
        END as contract_type_normalized,
        
        CASE
            WHEN work_type_cleaned LIKE '%remote%' THEN 'Remote'
            WHEN work_type_cleaned LIKE '%hybrid%' THEN 'Hybrid'
            WHEN work_type_cleaned LIKE '%onsite%' OR work_type_cleaned LIKE '%on-site%' THEN 'On-site'
            ELSE 'Not Specified'
        END as work_type_normalized
    
    FROM cleaned_jobs
    WHERE dedup_rank = 1  -- Keep only first occurrence (most recent)
)

SELECT * FROM title_normalized
    );
  
  
[0m11:24:31.337612 [debug] [Thread-3 (]: SQL status: OK in 0.027 seconds
[0m11:24:31.340608 [debug] [Thread-3 (]: Using duckdb connection "model.job_intelligent.int_job_title_normalization"
[0m11:24:31.341037 [debug] [Thread-3 (]: On model.job_intelligent.int_job_title_normalization: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.int_job_title_normalization"} */
alter table "memory"."main_silver"."int_job_title_normalization__dbt_tmp" rename to "int_job_title_normalization"
[0m11:24:31.342170 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m11:24:31.343728 [debug] [Thread-3 (]: On model.job_intelligent.int_job_title_normalization: COMMIT
[0m11:24:31.344143 [debug] [Thread-3 (]: Using duckdb connection "model.job_intelligent.int_job_title_normalization"
[0m11:24:31.344532 [debug] [Thread-3 (]: On model.job_intelligent.int_job_title_normalization: COMMIT
[0m11:24:31.345191 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m11:24:31.347415 [debug] [Thread-3 (]: Using duckdb connection "model.job_intelligent.int_job_title_normalization"
[0m11:24:31.347856 [debug] [Thread-3 (]: On model.job_intelligent.int_job_title_normalization: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.int_job_title_normalization"} */

      drop table if exists "memory"."main_silver"."int_job_title_normalization__dbt_backup" cascade
    
[0m11:24:31.348583 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m11:24:31.349985 [debug] [Thread-3 (]: On model.job_intelligent.int_job_title_normalization: Close
[0m11:24:31.350539 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '72bca6db-7fe3-464c-856e-a87dd623f887', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E4DE424F50>]}
[0m11:24:31.351189 [info ] [Thread-3 (]: 3 of 13 OK created sql table model main_silver.int_job_title_normalization ..... [[32mOK[0m in 0.05s]
[0m11:24:31.351992 [debug] [Thread-3 (]: Finished running node model.job_intelligent.int_job_title_normalization
[0m11:24:31.352973 [debug] [Thread-1 (]: Began running node model.job_intelligent.dim_location
[0m11:24:31.353470 [debug] [Thread-2 (]: Began running node model.job_intelligent.dim_time
[0m11:24:31.353947 [debug] [Thread-4 (]: Began running node model.job_intelligent.dim_company
[0m11:24:31.354590 [debug] [Thread-3 (]: Began running node model.job_intelligent.int_skills_extraction
[0m11:24:31.355290 [info ] [Thread-1 (]: 5 of 13 START sql table model main_gold.dim_location ........................... [RUN]
[0m11:24:31.356029 [info ] [Thread-2 (]: 6 of 13 START sql table model main_gold.dim_time ............................... [RUN]
[0m11:24:31.356789 [info ] [Thread-4 (]: 4 of 13 START sql table model main_gold.dim_company ............................ [RUN]
[0m11:24:31.357543 [info ] [Thread-3 (]: 7 of 13 START sql table model main_silver.int_skills_extraction ................ [RUN]
[0m11:24:31.358201 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.job_intelligent.stg_jobs_raw, now model.job_intelligent.dim_location)
[0m11:24:31.358776 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly model.job_intelligent.int_jobs_cleaned, now model.job_intelligent.dim_time)
[0m11:24:31.359430 [debug] [Thread-4 (]: Acquiring new duckdb connection 'model.job_intelligent.dim_company'
[0m11:24:31.359946 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly model.job_intelligent.int_job_title_normalization, now model.job_intelligent.int_skills_extraction)
[0m11:24:31.360514 [debug] [Thread-1 (]: Began compiling node model.job_intelligent.dim_location
[0m11:24:31.361128 [debug] [Thread-2 (]: Began compiling node model.job_intelligent.dim_time
[0m11:24:31.361662 [debug] [Thread-4 (]: Began compiling node model.job_intelligent.dim_company
[0m11:24:31.362152 [debug] [Thread-3 (]: Began compiling node model.job_intelligent.int_skills_extraction
[0m11:24:31.365666 [debug] [Thread-1 (]: Writing injected SQL for node "model.job_intelligent.dim_location"
[0m11:24:31.369591 [debug] [Thread-2 (]: Writing injected SQL for node "model.job_intelligent.dim_time"
[0m11:24:31.373047 [debug] [Thread-4 (]: Writing injected SQL for node "model.job_intelligent.dim_company"
[0m11:24:31.376322 [debug] [Thread-3 (]: Writing injected SQL for node "model.job_intelligent.int_skills_extraction"
[0m11:24:31.377617 [debug] [Thread-1 (]: Began executing node model.job_intelligent.dim_location
[0m11:24:31.381840 [debug] [Thread-1 (]: Writing runtime sql for node "model.job_intelligent.dim_location"
[0m11:24:31.382612 [debug] [Thread-3 (]: Began executing node model.job_intelligent.int_skills_extraction
[0m11:24:31.387572 [debug] [Thread-3 (]: Writing runtime sql for node "model.job_intelligent.int_skills_extraction"
[0m11:24:31.388329 [debug] [Thread-2 (]: Began executing node model.job_intelligent.dim_time
[0m11:24:31.394691 [debug] [Thread-2 (]: Writing runtime sql for node "model.job_intelligent.dim_time"
[0m11:24:31.395368 [debug] [Thread-4 (]: Began executing node model.job_intelligent.dim_company
[0m11:24:31.398307 [debug] [Thread-4 (]: Writing runtime sql for node "model.job_intelligent.dim_company"
[0m11:24:31.399144 [debug] [Thread-3 (]: Using duckdb connection "model.job_intelligent.int_skills_extraction"
[0m11:24:31.399832 [debug] [Thread-3 (]: On model.job_intelligent.int_skills_extraction: BEGIN
[0m11:24:31.400274 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m11:24:31.400795 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.dim_location"
[0m11:24:31.401740 [debug] [Thread-1 (]: On model.job_intelligent.dim_location: BEGIN
[0m11:24:31.402343 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:24:31.403167 [debug] [Thread-4 (]: Using duckdb connection "model.job_intelligent.dim_company"
[0m11:24:31.403860 [debug] [Thread-2 (]: Using duckdb connection "model.job_intelligent.dim_time"
[0m11:24:31.404634 [debug] [Thread-3 (]: SQL status: OK in 0.004 seconds
[0m11:24:31.405154 [debug] [Thread-4 (]: On model.job_intelligent.dim_company: BEGIN
[0m11:24:31.405647 [debug] [Thread-1 (]: SQL status: OK in 0.003 seconds
[0m11:24:31.406092 [debug] [Thread-2 (]: On model.job_intelligent.dim_time: BEGIN
[0m11:24:31.406583 [debug] [Thread-3 (]: Using duckdb connection "model.job_intelligent.int_skills_extraction"
[0m11:24:31.407070 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m11:24:31.407564 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.dim_location"
[0m11:24:31.408043 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m11:24:31.408651 [debug] [Thread-3 (]: On model.job_intelligent.int_skills_extraction: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.int_skills_extraction"} */

  
    
    

    create  table
      "memory"."main_silver"."int_skills_extraction__dbt_tmp"
  
    as (
      -- models/silver/int_skills_extraction.sql
-- Silver layer: Extraction des compétences depuis la description



WITH jobs_with_titles AS (
    SELECT * FROM "memory"."main_silver"."int_job_title_normalization"
),

skills_mapping AS (
    -- Définir un mapping de compétences communes Data/Tech
    SELECT
        'Python' as skill_name,
        'python|py |\.py' as skill_pattern
    UNION ALL SELECT 'SQL', 'sql|sql|sql server|postgres|oracle'
    UNION ALL SELECT 'Spark', 'spark|pyspark'
    UNION ALL SELECT 'Hadoop', 'hadoop|hdfs'
    UNION ALL SELECT 'Scala', 'scala'
    UNION ALL SELECT 'Java', '\bjava\b'
    UNION ALL SELECT 'R', '\br\b|r programming'
    UNION ALL SELECT 'Tableau', 'tableau'
    UNION ALL SELECT 'Power BI', 'power bi|powerbi'
    UNION ALL SELECT 'Looker', 'looker'
    UNION ALL SELECT 'AWS', 'aws|amazon web|s3 |ec2|redshift'
    UNION ALL SELECT 'Azure', 'azure|microsoft azure|synapse|cosmos'
    UNION ALL SELECT 'GCP', 'gcp|google cloud|bigquery'
    UNION ALL SELECT 'Airflow', 'airflow'
    UNION ALL SELECT 'DBT', '\bdbt\b|dbt'
    UNION ALL SELECT 'Kubernetes', 'kubernetes|k8s'
    UNION ALL SELECT 'Docker', 'docker'
    UNION ALL SELECT 'Git', 'git|github|gitlab'
    UNION ALL SELECT 'TensorFlow', 'tensorflow'
    UNION ALL SELECT 'PyTorch', 'pytorch'
    UNION ALL SELECT 'Scikit-learn', 'scikit|sklearn'
    UNION ALL SELECT 'Pandas', 'pandas'
    UNION ALL SELECT 'NumPy', 'numpy'
    UNION ALL SELECT 'Machine Learning', 'machine learning|deep learning|ml|artificial intelligence'
    UNION ALL SELECT 'Statistics', 'statistics|statistical|probability'
    UNION ALL SELECT 'Data Visualization', 'data visualization|visualization|charts|graphs'
),

jobs_exploded AS (
    SELECT
        j.*,
        s.skill_name,
        CASE 
            WHEN job_description_cleaned ~* s.skill_pattern THEN 1 
            ELSE 0 
        END as has_skill
    FROM jobs_with_titles j
    CROSS JOIN skills_mapping s
)

SELECT
    *
FROM jobs_exploded
WHERE has_skill = 1

ORDER BY job_title_cleaned, published_date DESC
    );
  
  
[0m11:24:31.409568 [debug] [Thread-1 (]: On model.job_intelligent.dim_location: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_location"} */

  
    
    

    create  table
      "memory"."main_gold"."dim_location__dbt_tmp"
  
    as (
      -- models/gold/dim_location.sql
-- Gold layer: Dimension Location



WITH jobs AS (
    SELECT DISTINCT
        location_cleaned as location_raw
    FROM "memory"."main_silver"."int_job_title_normalization"
    WHERE location_cleaned IS NOT NULL
),

location_parsed AS (
    SELECT
        location_raw,
        -- Extract city (before comma)
        CASE 
            WHEN POSITION(',' IN location_raw) > 0 
            THEN TRIM(SUBSTRING(location_raw, 1, POSITION(',' IN location_raw) - 1))
            ELSE location_raw
        END as city,
        
        -- Extract country (after comma)
        CASE 
            WHEN POSITION(',' IN location_raw) > 0 
            THEN TRIM(SUBSTRING(location_raw, POSITION(',' IN location_raw) + 1))
            ELSE 'Not Specified'
        END as country,
        
        -- Detect if remote
        CASE 
            WHEN location_raw LIKE '%remote%' 
            THEN 'Remote'
            ELSE 'On-site'
        END as work_location_type
    FROM jobs
),

ranked_locations AS (
    SELECT
        ROW_NUMBER() OVER (ORDER BY location_raw) as location_id,
        location_raw,
        city,
        country,
        work_location_type,
        CURRENT_TIMESTAMP() as created_at
    FROM location_parsed
)

SELECT
    location_id,
    location_raw,
    city,
    country,
    work_location_type,
    created_at
FROM ranked_locations
ORDER BY location_id
    );
  
  
[0m11:24:31.410354 [debug] [Thread-4 (]: SQL status: OK in 0.003 seconds
[0m11:24:31.411331 [debug] [Thread-2 (]: SQL status: OK in 0.003 seconds
[0m11:24:31.412191 [debug] [Thread-4 (]: Using duckdb connection "model.job_intelligent.dim_company"
[0m11:24:31.412739 [debug] [Thread-2 (]: Using duckdb connection "model.job_intelligent.dim_time"
[0m11:24:31.413404 [debug] [Thread-4 (]: On model.job_intelligent.dim_company: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_company"} */

  
    
    

    create  table
      "memory"."main_gold"."dim_company__dbt_tmp"
  
    as (
      -- models/gold/dim_company.sql
-- Gold layer: Dimension Company



WITH jobs AS (
    SELECT DISTINCT
        company_name_cleaned,
        company_url
    FROM "memory"."main_silver"."int_job_title_normalization"
    WHERE company_name_cleaned IS NOT NULL
),

ranked_companies AS (
    SELECT
        ROW_NUMBER() OVER (ORDER BY company_name_cleaned) as company_id,
        company_name_cleaned as company_name,
        company_url,
        CURRENT_TIMESTAMP() as created_at
    FROM jobs
)

SELECT
    company_id,
    company_name,
    company_url,
    created_at
FROM ranked_companies
ORDER BY company_id
    );
  
  
[0m11:24:31.414242 [debug] [Thread-2 (]: On model.job_intelligent.dim_time: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_time"} */

  
    
    

    create  table
      "memory"."main_gold"."dim_time__dbt_tmp"
  
    as (
      -- models/gold/dim_time.sql
-- Gold layer: Dimension Time



-- Generate a date dimension for time-based analysis
WITH date_spine AS (
    SELECT
        published_date,
        EXTRACT(YEAR FROM published_date) as year,
        EXTRACT(MONTH FROM published_date) as month,
        EXTRACT(QUARTER FROM published_date) as quarter,
        EXTRACT(WEEK FROM published_date) as week,
        EXTRACT(DAYOFWEEK FROM published_date) as day_of_week,
        DATE_TRUNC('month', published_date) as month_start,
        DATE_TRUNC('quarter', published_date) as quarter_start,
        DATE_TRUNC('year', published_date) as year_start,
        
        CASE 
            WHEN EXTRACT(MONTH FROM published_date) IN (1,2,3) THEN 'Q1'
            WHEN EXTRACT(MONTH FROM published_date) IN (4,5,6) THEN 'Q2'
            WHEN EXTRACT(MONTH FROM published_date) IN (7,8,9) THEN 'Q3'
            ELSE 'Q4'
        END as quarter_name,
        
        CASE EXTRACT(DAYOFWEEK FROM published_date)
            WHEN 0 THEN 'Sunday'
            WHEN 1 THEN 'Monday'
            WHEN 2 THEN 'Tuesday'
            WHEN 3 THEN 'Wednesday'
            WHEN 4 THEN 'Thursday'
            WHEN 5 THEN 'Friday'
            WHEN 6 THEN 'Saturday'
        END as day_name,
        
        CASE EXTRACT(MONTH FROM published_date)
            WHEN 1 THEN 'January'
            WHEN 2 THEN 'February'
            WHEN 3 THEN 'March'
            WHEN 4 THEN 'April'
            WHEN 5 THEN 'May'
            WHEN 6 THEN 'June'
            WHEN 7 THEN 'July'
            WHEN 8 THEN 'August'
            WHEN 9 THEN 'September'
            WHEN 10 THEN 'October'
            WHEN 11 THEN 'November'
            WHEN 12 THEN 'December'
        END as month_name
        
    FROM (
        SELECT DISTINCT published_date
        FROM "memory"."main_silver"."int_job_title_normalization"
        WHERE published_date IS NOT NULL
    )
)

SELECT
    published_date as date_id,
    year,
    month,
    quarter,
    week,
    day_of_week,
    day_name,
    month_name,
    quarter_name,
    month_start,
    quarter_start,
    year_start,
    CURRENT_TIMESTAMP() as created_at
FROM date_spine
ORDER BY published_date
    );
  
  
[0m11:24:31.415755 [debug] [Thread-3 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.int_skills_extraction"} */

  
    
    

    create  table
      "memory"."main_silver"."int_skills_extraction__dbt_tmp"
  
    as (
      -- models/silver/int_skills_extraction.sql
-- Silver layer: Extraction des compétences depuis la description



WITH jobs_with_titles AS (
    SELECT * FROM "memory"."main_silver"."int_job_title_normalization"
),

skills_mapping AS (
    -- Définir un mapping de compétences communes Data/Tech
    SELECT
        'Python' as skill_name,
        'python|py |\.py' as skill_pattern
    UNION ALL SELECT 'SQL', 'sql|sql|sql server|postgres|oracle'
    UNION ALL SELECT 'Spark', 'spark|pyspark'
    UNION ALL SELECT 'Hadoop', 'hadoop|hdfs'
    UNION ALL SELECT 'Scala', 'scala'
    UNION ALL SELECT 'Java', '\bjava\b'
    UNION ALL SELECT 'R', '\br\b|r programming'
    UNION ALL SELECT 'Tableau', 'tableau'
    UNION ALL SELECT 'Power BI', 'power bi|powerbi'
    UNION ALL SELECT 'Looker', 'looker'
    UNION ALL SELECT 'AWS', 'aws|amazon web|s3 |ec2|redshift'
    UNION ALL SELECT 'Azure', 'azure|microsoft azure|synapse|cosmos'
    UNION ALL SELECT 'GCP', 'gcp|google cloud|bigquery'
    UNION ALL SELECT 'Airflow', 'airflow'
    UNION ALL SELECT 'DBT', '\bdbt\b|dbt'
    UNION ALL SELECT 'Kubernetes', 'kubernetes|k8s'
    UNION ALL SELECT 'Docker', 'docker'
    UNION ALL SELECT 'Git', 'git|github|gitlab'
    UNION ALL SELECT 'TensorFlow', 'tensorflow'
    UNION ALL SELECT 'PyTorch', 'pytorch'
    UNION ALL SELECT 'Scikit-learn', 'scikit|sklearn'
    UNION ALL SELECT 'Pandas', 'pandas'
    UNION ALL SELECT 'NumPy', 'numpy'
    UNION ALL SELECT 'Machine Learning', 'machine learning|deep learning|ml|artificial intelligence'
    UNION ALL SELECT 'Statistics', 'statistics|statistical|probability'
    UNION ALL SELECT 'Data Visualization', 'data visualization|visualization|charts|graphs'
),

jobs_exploded AS (
    SELECT
        j.*,
        s.skill_name,
        CASE 
            WHEN job_description_cleaned ~* s.skill_pattern THEN 1 
            ELSE 0 
        END as has_skill
    FROM jobs_with_titles j
    CROSS JOIN skills_mapping s
)

SELECT
    *
FROM jobs_exploded
WHERE has_skill = 1

ORDER BY job_title_cleaned, published_date DESC
    );
  
  
[0m11:24:31.416493 [debug] [Thread-1 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_location"} */

  
    
    

    create  table
      "memory"."main_gold"."dim_location__dbt_tmp"
  
    as (
      -- models/gold/dim_location.sql
-- Gold layer: Dimension Location



WITH jobs AS (
    SELECT DISTINCT
        location_cleaned as location_raw
    FROM "memory"."main_silver"."int_job_title_normalization"
    WHERE location_cleaned IS NOT NULL
),

location_parsed AS (
    SELECT
        location_raw,
        -- Extract city (before comma)
        CASE 
            WHEN POSITION(',' IN location_raw) > 0 
            THEN TRIM(SUBSTRING(location_raw, 1, POSITION(',' IN location_raw) - 1))
            ELSE location_raw
        END as city,
        
        -- Extract country (after comma)
        CASE 
            WHEN POSITION(',' IN location_raw) > 0 
            THEN TRIM(SUBSTRING(location_raw, POSITION(',' IN location_raw) + 1))
            ELSE 'Not Specified'
        END as country,
        
        -- Detect if remote
        CASE 
            WHEN location_raw LIKE '%remote%' 
            THEN 'Remote'
            ELSE 'On-site'
        END as work_location_type
    FROM jobs
),

ranked_locations AS (
    SELECT
        ROW_NUMBER() OVER (ORDER BY location_raw) as location_id,
        location_raw,
        city,
        country,
        work_location_type,
        CURRENT_TIMESTAMP() as created_at
    FROM location_parsed
)

SELECT
    location_id,
    location_raw,
    city,
    country,
    work_location_type,
    created_at
FROM ranked_locations
ORDER BY location_id
    );
  
  
[0m11:24:31.417030 [debug] [Thread-3 (]: DuckDB adapter: Rolling back transaction.
[0m11:24:31.417620 [debug] [Thread-4 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_company"} */

  
    
    

    create  table
      "memory"."main_gold"."dim_company__dbt_tmp"
  
    as (
      -- models/gold/dim_company.sql
-- Gold layer: Dimension Company



WITH jobs AS (
    SELECT DISTINCT
        company_name_cleaned,
        company_url
    FROM "memory"."main_silver"."int_job_title_normalization"
    WHERE company_name_cleaned IS NOT NULL
),

ranked_companies AS (
    SELECT
        ROW_NUMBER() OVER (ORDER BY company_name_cleaned) as company_id,
        company_name_cleaned as company_name,
        company_url,
        CURRENT_TIMESTAMP() as created_at
    FROM jobs
)

SELECT
    company_id,
    company_name,
    company_url,
    created_at
FROM ranked_companies
ORDER BY company_id
    );
  
  
[0m11:24:31.418197 [debug] [Thread-1 (]: DuckDB adapter: Rolling back transaction.
[0m11:24:31.419047 [debug] [Thread-2 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_time"} */

  
    
    

    create  table
      "memory"."main_gold"."dim_time__dbt_tmp"
  
    as (
      -- models/gold/dim_time.sql
-- Gold layer: Dimension Time



-- Generate a date dimension for time-based analysis
WITH date_spine AS (
    SELECT
        published_date,
        EXTRACT(YEAR FROM published_date) as year,
        EXTRACT(MONTH FROM published_date) as month,
        EXTRACT(QUARTER FROM published_date) as quarter,
        EXTRACT(WEEK FROM published_date) as week,
        EXTRACT(DAYOFWEEK FROM published_date) as day_of_week,
        DATE_TRUNC('month', published_date) as month_start,
        DATE_TRUNC('quarter', published_date) as quarter_start,
        DATE_TRUNC('year', published_date) as year_start,
        
        CASE 
            WHEN EXTRACT(MONTH FROM published_date) IN (1,2,3) THEN 'Q1'
            WHEN EXTRACT(MONTH FROM published_date) IN (4,5,6) THEN 'Q2'
            WHEN EXTRACT(MONTH FROM published_date) IN (7,8,9) THEN 'Q3'
            ELSE 'Q4'
        END as quarter_name,
        
        CASE EXTRACT(DAYOFWEEK FROM published_date)
            WHEN 0 THEN 'Sunday'
            WHEN 1 THEN 'Monday'
            WHEN 2 THEN 'Tuesday'
            WHEN 3 THEN 'Wednesday'
            WHEN 4 THEN 'Thursday'
            WHEN 5 THEN 'Friday'
            WHEN 6 THEN 'Saturday'
        END as day_name,
        
        CASE EXTRACT(MONTH FROM published_date)
            WHEN 1 THEN 'January'
            WHEN 2 THEN 'February'
            WHEN 3 THEN 'March'
            WHEN 4 THEN 'April'
            WHEN 5 THEN 'May'
            WHEN 6 THEN 'June'
            WHEN 7 THEN 'July'
            WHEN 8 THEN 'August'
            WHEN 9 THEN 'September'
            WHEN 10 THEN 'October'
            WHEN 11 THEN 'November'
            WHEN 12 THEN 'December'
        END as month_name
        
    FROM (
        SELECT DISTINCT published_date
        FROM "memory"."main_silver"."int_job_title_normalization"
        WHERE published_date IS NOT NULL
    )
)

SELECT
    published_date as date_id,
    year,
    month,
    quarter,
    week,
    day_of_week,
    day_name,
    month_name,
    quarter_name,
    month_start,
    quarter_start,
    year_start,
    CURRENT_TIMESTAMP() as created_at
FROM date_spine
ORDER BY published_date
    );
  
  
[0m11:24:31.419797 [debug] [Thread-3 (]: On model.job_intelligent.int_skills_extraction: ROLLBACK
[0m11:24:31.420311 [debug] [Thread-4 (]: DuckDB adapter: Rolling back transaction.
[0m11:24:31.420885 [debug] [Thread-1 (]: On model.job_intelligent.dim_location: ROLLBACK
[0m11:24:31.421381 [debug] [Thread-2 (]: DuckDB adapter: Rolling back transaction.
[0m11:24:31.422018 [debug] [Thread-4 (]: On model.job_intelligent.dim_company: ROLLBACK
[0m11:24:31.422886 [debug] [Thread-2 (]: On model.job_intelligent.dim_time: ROLLBACK
[0m11:24:31.439623 [debug] [Thread-3 (]: Failed to rollback 'model.job_intelligent.int_skills_extraction'
[0m11:24:31.440093 [debug] [Thread-3 (]: On model.job_intelligent.int_skills_extraction: Close
[0m11:24:31.444087 [debug] [Thread-1 (]: Failed to rollback 'model.job_intelligent.dim_location'
[0m11:24:31.444474 [debug] [Thread-1 (]: On model.job_intelligent.dim_location: Close
[0m11:24:31.448545 [debug] [Thread-2 (]: Failed to rollback 'model.job_intelligent.dim_time'
[0m11:24:31.448928 [debug] [Thread-2 (]: On model.job_intelligent.dim_time: Close
[0m11:24:31.453888 [debug] [Thread-4 (]: Failed to rollback 'model.job_intelligent.dim_company'
[0m11:24:31.454357 [debug] [Thread-4 (]: On model.job_intelligent.dim_company: Close
[0m11:24:31.459466 [debug] [Thread-3 (]: Runtime Error in model int_skills_extraction (models\silver\int_skills_extraction.sql)
  Catalog Error: Scalar Function with name ~* does not exist!
  Did you mean "~"?
  
  LINE 57:             WHEN job_description_cleaned ~* s.skill_pattern THEN 1 
                                                    ^
[0m11:24:31.459965 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '72bca6db-7fe3-464c-856e-a87dd623f887', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E4DD2D7E50>]}
[0m11:24:31.460608 [error] [Thread-3 (]: 7 of 13 ERROR creating sql table model main_silver.int_skills_extraction ....... [[31mERROR[0m in 0.10s]
[0m11:24:31.461316 [debug] [Thread-3 (]: Finished running node model.job_intelligent.int_skills_extraction
[0m11:24:31.462183 [debug] [Thread-7 (]: Marking all children of 'model.job_intelligent.int_skills_extraction' to be skipped because of status 'error'.  Reason: Runtime Error in model int_skills_extraction (models\silver\int_skills_extraction.sql)
  Catalog Error: Scalar Function with name ~* does not exist!
  Did you mean "~"?
  
  LINE 57:             WHEN job_description_cleaned ~* s.skill_pattern THEN 1 
                                                    ^.
[0m11:24:31.466084 [debug] [Thread-4 (]: Runtime Error in model dim_company (models\gold\dim_company.sql)
  Catalog Error: Scalar Function with name current_timestamp does not exist!
  Did you mean "current_localtimestamp"?
  
  LINE 29:         CURRENT_TIMESTAMP() as created_at
                   ^
[0m11:24:31.466569 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '72bca6db-7fe3-464c-856e-a87dd623f887', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E4DE458830>]}
[0m11:24:31.467198 [error] [Thread-4 (]: 4 of 13 ERROR creating sql table model main_gold.dim_company ................... [[31mERROR[0m in 0.11s]
[0m11:24:31.467880 [debug] [Thread-4 (]: Finished running node model.job_intelligent.dim_company
[0m11:24:31.468320 [debug] [Thread-3 (]: Began running node model.job_intelligent.dim_skills
[0m11:24:31.471920 [debug] [Thread-1 (]: Runtime Error in model dim_location (models\gold\dim_location.sql)
  Catalog Error: Scalar Function with name current_timestamp does not exist!
  Did you mean "current_localtimestamp"?
  
  LINE 56:         CURRENT_TIMESTAMP() as created_at
                   ^
[0m11:24:31.472719 [debug] [Thread-7 (]: Marking all children of 'model.job_intelligent.dim_company' to be skipped because of status 'error'.  Reason: Runtime Error in model dim_company (models\gold\dim_company.sql)
  Catalog Error: Scalar Function with name current_timestamp does not exist!
  Did you mean "current_localtimestamp"?
  
  LINE 29:         CURRENT_TIMESTAMP() as created_at
                   ^.
[0m11:24:31.473334 [info ] [Thread-3 (]: 8 of 13 SKIP relation main_gold.dim_skills ..................................... [[33mSKIP[0m]
[0m11:24:31.474155 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '72bca6db-7fe3-464c-856e-a87dd623f887', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E4DD30F6B0>]}
[0m11:24:31.477262 [debug] [Thread-2 (]: Runtime Error in model dim_time (models\gold\dim_time.sql)
  Catalog Error: Scalar Function with name current_timestamp does not exist!
  Did you mean "current_localtimestamp"?
  
  LINE 81:     CURRENT_TIMESTAMP() as created_at
               ^
[0m11:24:31.477955 [debug] [Thread-3 (]: Finished running node model.job_intelligent.dim_skills
[0m11:24:31.478642 [error] [Thread-1 (]: 5 of 13 ERROR creating sql table model main_gold.dim_location .................. [[31mERROR[0m in 0.12s]
[0m11:24:31.479177 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '72bca6db-7fe3-464c-856e-a87dd623f887', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E4DCB2BD70>]}
[0m11:24:31.479996 [debug] [Thread-1 (]: Finished running node model.job_intelligent.dim_location
[0m11:24:31.480720 [error] [Thread-2 (]: 6 of 13 ERROR creating sql table model main_gold.dim_time ...................... [[31mERROR[0m in 0.12s]
[0m11:24:31.481485 [debug] [Thread-7 (]: Marking all children of 'model.job_intelligent.dim_location' to be skipped because of status 'error'.  Reason: Runtime Error in model dim_location (models\gold\dim_location.sql)
  Catalog Error: Scalar Function with name current_timestamp does not exist!
  Did you mean "current_localtimestamp"?
  
  LINE 56:         CURRENT_TIMESTAMP() as created_at
                   ^.
[0m11:24:31.482143 [debug] [Thread-2 (]: Finished running node model.job_intelligent.dim_time
[0m11:24:31.482881 [debug] [Thread-7 (]: Marking all children of 'model.job_intelligent.dim_time' to be skipped because of status 'error'.  Reason: Runtime Error in model dim_time (models\gold\dim_time.sql)
  Catalog Error: Scalar Function with name current_timestamp does not exist!
  Did you mean "current_localtimestamp"?
  
  LINE 81:     CURRENT_TIMESTAMP() as created_at
               ^.
[0m11:24:31.483557 [debug] [Thread-4 (]: Began running node model.job_intelligent.fact_job_offers
[0m11:24:31.483990 [info ] [Thread-4 (]: 9 of 13 SKIP relation main_gold.fact_job_offers ................................ [[33mSKIP[0m]
[0m11:24:31.484470 [debug] [Thread-4 (]: Finished running node model.job_intelligent.fact_job_offers
[0m11:24:31.485177 [debug] [Thread-2 (]: Began running node model.job_intelligent.fact_job_skills
[0m11:24:31.485800 [info ] [Thread-2 (]: 12 of 13 SKIP relation main_gold.fact_job_skills ............................... [[33mSKIP[0m]
[0m11:24:31.486393 [debug] [Thread-3 (]: Began running node model.job_intelligent.agg_job_offers_by_category_time
[0m11:24:31.486865 [debug] [Thread-1 (]: Began running node model.job_intelligent.agg_location_analysis
[0m11:24:31.487411 [debug] [Thread-2 (]: Finished running node model.job_intelligent.fact_job_skills
[0m11:24:31.487960 [info ] [Thread-3 (]: 10 of 13 SKIP relation main_gold.agg_job_offers_by_category_time ............... [[33mSKIP[0m]
[0m11:24:31.488526 [info ] [Thread-1 (]: 11 of 13 SKIP relation main_gold.agg_location_analysis ......................... [[33mSKIP[0m]
[0m11:24:31.489391 [debug] [Thread-3 (]: Finished running node model.job_intelligent.agg_job_offers_by_category_time
[0m11:24:31.489886 [debug] [Thread-1 (]: Finished running node model.job_intelligent.agg_location_analysis
[0m11:24:31.490317 [debug] [Thread-4 (]: Began running node model.job_intelligent.agg_skills_demand
[0m11:24:31.491105 [info ] [Thread-4 (]: 13 of 13 SKIP relation main_gold.agg_skills_demand ............................. [[33mSKIP[0m]
[0m11:24:31.491586 [debug] [Thread-4 (]: Finished running node model.job_intelligent.agg_skills_demand
[0m11:24:31.493223 [debug] [MainThread]: Using duckdb connection "master"
[0m11:24:31.493559 [debug] [MainThread]: On master: BEGIN
[0m11:24:31.493862 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m11:24:31.494468 [debug] [MainThread]: SQL status: OK in 0.001 seconds
[0m11:24:31.494781 [debug] [MainThread]: On master: COMMIT
[0m11:24:31.495085 [debug] [MainThread]: Using duckdb connection "master"
[0m11:24:31.495382 [debug] [MainThread]: On master: COMMIT
[0m11:24:31.495858 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m11:24:31.496201 [debug] [MainThread]: On master: Close
[0m11:24:31.496588 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:24:31.496872 [debug] [MainThread]: Connection 'create_memory_main_silver' was properly closed.
[0m11:24:31.497143 [debug] [MainThread]: Connection 'create_memory_main_gold' was properly closed.
[0m11:24:31.497418 [debug] [MainThread]: Connection 'create_memory_main_bronze' was properly closed.
[0m11:24:31.497684 [debug] [MainThread]: Connection 'list_memory_main_bronze' was properly closed.
[0m11:24:31.497948 [debug] [MainThread]: Connection 'list_memory_main_silver' was properly closed.
[0m11:24:31.498210 [debug] [MainThread]: Connection 'list_memory_main_gold' was properly closed.
[0m11:24:31.498472 [debug] [MainThread]: Connection 'model.job_intelligent.dim_location' was properly closed.
[0m11:24:31.498734 [debug] [MainThread]: Connection 'model.job_intelligent.dim_time' was properly closed.
[0m11:24:31.498996 [debug] [MainThread]: Connection 'model.job_intelligent.int_skills_extraction' was properly closed.
[0m11:24:31.499257 [debug] [MainThread]: Connection 'model.job_intelligent.dim_company' was properly closed.
[0m11:24:31.499651 [info ] [MainThread]: 
[0m11:24:31.500013 [info ] [MainThread]: Finished running 12 table models, 1 view model in 0 hours 0 minutes and 0.95 seconds (0.95s).
[0m11:24:31.501887 [debug] [MainThread]: Command end result
[0m11:24:31.523099 [debug] [MainThread]: Wrote artifact WritableManifest to D:\lab2\dbt_project\target\manifest.json
[0m11:24:31.525174 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\lab2\dbt_project\target\semantic_manifest.json
[0m11:24:31.530819 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\lab2\dbt_project\target\run_results.json
[0m11:24:31.531188 [info ] [MainThread]: 
[0m11:24:31.531536 [info ] [MainThread]: [31mCompleted with 4 errors, 0 partial successes, and 0 warnings:[0m
[0m11:24:31.531851 [info ] [MainThread]: 
[0m11:24:31.532217 [error] [MainThread]: [31mFailure in model int_skills_extraction (models\silver\int_skills_extraction.sql)[0m
[0m11:24:31.532587 [error] [MainThread]:   Runtime Error in model int_skills_extraction (models\silver\int_skills_extraction.sql)
  Catalog Error: Scalar Function with name ~* does not exist!
  Did you mean "~"?
  
  LINE 57:             WHEN job_description_cleaned ~* s.skill_pattern THEN 1 
                                                    ^
[0m11:24:31.532890 [info ] [MainThread]: 
[0m11:24:31.533231 [info ] [MainThread]:   compiled code at target\compiled\job_intelligent\models\silver\int_skills_extraction.sql
[0m11:24:31.533532 [info ] [MainThread]: 
[0m11:24:31.533879 [error] [MainThread]: [31mFailure in model dim_company (models\gold\dim_company.sql)[0m
[0m11:24:31.534237 [error] [MainThread]:   Runtime Error in model dim_company (models\gold\dim_company.sql)
  Catalog Error: Scalar Function with name current_timestamp does not exist!
  Did you mean "current_localtimestamp"?
  
  LINE 29:         CURRENT_TIMESTAMP() as created_at
                   ^
[0m11:24:31.534531 [info ] [MainThread]: 
[0m11:24:31.535015 [info ] [MainThread]:   compiled code at target\compiled\job_intelligent\models\gold\dim_company.sql
[0m11:24:31.535415 [info ] [MainThread]: 
[0m11:24:31.535831 [error] [MainThread]: [31mFailure in model dim_location (models\gold\dim_location.sql)[0m
[0m11:24:31.536236 [error] [MainThread]:   Runtime Error in model dim_location (models\gold\dim_location.sql)
  Catalog Error: Scalar Function with name current_timestamp does not exist!
  Did you mean "current_localtimestamp"?
  
  LINE 56:         CURRENT_TIMESTAMP() as created_at
                   ^
[0m11:24:31.536565 [info ] [MainThread]: 
[0m11:24:31.536936 [info ] [MainThread]:   compiled code at target\compiled\job_intelligent\models\gold\dim_location.sql
[0m11:24:31.537260 [info ] [MainThread]: 
[0m11:24:31.537644 [error] [MainThread]: [31mFailure in model dim_time (models\gold\dim_time.sql)[0m
[0m11:24:31.538034 [error] [MainThread]:   Runtime Error in model dim_time (models\gold\dim_time.sql)
  Catalog Error: Scalar Function with name current_timestamp does not exist!
  Did you mean "current_localtimestamp"?
  
  LINE 81:     CURRENT_TIMESTAMP() as created_at
               ^
[0m11:24:31.538429 [info ] [MainThread]: 
[0m11:24:31.539019 [info ] [MainThread]:   compiled code at target\compiled\job_intelligent\models\gold\dim_time.sql
[0m11:24:31.539504 [info ] [MainThread]: 
[0m11:24:31.540016 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=4 SKIP=6 NO-OP=0 TOTAL=13
[0m11:24:31.541122 [debug] [MainThread]: Command `dbt run` failed at 11:24:31.541005 after 2.27 seconds
[0m11:24:31.541532 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E4D8F60230>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E4DE46FD10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E4DE46FF50>]}
[0m11:24:31.541990 [debug] [MainThread]: Flushing usage events
[0m11:24:33.015315 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:25:30.781787 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002862E8B9FD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002862C5C4190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002862C437890>]}


============================== 11:25:30.791216 | cfb2d88a-cb0d-4ea7-869f-c08e4210eaa7 ==============================
[0m11:25:30.791216 [info ] [MainThread]: Running with dbt=1.10.13
[0m11:25:30.792333 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'target_path': 'None', 'cache_selected_only': 'False', 'warn_error': 'None', 'profiles_dir': 'D:\\lab2\\dbt_project', 'debug': 'False', 'static_parser': 'True', 'log_cache_events': 'False', 'empty': 'None', 'log_format': 'default', 'quiet': 'False', 'use_colors': 'True', 'partial_parse': 'True', 'version_check': 'True', 'log_path': 'D:\\lab2\\dbt_project\\logs', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'fail_fast': 'False', 'invocation_command': 'dbt debug', 'no_print': 'None', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'write_json': 'True'}
[0m11:25:30.828806 [info ] [MainThread]: dbt version: 1.10.13
[0m11:25:30.829389 [info ] [MainThread]: python version: 3.13.2
[0m11:25:30.829869 [info ] [MainThread]: python path: C:\Users\HP\AppData\Local\Programs\Python\Python313\python.exe
[0m11:25:30.830332 [info ] [MainThread]: os info: Windows-11-10.0.26200-SP0
[0m11:25:30.977763 [info ] [MainThread]: Using profiles dir at D:\lab2\dbt_project
[0m11:25:30.978247 [info ] [MainThread]: Using profiles.yml file at D:\lab2\dbt_project\profiles.yml
[0m11:25:30.978609 [info ] [MainThread]: Using dbt_project.yml file at D:\lab2\dbt_project\dbt_project.yml
[0m11:25:30.981371 [info ] [MainThread]: adapter type: duckdb
[0m11:25:30.981794 [info ] [MainThread]: adapter version: 1.10.0
[0m11:25:31.118691 [info ] [MainThread]: Configuration:
[0m11:25:31.119885 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m11:25:31.120245 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m11:25:31.120541 [info ] [MainThread]: Required dependencies:
[0m11:25:31.120980 [debug] [MainThread]: Executing "git --help"
[0m11:25:31.163913 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--no-lazy-fetch]\n           [--no-optional-locks] [--no-advice] [--bare] [--git-dir=<path>]\n           [--work-tree=<path>] [--namespace=<name>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone      Clone a repository into a new directory\n   init       Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add        Add file contents to the index\n   mv         Move or rename a file, a directory, or a symlink\n   restore    Restore working tree files\n   rm         Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect     Use binary search to find the commit that introduced a bug\n   diff       Show changes between commits, commit and working tree, etc\n   grep       Print lines matching a pattern\n   log        Show commit logs\n   show       Show various types of objects\n   status     Show the working tree status\n\ngrow, mark and tweak your common history\n   backfill   Download missing objects in a partial clone\n   branch     List, create, or delete branches\n   commit     Record changes to the repository\n   merge      Join two or more development histories together\n   rebase     Reapply commits on top of another base tip\n   reset      Reset current HEAD to the specified state\n   switch     Switch branches\n   tag        Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch      Download objects and refs from another repository\n   pull       Fetch from and integrate with another repository or a local branch\n   push       Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m11:25:31.164467 [debug] [MainThread]: STDERR: "b''"
[0m11:25:31.164951 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m11:25:31.165357 [info ] [MainThread]: Connection:
[0m11:25:31.165736 [info ] [MainThread]:   database: memory
[0m11:25:31.166101 [info ] [MainThread]:   schema: main
[0m11:25:31.166452 [info ] [MainThread]:   path: :memory:
[0m11:25:31.166801 [info ] [MainThread]:   config_options: None
[0m11:25:31.167151 [info ] [MainThread]:   extensions: None
[0m11:25:31.167502 [info ] [MainThread]:   settings: {}
[0m11:25:31.167910 [info ] [MainThread]:   external_root: .
[0m11:25:31.168269 [info ] [MainThread]:   use_credential_provider: None
[0m11:25:31.168623 [info ] [MainThread]:   attach: None
[0m11:25:31.168973 [info ] [MainThread]:   filesystems: None
[0m11:25:31.169321 [info ] [MainThread]:   remote: None
[0m11:25:31.169670 [info ] [MainThread]:   plugins: None
[0m11:25:31.170028 [info ] [MainThread]:   disable_transactions: False
[0m11:25:31.170681 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m11:25:31.450622 [debug] [MainThread]: Acquiring new duckdb connection 'debug'
[0m11:25:31.514156 [debug] [MainThread]: Using duckdb connection "debug"
[0m11:25:31.514498 [debug] [MainThread]: On debug: select 1 as id
[0m11:25:31.514759 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:25:31.530957 [debug] [MainThread]: SQL status: OK in 0.016 seconds
[0m11:25:31.531851 [debug] [MainThread]: On debug: Close
[0m11:25:31.532220 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m11:25:31.532595 [info ] [MainThread]: [32mAll checks passed![0m
[0m11:25:31.533536 [debug] [MainThread]: Command `dbt debug` succeeded at 11:25:31.533409 after 0.98 seconds
[0m11:25:31.533864 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m11:25:31.534215 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028630DD9F30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028630E7D5B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028630D50E20>]}
[0m11:25:31.534616 [debug] [MainThread]: Flushing usage events
[0m11:25:32.611878 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:25:36.122170 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028BEA6F9FD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028BE8400190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028BE8277890>]}


============================== 11:25:36.132408 | 413dc72c-d267-4be9-932d-4e94250d521e ==============================
[0m11:25:36.132408 [info ] [MainThread]: Running with dbt=1.10.13
[0m11:25:36.133547 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'write_json': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'empty': 'False', 'debug': 'False', 'static_parser': 'True', 'log_cache_events': 'False', 'profiles_dir': 'D:\\lab2\\dbt_project', 'quiet': 'False', 'log_format': 'default', 'use_colors': 'True', 'partial_parse': 'True', 'version_check': 'True', 'log_path': 'D:\\lab2\\dbt_project\\logs', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'fail_fast': 'False', 'invocation_command': 'dbt run', 'no_print': 'None', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'target_path': 'None'}
[0m11:25:36.476691 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '413dc72c-d267-4be9-932d-4e94250d521e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028BEABAF230>]}
[0m11:25:36.554664 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '413dc72c-d267-4be9-932d-4e94250d521e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028BEA768380>]}
[0m11:25:36.557467 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m11:25:36.805891 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m11:25:36.990051 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 4 files changed.
[0m11:25:36.990741 [debug] [MainThread]: Partial parsing: updated file: job_intelligent://models\gold\dim_time.sql
[0m11:25:36.991186 [debug] [MainThread]: Partial parsing: updated file: job_intelligent://models\silver\int_skills_extraction.sql
[0m11:25:36.991598 [debug] [MainThread]: Partial parsing: updated file: job_intelligent://models\gold\dim_company.sql
[0m11:25:36.992008 [debug] [MainThread]: Partial parsing: updated file: job_intelligent://models\gold\dim_location.sql
[0m11:25:37.419759 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- seeds
[0m11:25:37.425388 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '413dc72c-d267-4be9-932d-4e94250d521e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028BECA94A50>]}
[0m11:25:37.486641 [debug] [MainThread]: Wrote artifact WritableManifest to D:\lab2\dbt_project\target\manifest.json
[0m11:25:37.489090 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\lab2\dbt_project\target\semantic_manifest.json
[0m11:25:37.522573 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '413dc72c-d267-4be9-932d-4e94250d521e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028BEB6EF7A0>]}
[0m11:25:37.523101 [info ] [MainThread]: Found 13 models, 456 macros
[0m11:25:37.523510 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '413dc72c-d267-4be9-932d-4e94250d521e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028BECBC1B70>]}
[0m11:25:37.525724 [info ] [MainThread]: 
[0m11:25:37.526157 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m11:25:37.526524 [info ] [MainThread]: 
[0m11:25:37.527071 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m11:25:37.533086 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_memory'
[0m11:25:37.548905 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_memory'
[0m11:25:37.552026 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_memory'
[0m11:25:37.635447 [debug] [ThreadPool]: Using duckdb connection "list_memory"
[0m11:25:37.636021 [debug] [ThreadPool]: On list_memory: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_memory"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"memory"'
    
  
  
[0m11:25:37.636309 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:25:37.636749 [debug] [ThreadPool]: Using duckdb connection "list_memory"
[0m11:25:37.637084 [debug] [ThreadPool]: On list_memory: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_memory"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"memory"'
    
  
  
[0m11:25:37.637376 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:25:37.638026 [debug] [ThreadPool]: Using duckdb connection "list_memory"
[0m11:25:37.638350 [debug] [ThreadPool]: On list_memory: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_memory"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"memory"'
    
  
  
[0m11:25:37.638625 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:25:37.655440 [debug] [ThreadPool]: SQL status: OK in 0.018 seconds
[0m11:25:37.656986 [debug] [ThreadPool]: On list_memory: Close
[0m11:25:37.657726 [debug] [ThreadPool]: SQL status: OK in 0.019 seconds
[0m11:25:37.658198 [debug] [ThreadPool]: SQL status: OK in 0.022 seconds
[0m11:25:37.659504 [debug] [ThreadPool]: On list_memory: Close
[0m11:25:37.660901 [debug] [ThreadPool]: On list_memory: Close
[0m11:25:37.661682 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_memory, now create_memory_main_bronze)
[0m11:25:37.662149 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_memory, now create_memory_main_silver)
[0m11:25:37.662808 [debug] [ThreadPool]: Creating schema "database: "memory"
schema: "main_bronze"
"
[0m11:25:37.663372 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_memory, now create_memory_main_gold)
[0m11:25:37.663968 [debug] [ThreadPool]: Creating schema "database: "memory"
schema: "main_silver"
"
[0m11:25:37.671377 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_bronze"
[0m11:25:37.672135 [debug] [ThreadPool]: Creating schema "database: "memory"
schema: "main_gold"
"
[0m11:25:37.674688 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_silver"
[0m11:25:37.675247 [debug] [ThreadPool]: On create_memory_main_bronze: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_memory_main_bronze"} */

    
        select type from duckdb_databases()
        where lower(database_name)='memory'
        and type='sqlite'
    
  
[0m11:25:37.677208 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_gold"
[0m11:25:37.677739 [debug] [ThreadPool]: On create_memory_main_silver: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_memory_main_silver"} */

    
        select type from duckdb_databases()
        where lower(database_name)='memory'
        and type='sqlite'
    
  
[0m11:25:37.678205 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:25:37.678703 [debug] [ThreadPool]: On create_memory_main_gold: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_memory_main_gold"} */

    
        select type from duckdb_databases()
        where lower(database_name)='memory'
        and type='sqlite'
    
  
[0m11:25:37.679169 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:25:37.679967 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:25:37.680891 [debug] [ThreadPool]: SQL status: OK in 0.003 seconds
[0m11:25:37.682494 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_bronze"
[0m11:25:37.682940 [debug] [ThreadPool]: On create_memory_main_bronze: BEGIN
[0m11:25:37.683453 [debug] [ThreadPool]: SQL status: OK in 0.004 seconds
[0m11:25:37.683934 [debug] [ThreadPool]: SQL status: OK in 0.004 seconds
[0m11:25:37.685400 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_silver"
[0m11:25:37.685798 [debug] [ThreadPool]: On create_memory_main_silver: BEGIN
[0m11:25:37.687318 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_gold"
[0m11:25:37.687715 [debug] [ThreadPool]: On create_memory_main_gold: BEGIN
[0m11:25:37.688235 [debug] [ThreadPool]: SQL status: OK in 0.005 seconds
[0m11:25:37.688624 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_bronze"
[0m11:25:37.689014 [debug] [ThreadPool]: On create_memory_main_bronze: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_memory_main_bronze"} */

    
    
        create schema if not exists "memory"."main_bronze"
    
[0m11:25:37.689652 [debug] [ThreadPool]: SQL status: OK in 0.002 seconds
[0m11:25:37.690044 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_gold"
[0m11:25:37.690448 [debug] [ThreadPool]: On create_memory_main_gold: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_memory_main_gold"} */

    
    
        create schema if not exists "memory"."main_gold"
    
[0m11:25:37.690902 [debug] [ThreadPool]: SQL status: OK in 0.005 seconds
[0m11:25:37.691351 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_silver"
[0m11:25:37.691741 [debug] [ThreadPool]: On create_memory_main_silver: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_memory_main_silver"} */

    
    
        create schema if not exists "memory"."main_silver"
    
[0m11:25:37.692466 [debug] [ThreadPool]: SQL status: OK in 0.003 seconds
[0m11:25:37.693493 [debug] [ThreadPool]: On create_memory_main_bronze: COMMIT
[0m11:25:37.693890 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_bronze"
[0m11:25:37.694274 [debug] [ThreadPool]: On create_memory_main_bronze: COMMIT
[0m11:25:37.694782 [debug] [ThreadPool]: SQL status: OK in 0.004 seconds
[0m11:25:37.695807 [debug] [ThreadPool]: On create_memory_main_gold: COMMIT
[0m11:25:37.696201 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_gold"
[0m11:25:37.696568 [debug] [ThreadPool]: On create_memory_main_gold: COMMIT
[0m11:25:37.697029 [debug] [ThreadPool]: SQL status: OK in 0.005 seconds
[0m11:25:37.698054 [debug] [ThreadPool]: On create_memory_main_silver: COMMIT
[0m11:25:37.698454 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_silver"
[0m11:25:37.698828 [debug] [ThreadPool]: On create_memory_main_silver: COMMIT
[0m11:25:37.699483 [debug] [ThreadPool]: SQL status: OK in 0.005 seconds
[0m11:25:37.700010 [debug] [ThreadPool]: On create_memory_main_bronze: Close
[0m11:25:37.700459 [debug] [ThreadPool]: SQL status: OK in 0.004 seconds
[0m11:25:37.700877 [debug] [ThreadPool]: On create_memory_main_gold: Close
[0m11:25:37.701882 [debug] [ThreadPool]: SQL status: OK in 0.003 seconds
[0m11:25:37.702274 [debug] [ThreadPool]: On create_memory_main_silver: Close
[0m11:25:37.705172 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_memory_main_bronze'
[0m11:25:37.711294 [debug] [ThreadPool]: Using duckdb connection "list_memory_main_bronze"
[0m11:25:37.711693 [debug] [ThreadPool]: On list_memory_main_bronze: BEGIN
[0m11:25:37.712043 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:25:37.712919 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m11:25:37.713309 [debug] [ThreadPool]: Using duckdb connection "list_memory_main_bronze"
[0m11:25:37.713701 [debug] [ThreadPool]: On list_memory_main_bronze: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_memory_main_bronze"} */
select
      'memory' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main_bronze'
    and lower(table_catalog) = 'memory'
  
[0m11:25:37.714447 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_memory_main_silver'
[0m11:25:37.716871 [debug] [ThreadPool]: Using duckdb connection "list_memory_main_silver"
[0m11:25:37.717280 [debug] [ThreadPool]: On list_memory_main_silver: BEGIN
[0m11:25:37.717640 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:25:37.718490 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m11:25:37.718924 [debug] [ThreadPool]: Using duckdb connection "list_memory_main_silver"
[0m11:25:37.719316 [debug] [ThreadPool]: On list_memory_main_silver: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_memory_main_silver"} */
select
      'memory' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main_silver'
    and lower(table_catalog) = 'memory'
  
[0m11:25:37.720073 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_memory_main_gold'
[0m11:25:37.722050 [debug] [ThreadPool]: Using duckdb connection "list_memory_main_gold"
[0m11:25:37.722559 [debug] [ThreadPool]: On list_memory_main_gold: BEGIN
[0m11:25:37.722927 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:25:37.723650 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m11:25:37.724025 [debug] [ThreadPool]: Using duckdb connection "list_memory_main_gold"
[0m11:25:37.724421 [debug] [ThreadPool]: On list_memory_main_gold: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_memory_main_gold"} */
select
      'memory' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main_gold'
    and lower(table_catalog) = 'memory'
  
[0m11:25:37.734108 [debug] [ThreadPool]: SQL status: OK in 0.009 seconds
[0m11:25:37.735731 [debug] [ThreadPool]: On list_memory_main_gold: ROLLBACK
[0m11:25:37.736292 [debug] [ThreadPool]: SQL status: OK in 0.022 seconds
[0m11:25:37.737689 [debug] [ThreadPool]: On list_memory_main_bronze: ROLLBACK
[0m11:25:37.738203 [debug] [ThreadPool]: SQL status: OK in 0.018 seconds
[0m11:25:37.739650 [debug] [ThreadPool]: On list_memory_main_silver: ROLLBACK
[0m11:25:37.741893 [debug] [ThreadPool]: Failed to rollback 'list_memory_main_bronze'
[0m11:25:37.742303 [debug] [ThreadPool]: On list_memory_main_bronze: Close
[0m11:25:37.743777 [debug] [ThreadPool]: Failed to rollback 'list_memory_main_gold'
[0m11:25:37.744165 [debug] [ThreadPool]: On list_memory_main_gold: Close
[0m11:25:37.745428 [debug] [ThreadPool]: Failed to rollback 'list_memory_main_silver'
[0m11:25:37.745819 [debug] [ThreadPool]: On list_memory_main_silver: Close
[0m11:25:37.746971 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '413dc72c-d267-4be9-932d-4e94250d521e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028BED3268F0>]}
[0m11:25:37.747758 [debug] [MainThread]: Using duckdb connection "master"
[0m11:25:37.748155 [debug] [MainThread]: On master: BEGIN
[0m11:25:37.748526 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:25:37.749335 [debug] [MainThread]: SQL status: OK in 0.001 seconds
[0m11:25:37.749718 [debug] [MainThread]: On master: COMMIT
[0m11:25:37.750084 [debug] [MainThread]: Using duckdb connection "master"
[0m11:25:37.750439 [debug] [MainThread]: On master: COMMIT
[0m11:25:37.751040 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m11:25:37.751428 [debug] [MainThread]: On master: Close
[0m11:25:37.757563 [debug] [Thread-1 (]: Began running node model.job_intelligent.stg_jobs_raw
[0m11:25:37.758235 [info ] [Thread-1 (]: 1 of 13 START sql view model main_bronze.stg_jobs_raw .......................... [RUN]
[0m11:25:37.758926 [debug] [Thread-1 (]: Acquiring new duckdb connection 'model.job_intelligent.stg_jobs_raw'
[0m11:25:37.759392 [debug] [Thread-1 (]: Began compiling node model.job_intelligent.stg_jobs_raw
[0m11:25:37.768582 [debug] [Thread-1 (]: Writing injected SQL for node "model.job_intelligent.stg_jobs_raw"
[0m11:25:37.769573 [debug] [Thread-1 (]: Began executing node model.job_intelligent.stg_jobs_raw
[0m11:25:37.800416 [debug] [Thread-1 (]: Writing runtime sql for node "model.job_intelligent.stg_jobs_raw"
[0m11:25:37.801322 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.stg_jobs_raw"
[0m11:25:37.801753 [debug] [Thread-1 (]: On model.job_intelligent.stg_jobs_raw: BEGIN
[0m11:25:37.802066 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m11:25:37.802850 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m11:25:37.803183 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.stg_jobs_raw"
[0m11:25:37.803533 [debug] [Thread-1 (]: On model.job_intelligent.stg_jobs_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.stg_jobs_raw"} */

  
  create view "memory"."main_bronze"."stg_jobs_raw__dbt_tmp" as (
    -- models/bronze/stg_jobs_raw.sql
-- Bronze layer: Lecture directe des données brutes depuis final_data.csv
-- Renommage et typage minimal



-- Read directly from CSV file
SELECT
    -- Renommer les colonnes pour cohérence
    title as job_title,
    location,
    postedTime as posted_time,
    publishedAt as published_at,
    jobUrl as job_url,
    companyName as company_name,
    companyUrl as company_url,
    description as job_description,
    contractType as contract_type,
    workType as work_type,
    
    -- Métadonnées de traçabilité
    NOW() as ingestion_timestamp,
    '2026-01-08 10:25:36.003463+00:00' as dbt_run_id

FROM read_csv_auto('D:/lab2/final_data.csv')

WHERE 1=1
    -- Filtrer les lignes vides
    AND title IS NOT NULL
    AND description IS NOT NULL
  );

[0m11:25:37.843370 [debug] [Thread-1 (]: SQL status: OK in 0.039 seconds
[0m11:25:37.848583 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.stg_jobs_raw"
[0m11:25:37.848930 [debug] [Thread-1 (]: On model.job_intelligent.stg_jobs_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.stg_jobs_raw"} */
alter view "memory"."main_bronze"."stg_jobs_raw__dbt_tmp" rename to "stg_jobs_raw"
[0m11:25:37.849564 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m11:25:37.858255 [debug] [Thread-1 (]: On model.job_intelligent.stg_jobs_raw: COMMIT
[0m11:25:37.858646 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.stg_jobs_raw"
[0m11:25:37.859110 [debug] [Thread-1 (]: On model.job_intelligent.stg_jobs_raw: COMMIT
[0m11:25:37.859876 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m11:25:37.864495 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.stg_jobs_raw"
[0m11:25:37.864856 [debug] [Thread-1 (]: On model.job_intelligent.stg_jobs_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.stg_jobs_raw"} */

      drop view if exists "memory"."main_bronze"."stg_jobs_raw__dbt_backup" cascade
    
[0m11:25:37.865529 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m11:25:37.867582 [debug] [Thread-1 (]: On model.job_intelligent.stg_jobs_raw: Close
[0m11:25:37.869595 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '413dc72c-d267-4be9-932d-4e94250d521e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028BECC493D0>]}
[0m11:25:37.870211 [info ] [Thread-1 (]: 1 of 13 OK created sql view model main_bronze.stg_jobs_raw ..................... [[32mOK[0m in 0.11s]
[0m11:25:37.870856 [debug] [Thread-1 (]: Finished running node model.job_intelligent.stg_jobs_raw
[0m11:25:37.871724 [debug] [Thread-2 (]: Began running node model.job_intelligent.int_jobs_cleaned
[0m11:25:37.872156 [info ] [Thread-2 (]: 2 of 13 START sql table model main_silver.int_jobs_cleaned ..................... [RUN]
[0m11:25:37.872652 [debug] [Thread-2 (]: Acquiring new duckdb connection 'model.job_intelligent.int_jobs_cleaned'
[0m11:25:37.872991 [debug] [Thread-2 (]: Began compiling node model.job_intelligent.int_jobs_cleaned
[0m11:25:37.876228 [debug] [Thread-2 (]: Writing injected SQL for node "model.job_intelligent.int_jobs_cleaned"
[0m11:25:37.877021 [debug] [Thread-2 (]: Began executing node model.job_intelligent.int_jobs_cleaned
[0m11:25:37.894173 [debug] [Thread-2 (]: Writing runtime sql for node "model.job_intelligent.int_jobs_cleaned"
[0m11:25:37.895069 [debug] [Thread-2 (]: Using duckdb connection "model.job_intelligent.int_jobs_cleaned"
[0m11:25:37.895469 [debug] [Thread-2 (]: On model.job_intelligent.int_jobs_cleaned: BEGIN
[0m11:25:37.895784 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m11:25:37.896438 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m11:25:37.896760 [debug] [Thread-2 (]: Using duckdb connection "model.job_intelligent.int_jobs_cleaned"
[0m11:25:37.897138 [debug] [Thread-2 (]: On model.job_intelligent.int_jobs_cleaned: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.int_jobs_cleaned"} */

  
    
    

    create  table
      "memory"."main_silver"."int_jobs_cleaned__dbt_tmp"
  
    as (
      -- models/silver/int_jobs_cleaned.sql
-- Silver layer: Nettoyage et normalisation des données brutes



WITH raw_jobs AS (
    SELECT * FROM "memory"."main_bronze"."stg_jobs_raw"
),

cleaned_jobs AS (
    SELECT
        -- Texte: lowercase, trim, remove special characters
        LOWER(TRIM(job_title)) as job_title_cleaned,
        LOWER(TRIM(location)) as location_cleaned,
        LOWER(TRIM(company_name)) as company_name_cleaned,
        LOWER(TRIM(job_description)) as job_description_cleaned,
        LOWER(TRIM(contract_type)) as contract_type_cleaned,
        LOWER(TRIM(work_type)) as work_type_cleaned,
        
        -- URLs as-is
        job_url,
        company_url,
        
        -- Dates
        TRY_CAST(published_at AS DATE) as published_date,
        posted_time,
        
        -- Extract year-month for time-based analysis
        DATE_TRUNC('month', TRY_CAST(published_at AS DATE)) as published_year_month,
        EXTRACT(YEAR FROM TRY_CAST(published_at AS DATE)) as published_year,
        EXTRACT(MONTH FROM TRY_CAST(published_at AS DATE)) as published_month,
        
        -- Métadonnées
        ingestion_timestamp
    FROM raw_jobs
)

SELECT
    *,
    -- Deduplication flag
    ROW_NUMBER() OVER (
        PARTITION BY job_title_cleaned, company_name_cleaned, location_cleaned 
        ORDER BY published_date DESC
    ) as dedup_rank
FROM cleaned_jobs
    );
  
  
[0m11:25:38.023768 [debug] [Thread-2 (]: SQL status: OK in 0.126 seconds
[0m11:25:38.026650 [debug] [Thread-2 (]: Using duckdb connection "model.job_intelligent.int_jobs_cleaned"
[0m11:25:38.027004 [debug] [Thread-2 (]: On model.job_intelligent.int_jobs_cleaned: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.int_jobs_cleaned"} */
alter table "memory"."main_silver"."int_jobs_cleaned__dbt_tmp" rename to "int_jobs_cleaned"
[0m11:25:38.027757 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m11:25:38.031954 [debug] [Thread-2 (]: On model.job_intelligent.int_jobs_cleaned: COMMIT
[0m11:25:38.032303 [debug] [Thread-2 (]: Using duckdb connection "model.job_intelligent.int_jobs_cleaned"
[0m11:25:38.032631 [debug] [Thread-2 (]: On model.job_intelligent.int_jobs_cleaned: COMMIT
[0m11:25:38.036763 [debug] [Thread-2 (]: SQL status: OK in 0.004 seconds
[0m11:25:38.038739 [debug] [Thread-2 (]: Using duckdb connection "model.job_intelligent.int_jobs_cleaned"
[0m11:25:38.039080 [debug] [Thread-2 (]: On model.job_intelligent.int_jobs_cleaned: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.int_jobs_cleaned"} */

      drop table if exists "memory"."main_silver"."int_jobs_cleaned__dbt_backup" cascade
    
[0m11:25:38.039676 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m11:25:38.040844 [debug] [Thread-2 (]: On model.job_intelligent.int_jobs_cleaned: Close
[0m11:25:38.041333 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '413dc72c-d267-4be9-932d-4e94250d521e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028BED3B0050>]}
[0m11:25:38.041871 [info ] [Thread-2 (]: 2 of 13 OK created sql table model main_silver.int_jobs_cleaned ................ [[32mOK[0m in 0.17s]
[0m11:25:38.042449 [debug] [Thread-2 (]: Finished running node model.job_intelligent.int_jobs_cleaned
[0m11:25:38.043290 [debug] [Thread-3 (]: Began running node model.job_intelligent.int_job_title_normalization
[0m11:25:38.043732 [info ] [Thread-3 (]: 3 of 13 START sql table model main_silver.int_job_title_normalization .......... [RUN]
[0m11:25:38.044227 [debug] [Thread-3 (]: Acquiring new duckdb connection 'model.job_intelligent.int_job_title_normalization'
[0m11:25:38.044568 [debug] [Thread-3 (]: Began compiling node model.job_intelligent.int_job_title_normalization
[0m11:25:38.047736 [debug] [Thread-3 (]: Writing injected SQL for node "model.job_intelligent.int_job_title_normalization"
[0m11:25:38.048666 [debug] [Thread-3 (]: Began executing node model.job_intelligent.int_job_title_normalization
[0m11:25:38.051605 [debug] [Thread-3 (]: Writing runtime sql for node "model.job_intelligent.int_job_title_normalization"
[0m11:25:38.052505 [debug] [Thread-3 (]: Using duckdb connection "model.job_intelligent.int_job_title_normalization"
[0m11:25:38.052871 [debug] [Thread-3 (]: On model.job_intelligent.int_job_title_normalization: BEGIN
[0m11:25:38.053187 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m11:25:38.053841 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m11:25:38.054180 [debug] [Thread-3 (]: Using duckdb connection "model.job_intelligent.int_job_title_normalization"
[0m11:25:38.054594 [debug] [Thread-3 (]: On model.job_intelligent.int_job_title_normalization: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.int_job_title_normalization"} */

  
    
    

    create  table
      "memory"."main_silver"."int_job_title_normalization__dbt_tmp"
  
    as (
      -- models/silver/int_job_title_normalization.sql
-- Silver layer: Normaliser les intitulés de postes



WITH cleaned_jobs AS (
    SELECT * FROM "memory"."main_silver"."int_jobs_cleaned"
),

title_normalized AS (
    SELECT
        *,
        CASE
            WHEN job_title_cleaned LIKE '%data engineer%' THEN 'Data Engineer'
            WHEN job_title_cleaned LIKE '%data scientist%' THEN 'Data Scientist'
            WHEN job_title_cleaned LIKE '%data analyst%' THEN 'Data Analyst'
            WHEN job_title_cleaned LIKE '%analytics engineer%' THEN 'Analytics Engineer'
            WHEN job_title_cleaned LIKE '%ml engineer%' OR job_title_cleaned LIKE '%machine learning%' THEN 'ML Engineer'
            WHEN job_title_cleaned LIKE '%data architect%' THEN 'Data Architect'
            WHEN job_title_cleaned LIKE '%bi developer%' OR job_title_cleaned LIKE '%business intelligence%' THEN 'BI Developer'
            WHEN job_title_cleaned LIKE '%etl%' OR job_title_cleaned LIKE '%pipeline%' THEN 'ETL/Pipeline Engineer'
            WHEN job_title_cleaned LIKE '%data engineer%' THEN 'Data Engineer'
            ELSE 'Other Data Role'
        END as job_category,
        
        CASE
            WHEN contract_type_cleaned LIKE '%cdi%' OR contract_type_cleaned LIKE '%permanent%' THEN 'Permanent'
            WHEN contract_type_cleaned LIKE '%cdd%' OR contract_type_cleaned LIKE '%contract%' THEN 'Contract'
            WHEN contract_type_cleaned LIKE '%stage%' OR contract_type_cleaned LIKE '%internship%' THEN 'Internship'
            WHEN contract_type_cleaned LIKE '%freelance%' THEN 'Freelance'
            ELSE 'Not Specified'
        END as contract_type_normalized,
        
        CASE
            WHEN work_type_cleaned LIKE '%remote%' THEN 'Remote'
            WHEN work_type_cleaned LIKE '%hybrid%' THEN 'Hybrid'
            WHEN work_type_cleaned LIKE '%onsite%' OR work_type_cleaned LIKE '%on-site%' THEN 'On-site'
            ELSE 'Not Specified'
        END as work_type_normalized
    
    FROM cleaned_jobs
    WHERE dedup_rank = 1  -- Keep only first occurrence (most recent)
)

SELECT * FROM title_normalized
    );
  
  
[0m11:25:38.079656 [debug] [Thread-3 (]: SQL status: OK in 0.025 seconds
[0m11:25:38.082288 [debug] [Thread-3 (]: Using duckdb connection "model.job_intelligent.int_job_title_normalization"
[0m11:25:38.082679 [debug] [Thread-3 (]: On model.job_intelligent.int_job_title_normalization: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.int_job_title_normalization"} */
alter table "memory"."main_silver"."int_job_title_normalization__dbt_tmp" rename to "int_job_title_normalization"
[0m11:25:38.083646 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m11:25:38.084966 [debug] [Thread-3 (]: On model.job_intelligent.int_job_title_normalization: COMMIT
[0m11:25:38.085317 [debug] [Thread-3 (]: Using duckdb connection "model.job_intelligent.int_job_title_normalization"
[0m11:25:38.085638 [debug] [Thread-3 (]: On model.job_intelligent.int_job_title_normalization: COMMIT
[0m11:25:38.086202 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m11:25:38.088296 [debug] [Thread-3 (]: Using duckdb connection "model.job_intelligent.int_job_title_normalization"
[0m11:25:38.088638 [debug] [Thread-3 (]: On model.job_intelligent.int_job_title_normalization: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.int_job_title_normalization"} */

      drop table if exists "memory"."main_silver"."int_job_title_normalization__dbt_backup" cascade
    
[0m11:25:38.089242 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m11:25:38.090348 [debug] [Thread-3 (]: On model.job_intelligent.int_job_title_normalization: Close
[0m11:25:38.090806 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '413dc72c-d267-4be9-932d-4e94250d521e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028BEB8AE350>]}
[0m11:25:38.091349 [info ] [Thread-3 (]: 3 of 13 OK created sql table model main_silver.int_job_title_normalization ..... [[32mOK[0m in 0.05s]
[0m11:25:38.091918 [debug] [Thread-3 (]: Finished running node model.job_intelligent.int_job_title_normalization
[0m11:25:38.092659 [debug] [Thread-3 (]: Began running node model.job_intelligent.int_skills_extraction
[0m11:25:38.093090 [info ] [Thread-3 (]: 7 of 13 START sql table model main_silver.int_skills_extraction ................ [RUN]
[0m11:25:38.093508 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly model.job_intelligent.int_job_title_normalization, now model.job_intelligent.int_skills_extraction)
[0m11:25:38.093848 [debug] [Thread-3 (]: Began compiling node model.job_intelligent.int_skills_extraction
[0m11:25:38.096550 [debug] [Thread-3 (]: Writing injected SQL for node "model.job_intelligent.int_skills_extraction"
[0m11:25:38.096991 [debug] [Thread-4 (]: Began running node model.job_intelligent.dim_company
[0m11:25:38.097455 [info ] [Thread-4 (]: 4 of 13 START sql table model main_gold.dim_company ............................ [RUN]
[0m11:25:38.098101 [debug] [Thread-4 (]: Acquiring new duckdb connection 'model.job_intelligent.dim_company'
[0m11:25:38.098435 [debug] [Thread-4 (]: Began compiling node model.job_intelligent.dim_company
[0m11:25:38.101766 [debug] [Thread-4 (]: Writing injected SQL for node "model.job_intelligent.dim_company"
[0m11:25:38.102318 [debug] [Thread-1 (]: Began running node model.job_intelligent.dim_location
[0m11:25:38.102850 [info ] [Thread-1 (]: 5 of 13 START sql table model main_gold.dim_location ........................... [RUN]
[0m11:25:38.103342 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.job_intelligent.stg_jobs_raw, now model.job_intelligent.dim_location)
[0m11:25:38.103762 [debug] [Thread-1 (]: Began compiling node model.job_intelligent.dim_location
[0m11:25:38.106507 [debug] [Thread-1 (]: Writing injected SQL for node "model.job_intelligent.dim_location"
[0m11:25:38.107169 [debug] [Thread-2 (]: Began running node model.job_intelligent.dim_time
[0m11:25:38.107639 [info ] [Thread-2 (]: 6 of 13 START sql table model main_gold.dim_time ............................... [RUN]
[0m11:25:38.108058 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly model.job_intelligent.int_jobs_cleaned, now model.job_intelligent.dim_time)
[0m11:25:38.108385 [debug] [Thread-2 (]: Began compiling node model.job_intelligent.dim_time
[0m11:25:38.110987 [debug] [Thread-2 (]: Writing injected SQL for node "model.job_intelligent.dim_time"
[0m11:25:38.111941 [debug] [Thread-2 (]: Began executing node model.job_intelligent.dim_time
[0m11:25:38.112338 [debug] [Thread-4 (]: Began executing node model.job_intelligent.dim_company
[0m11:25:38.115507 [debug] [Thread-2 (]: Writing runtime sql for node "model.job_intelligent.dim_time"
[0m11:25:38.118825 [debug] [Thread-4 (]: Writing runtime sql for node "model.job_intelligent.dim_company"
[0m11:25:38.119408 [debug] [Thread-3 (]: Began executing node model.job_intelligent.int_skills_extraction
[0m11:25:38.119965 [debug] [Thread-1 (]: Began executing node model.job_intelligent.dim_location
[0m11:25:38.123437 [debug] [Thread-3 (]: Writing runtime sql for node "model.job_intelligent.int_skills_extraction"
[0m11:25:38.126168 [debug] [Thread-1 (]: Writing runtime sql for node "model.job_intelligent.dim_location"
[0m11:25:38.126902 [debug] [Thread-4 (]: Using duckdb connection "model.job_intelligent.dim_company"
[0m11:25:38.127436 [debug] [Thread-2 (]: Using duckdb connection "model.job_intelligent.dim_time"
[0m11:25:38.128038 [debug] [Thread-4 (]: On model.job_intelligent.dim_company: BEGIN
[0m11:25:38.128602 [debug] [Thread-2 (]: On model.job_intelligent.dim_time: BEGIN
[0m11:25:38.129195 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.dim_location"
[0m11:25:38.129764 [debug] [Thread-3 (]: Using duckdb connection "model.job_intelligent.int_skills_extraction"
[0m11:25:38.130171 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m11:25:38.130532 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m11:25:38.130938 [debug] [Thread-1 (]: On model.job_intelligent.dim_location: BEGIN
[0m11:25:38.131412 [debug] [Thread-3 (]: On model.job_intelligent.int_skills_extraction: BEGIN
[0m11:25:38.132377 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:25:38.132759 [debug] [Thread-4 (]: SQL status: OK in 0.003 seconds
[0m11:25:38.133193 [debug] [Thread-2 (]: SQL status: OK in 0.003 seconds
[0m11:25:38.133625 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m11:25:38.134405 [debug] [Thread-4 (]: Using duckdb connection "model.job_intelligent.dim_company"
[0m11:25:38.134940 [debug] [Thread-1 (]: SQL status: OK in 0.003 seconds
[0m11:25:38.135318 [debug] [Thread-2 (]: Using duckdb connection "model.job_intelligent.dim_time"
[0m11:25:38.136096 [debug] [Thread-4 (]: On model.job_intelligent.dim_company: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_company"} */

  
    
    

    create  table
      "memory"."main_gold"."dim_company__dbt_tmp"
  
    as (
      -- models/gold/dim_company.sql
-- Gold layer: Dimension Company



WITH jobs AS (
    SELECT DISTINCT
        company_name_cleaned,
        company_url
    FROM "memory"."main_silver"."int_job_title_normalization"
    WHERE company_name_cleaned IS NOT NULL
),

ranked_companies AS (
    SELECT
        ROW_NUMBER() OVER (ORDER BY company_name_cleaned) as company_id,
        company_name_cleaned as company_name,
        company_url,
        NOW() as created_at
    FROM jobs
)

SELECT
    company_id,
    company_name,
    company_url,
    created_at
FROM ranked_companies
ORDER BY company_id
    );
  
  
[0m11:25:38.136587 [debug] [Thread-3 (]: SQL status: OK in 0.003 seconds
[0m11:25:38.136963 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.dim_location"
[0m11:25:38.137518 [debug] [Thread-2 (]: On model.job_intelligent.dim_time: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_time"} */

  
    
    

    create  table
      "memory"."main_gold"."dim_time__dbt_tmp"
  
    as (
      -- models/gold/dim_time.sql
-- Gold layer: Dimension Time



-- Generate a date dimension for time-based analysis
WITH date_spine AS (
    SELECT
        published_date,
        EXTRACT(YEAR FROM published_date) as year,
        EXTRACT(MONTH FROM published_date) as month,
        EXTRACT(QUARTER FROM published_date) as quarter,
        EXTRACT(WEEK FROM published_date) as week,
        EXTRACT(DAYOFWEEK FROM published_date) as day_of_week,
        DATE_TRUNC('month', published_date) as month_start,
        DATE_TRUNC('quarter', published_date) as quarter_start,
        DATE_TRUNC('year', published_date) as year_start,
        
        CASE 
            WHEN EXTRACT(MONTH FROM published_date) IN (1,2,3) THEN 'Q1'
            WHEN EXTRACT(MONTH FROM published_date) IN (4,5,6) THEN 'Q2'
            WHEN EXTRACT(MONTH FROM published_date) IN (7,8,9) THEN 'Q3'
            ELSE 'Q4'
        END as quarter_name,
        
        CASE EXTRACT(DAYOFWEEK FROM published_date)
            WHEN 0 THEN 'Sunday'
            WHEN 1 THEN 'Monday'
            WHEN 2 THEN 'Tuesday'
            WHEN 3 THEN 'Wednesday'
            WHEN 4 THEN 'Thursday'
            WHEN 5 THEN 'Friday'
            WHEN 6 THEN 'Saturday'
        END as day_name,
        
        CASE EXTRACT(MONTH FROM published_date)
            WHEN 1 THEN 'January'
            WHEN 2 THEN 'February'
            WHEN 3 THEN 'March'
            WHEN 4 THEN 'April'
            WHEN 5 THEN 'May'
            WHEN 6 THEN 'June'
            WHEN 7 THEN 'July'
            WHEN 8 THEN 'August'
            WHEN 9 THEN 'September'
            WHEN 10 THEN 'October'
            WHEN 11 THEN 'November'
            WHEN 12 THEN 'December'
        END as month_name
        
    FROM (
        SELECT DISTINCT published_date
        FROM "memory"."main_silver"."int_job_title_normalization"
        WHERE published_date IS NOT NULL
    )
)

SELECT
    published_date as date_id,
    year,
    month,
    quarter,
    week,
    day_of_week,
    day_name,
    month_name,
    quarter_name,
    month_start,
    quarter_start,
    year_start,
    NOW() as created_at
FROM date_spine
ORDER BY published_date
    );
  
  
[0m11:25:38.138241 [debug] [Thread-3 (]: Using duckdb connection "model.job_intelligent.int_skills_extraction"
[0m11:25:38.138820 [debug] [Thread-1 (]: On model.job_intelligent.dim_location: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_location"} */

  
    
    

    create  table
      "memory"."main_gold"."dim_location__dbt_tmp"
  
    as (
      -- models/gold/dim_location.sql
-- Gold layer: Dimension Location



WITH jobs AS (
    SELECT DISTINCT
        location_cleaned as location_raw
    FROM "memory"."main_silver"."int_job_title_normalization"
    WHERE location_cleaned IS NOT NULL
),

location_parsed AS (
    SELECT
        location_raw,
        -- Extract city (before comma)
        CASE 
            WHEN POSITION(',' IN location_raw) > 0 
            THEN TRIM(SUBSTRING(location_raw, 1, POSITION(',' IN location_raw) - 1))
            ELSE location_raw
        END as city,
        
        -- Extract country (after comma)
        CASE 
            WHEN POSITION(',' IN location_raw) > 0 
            THEN TRIM(SUBSTRING(location_raw, POSITION(',' IN location_raw) + 1))
            ELSE 'Not Specified'
        END as country,
        
        -- Detect if remote
        CASE 
            WHEN location_raw LIKE '%remote%' 
            THEN 'Remote'
            ELSE 'On-site'
        END as work_location_type
    FROM jobs
),

ranked_locations AS (
    SELECT
        ROW_NUMBER() OVER (ORDER BY location_raw) as location_id,
        location_raw,
        city,
        country,
        work_location_type,
        NOW() as created_at
    FROM location_parsed
)

SELECT
    location_id,
    location_raw,
    city,
    country,
    work_location_type,
    created_at
FROM ranked_locations
ORDER BY location_id
    );
  
  
[0m11:25:38.139886 [debug] [Thread-3 (]: On model.job_intelligent.int_skills_extraction: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.int_skills_extraction"} */

  
    
    

    create  table
      "memory"."main_silver"."int_skills_extraction__dbt_tmp"
  
    as (
      -- models/silver/int_skills_extraction.sql
-- Silver layer: Extraction des compétences depuis la description



WITH jobs_with_titles AS (
    SELECT * FROM "memory"."main_silver"."int_job_title_normalization"
),

skills_mapping AS (
    -- Définir un mapping de compétences communes Data/Tech
    SELECT
        'Python' as skill_name,
        'python|py |\.py' as skill_pattern
    UNION ALL SELECT 'SQL', 'sql|sql|sql server|postgres|oracle'
    UNION ALL SELECT 'Spark', 'spark|pyspark'
    UNION ALL SELECT 'Hadoop', 'hadoop|hdfs'
    UNION ALL SELECT 'Scala', 'scala'
    UNION ALL SELECT 'Java', '\bjava\b'
    UNION ALL SELECT 'R', '\br\b|r programming'
    UNION ALL SELECT 'Tableau', 'tableau'
    UNION ALL SELECT 'Power BI', 'power bi|powerbi'
    UNION ALL SELECT 'Looker', 'looker'
    UNION ALL SELECT 'AWS', 'aws|amazon web|s3 |ec2|redshift'
    UNION ALL SELECT 'Azure', 'azure|microsoft azure|synapse|cosmos'
    UNION ALL SELECT 'GCP', 'gcp|google cloud|bigquery'
    UNION ALL SELECT 'Airflow', 'airflow'
    UNION ALL SELECT 'DBT', '\bdbt\b|dbt'
    UNION ALL SELECT 'Kubernetes', 'kubernetes|k8s'
    UNION ALL SELECT 'Docker', 'docker'
    UNION ALL SELECT 'Git', 'git|github|gitlab'
    UNION ALL SELECT 'TensorFlow', 'tensorflow'
    UNION ALL SELECT 'PyTorch', 'pytorch'
    UNION ALL SELECT 'Scikit-learn', 'scikit|sklearn'
    UNION ALL SELECT 'Pandas', 'pandas'
    UNION ALL SELECT 'NumPy', 'numpy'
    UNION ALL SELECT 'Machine Learning', 'machine learning|deep learning|ml|artificial intelligence'
    UNION ALL SELECT 'Statistics', 'statistics|statistical|probability'
    UNION ALL SELECT 'Data Visualization', 'data visualization|visualization|charts|graphs'
),

jobs_exploded AS (
    SELECT
        j.*,
        s.skill_name,
        CASE 
            WHEN job_description_cleaned ILIKE '%' || s.skill_pattern || '%' THEN 1 
            ELSE 0 
        END as has_skill
    FROM jobs_with_titles j
    CROSS JOIN skills_mapping s
)

SELECT
    *
FROM jobs_exploded
WHERE has_skill = 1

ORDER BY job_title_cleaned, published_date DESC
    );
  
  
[0m11:25:38.147412 [debug] [Thread-4 (]: SQL status: OK in 0.009 seconds
[0m11:25:38.151065 [debug] [Thread-4 (]: Using duckdb connection "model.job_intelligent.dim_company"
[0m11:25:38.151683 [debug] [Thread-4 (]: On model.job_intelligent.dim_company: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_company"} */
alter table "memory"."main_gold"."dim_company__dbt_tmp" rename to "dim_company"
[0m11:25:38.153121 [debug] [Thread-4 (]: SQL status: OK in 0.001 seconds
[0m11:25:38.154812 [debug] [Thread-4 (]: On model.job_intelligent.dim_company: COMMIT
[0m11:25:38.155227 [debug] [Thread-4 (]: Using duckdb connection "model.job_intelligent.dim_company"
[0m11:25:38.155592 [debug] [Thread-4 (]: On model.job_intelligent.dim_company: COMMIT
[0m11:25:38.156201 [debug] [Thread-2 (]: SQL status: OK in 0.017 seconds
[0m11:25:38.158539 [debug] [Thread-2 (]: Using duckdb connection "model.job_intelligent.dim_time"
[0m11:25:38.158916 [debug] [Thread-2 (]: On model.job_intelligent.dim_time: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_time"} */
alter table "memory"."main_gold"."dim_time__dbt_tmp" rename to "dim_time"
[0m11:25:38.159459 [debug] [Thread-1 (]: SQL status: OK in 0.019 seconds
[0m11:25:38.161884 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.dim_location"
[0m11:25:38.162277 [debug] [Thread-1 (]: On model.job_intelligent.dim_location: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_location"} */
alter table "memory"."main_gold"."dim_location__dbt_tmp" rename to "dim_location"
[0m11:25:38.162830 [debug] [Thread-4 (]: SQL status: OK in 0.007 seconds
[0m11:25:38.164853 [debug] [Thread-4 (]: Using duckdb connection "model.job_intelligent.dim_company"
[0m11:25:38.165239 [debug] [Thread-4 (]: On model.job_intelligent.dim_company: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_company"} */

      drop table if exists "memory"."main_gold"."dim_company__dbt_backup" cascade
    
[0m11:25:38.165906 [debug] [Thread-2 (]: SQL status: OK in 0.007 seconds
[0m11:25:38.167265 [debug] [Thread-2 (]: On model.job_intelligent.dim_time: COMMIT
[0m11:25:38.167689 [debug] [Thread-2 (]: Using duckdb connection "model.job_intelligent.dim_time"
[0m11:25:38.168045 [debug] [Thread-2 (]: On model.job_intelligent.dim_time: COMMIT
[0m11:25:38.168640 [debug] [Thread-1 (]: SQL status: OK in 0.006 seconds
[0m11:25:38.170023 [debug] [Thread-1 (]: On model.job_intelligent.dim_location: COMMIT
[0m11:25:38.170403 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.dim_location"
[0m11:25:38.170760 [debug] [Thread-1 (]: On model.job_intelligent.dim_location: COMMIT
[0m11:25:38.171409 [debug] [Thread-2 (]: SQL status: OK in 0.003 seconds
[0m11:25:38.173204 [debug] [Thread-2 (]: Using duckdb connection "model.job_intelligent.dim_time"
[0m11:25:38.173576 [debug] [Thread-2 (]: On model.job_intelligent.dim_time: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_time"} */

      drop table if exists "memory"."main_gold"."dim_time__dbt_backup" cascade
    
[0m11:25:38.174203 [debug] [Thread-4 (]: SQL status: OK in 0.009 seconds
[0m11:25:38.175397 [debug] [Thread-4 (]: On model.job_intelligent.dim_company: Close
[0m11:25:38.175928 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '413dc72c-d267-4be9-932d-4e94250d521e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028BED3FE7D0>]}
[0m11:25:38.176498 [info ] [Thread-4 (]: 4 of 13 OK created sql table model main_gold.dim_company ....................... [[32mOK[0m in 0.08s]
[0m11:25:38.177175 [debug] [Thread-4 (]: Finished running node model.job_intelligent.dim_company
[0m11:25:38.177612 [debug] [Thread-2 (]: SQL status: OK in 0.004 seconds
[0m11:25:38.178703 [debug] [Thread-2 (]: On model.job_intelligent.dim_time: Close
[0m11:25:38.179429 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '413dc72c-d267-4be9-932d-4e94250d521e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028BED4EBE00>]}
[0m11:25:38.180178 [info ] [Thread-2 (]: 6 of 13 OK created sql table model main_gold.dim_time .......................... [[32mOK[0m in 0.07s]
[0m11:25:38.181048 [debug] [Thread-2 (]: Finished running node model.job_intelligent.dim_time
[0m11:25:38.181586 [debug] [Thread-1 (]: SQL status: OK in 0.010 seconds
[0m11:25:38.183756 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.dim_location"
[0m11:25:38.184107 [debug] [Thread-1 (]: On model.job_intelligent.dim_location: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_location"} */

      drop table if exists "memory"."main_gold"."dim_location__dbt_backup" cascade
    
[0m11:25:38.184760 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m11:25:38.185915 [debug] [Thread-1 (]: On model.job_intelligent.dim_location: Close
[0m11:25:38.186373 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '413dc72c-d267-4be9-932d-4e94250d521e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028BED4FB3B0>]}
[0m11:25:38.186894 [info ] [Thread-1 (]: 5 of 13 OK created sql table model main_gold.dim_location ...................... [[32mOK[0m in 0.08s]
[0m11:25:38.187497 [debug] [Thread-1 (]: Finished running node model.job_intelligent.dim_location
[0m11:25:38.188182 [debug] [Thread-4 (]: Began running node model.job_intelligent.fact_job_offers
[0m11:25:38.188600 [info ] [Thread-4 (]: 8 of 13 START sql table model main_gold.fact_job_offers ........................ [RUN]
[0m11:25:38.189053 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly model.job_intelligent.dim_company, now model.job_intelligent.fact_job_offers)
[0m11:25:38.189382 [debug] [Thread-4 (]: Began compiling node model.job_intelligent.fact_job_offers
[0m11:25:38.192696 [debug] [Thread-4 (]: Writing injected SQL for node "model.job_intelligent.fact_job_offers"
[0m11:25:38.193546 [debug] [Thread-4 (]: Began executing node model.job_intelligent.fact_job_offers
[0m11:25:38.196135 [debug] [Thread-4 (]: Writing runtime sql for node "model.job_intelligent.fact_job_offers"
[0m11:25:38.197073 [debug] [Thread-4 (]: Using duckdb connection "model.job_intelligent.fact_job_offers"
[0m11:25:38.197496 [debug] [Thread-4 (]: On model.job_intelligent.fact_job_offers: BEGIN
[0m11:25:38.197808 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m11:25:38.198476 [debug] [Thread-4 (]: SQL status: OK in 0.001 seconds
[0m11:25:38.198800 [debug] [Thread-4 (]: Using duckdb connection "model.job_intelligent.fact_job_offers"
[0m11:25:38.199216 [debug] [Thread-4 (]: On model.job_intelligent.fact_job_offers: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.fact_job_offers"} */

  
    
    

    create  table
      "memory"."main_gold"."fact_job_offers__dbt_tmp"
  
    as (
      -- models/gold/fact_job_offers.sql
-- Gold layer: Fact Table - Job Offers (Schéma en Étoile)



WITH jobs AS (
    SELECT
        j.*
    FROM "memory"."main_silver"."int_job_title_normalization" j
),

companies AS (
    SELECT * FROM "memory"."main_gold"."dim_company"
),

locations AS (
    SELECT * FROM "memory"."main_gold"."dim_location"
),

times AS (
    SELECT * FROM "memory"."main_gold"."dim_time"
),

fact_table AS (
    SELECT
        -- Surrogate keys
        ROW_NUMBER() OVER (ORDER BY j.job_url, j.company_name_cleaned) as job_offer_id,
        
        -- Foreign keys
        c.company_id,
        l.location_id,
        t.date_id as published_date_id,
        
        -- Job dimensions
        j.job_title_cleaned as job_title,
        j.job_category,
        j.contract_type_normalized as contract_type,
        j.work_type_normalized as work_type,
        
        -- URLs
        j.job_url,
        j.company_url,
        
        -- Description
        j.job_description_cleaned as job_description,
        
        -- Time dimension
        j.published_date,
        j.posted_time,
        j.published_year_month,
        j.published_year,
        j.published_month,
        
        -- Metrics
        LENGTH(j.job_description_cleaned) as description_length,
        (LENGTH(j.job_description_cleaned) - LENGTH(REPLACE(j.job_description_cleaned, ' ', ''))) + 1 as word_count,
        
        -- Flags
        CASE WHEN j.work_type_normalized = 'Remote' THEN 1 ELSE 0 END as is_remote,
        CASE WHEN j.contract_type_normalized = 'Permanent' THEN 1 ELSE 0 END as is_permanent,
        
        -- Metadata
        CURRENT_TIMESTAMP() as created_at,
        j.ingestion_timestamp
        
    FROM jobs j
    LEFT JOIN companies c ON j.company_name_cleaned = c.company_name
    LEFT JOIN locations l ON j.location_cleaned = l.location_raw
    LEFT JOIN times t ON j.published_date = t.date_id
)

SELECT
    job_offer_id,
    company_id,
    location_id,
    published_date_id,
    job_title,
    job_category,
    contract_type,
    work_type,
    job_url,
    company_url,
    job_description,
    published_date,
    posted_time,
    published_year_month,
    published_year,
    published_month,
    description_length,
    word_count,
    is_remote,
    is_permanent,
    created_at,
    ingestion_timestamp
FROM fact_table
ORDER BY published_date DESC, job_offer_id
    );
  
  
[0m11:25:38.203108 [debug] [Thread-4 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.fact_job_offers"} */

  
    
    

    create  table
      "memory"."main_gold"."fact_job_offers__dbt_tmp"
  
    as (
      -- models/gold/fact_job_offers.sql
-- Gold layer: Fact Table - Job Offers (Schéma en Étoile)



WITH jobs AS (
    SELECT
        j.*
    FROM "memory"."main_silver"."int_job_title_normalization" j
),

companies AS (
    SELECT * FROM "memory"."main_gold"."dim_company"
),

locations AS (
    SELECT * FROM "memory"."main_gold"."dim_location"
),

times AS (
    SELECT * FROM "memory"."main_gold"."dim_time"
),

fact_table AS (
    SELECT
        -- Surrogate keys
        ROW_NUMBER() OVER (ORDER BY j.job_url, j.company_name_cleaned) as job_offer_id,
        
        -- Foreign keys
        c.company_id,
        l.location_id,
        t.date_id as published_date_id,
        
        -- Job dimensions
        j.job_title_cleaned as job_title,
        j.job_category,
        j.contract_type_normalized as contract_type,
        j.work_type_normalized as work_type,
        
        -- URLs
        j.job_url,
        j.company_url,
        
        -- Description
        j.job_description_cleaned as job_description,
        
        -- Time dimension
        j.published_date,
        j.posted_time,
        j.published_year_month,
        j.published_year,
        j.published_month,
        
        -- Metrics
        LENGTH(j.job_description_cleaned) as description_length,
        (LENGTH(j.job_description_cleaned) - LENGTH(REPLACE(j.job_description_cleaned, ' ', ''))) + 1 as word_count,
        
        -- Flags
        CASE WHEN j.work_type_normalized = 'Remote' THEN 1 ELSE 0 END as is_remote,
        CASE WHEN j.contract_type_normalized = 'Permanent' THEN 1 ELSE 0 END as is_permanent,
        
        -- Metadata
        CURRENT_TIMESTAMP() as created_at,
        j.ingestion_timestamp
        
    FROM jobs j
    LEFT JOIN companies c ON j.company_name_cleaned = c.company_name
    LEFT JOIN locations l ON j.location_cleaned = l.location_raw
    LEFT JOIN times t ON j.published_date = t.date_id
)

SELECT
    job_offer_id,
    company_id,
    location_id,
    published_date_id,
    job_title,
    job_category,
    contract_type,
    work_type,
    job_url,
    company_url,
    job_description,
    published_date,
    posted_time,
    published_year_month,
    published_year,
    published_month,
    description_length,
    word_count,
    is_remote,
    is_permanent,
    created_at,
    ingestion_timestamp
FROM fact_table
ORDER BY published_date DESC, job_offer_id
    );
  
  
[0m11:25:38.203617 [debug] [Thread-4 (]: DuckDB adapter: Rolling back transaction.
[0m11:25:38.204043 [debug] [Thread-4 (]: On model.job_intelligent.fact_job_offers: ROLLBACK
[0m11:25:38.210998 [debug] [Thread-4 (]: Failed to rollback 'model.job_intelligent.fact_job_offers'
[0m11:25:38.211471 [debug] [Thread-4 (]: On model.job_intelligent.fact_job_offers: Close
[0m11:25:38.215337 [debug] [Thread-4 (]: Runtime Error in model fact_job_offers (models\gold\fact_job_offers.sql)
  Catalog Error: Scalar Function with name current_timestamp does not exist!
  Did you mean "current_localtimestamp"?
  
  LINE 73:         CURRENT_TIMESTAMP() as created_at,
                   ^
[0m11:25:38.215764 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '413dc72c-d267-4be9-932d-4e94250d521e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028BEE5DBB90>]}
[0m11:25:38.216313 [error] [Thread-4 (]: 8 of 13 ERROR creating sql table model main_gold.fact_job_offers ............... [[31mERROR[0m in 0.03s]
[0m11:25:38.216889 [debug] [Thread-4 (]: Finished running node model.job_intelligent.fact_job_offers
[0m11:25:38.217519 [debug] [Thread-7 (]: Marking all children of 'model.job_intelligent.fact_job_offers' to be skipped because of status 'error'.  Reason: Runtime Error in model fact_job_offers (models\gold\fact_job_offers.sql)
  Catalog Error: Scalar Function with name current_timestamp does not exist!
  Did you mean "current_localtimestamp"?
  
  LINE 73:         CURRENT_TIMESTAMP() as created_at,
                   ^.
[0m11:25:38.218725 [debug] [Thread-2 (]: Began running node model.job_intelligent.agg_job_offers_by_category_time
[0m11:25:38.219529 [debug] [Thread-1 (]: Began running node model.job_intelligent.agg_location_analysis
[0m11:25:38.219101 [info ] [Thread-2 (]: 9 of 13 SKIP relation main_gold.agg_job_offers_by_category_time ................ [[33mSKIP[0m]
[0m11:25:38.220667 [debug] [Thread-2 (]: Finished running node model.job_intelligent.agg_job_offers_by_category_time
[0m11:25:38.220073 [info ] [Thread-1 (]: 10 of 13 SKIP relation main_gold.agg_location_analysis ......................... [[33mSKIP[0m]
[0m11:25:38.221364 [debug] [Thread-1 (]: Finished running node model.job_intelligent.agg_location_analysis
[0m11:25:39.939837 [debug] [Thread-3 (]: SQL status: OK in 1.799 seconds
[0m11:25:39.942781 [debug] [Thread-3 (]: Using duckdb connection "model.job_intelligent.int_skills_extraction"
[0m11:25:39.943130 [debug] [Thread-3 (]: On model.job_intelligent.int_skills_extraction: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.int_skills_extraction"} */
alter table "memory"."main_silver"."int_skills_extraction__dbt_tmp" rename to "int_skills_extraction"
[0m11:25:39.943809 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m11:25:39.945043 [debug] [Thread-3 (]: On model.job_intelligent.int_skills_extraction: COMMIT
[0m11:25:39.945372 [debug] [Thread-3 (]: Using duckdb connection "model.job_intelligent.int_skills_extraction"
[0m11:25:39.945682 [debug] [Thread-3 (]: On model.job_intelligent.int_skills_extraction: COMMIT
[0m11:25:39.946432 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m11:25:39.948489 [debug] [Thread-3 (]: Using duckdb connection "model.job_intelligent.int_skills_extraction"
[0m11:25:39.948878 [debug] [Thread-3 (]: On model.job_intelligent.int_skills_extraction: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.int_skills_extraction"} */

      drop table if exists "memory"."main_silver"."int_skills_extraction__dbt_backup" cascade
    
[0m11:25:39.949683 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m11:25:39.950965 [debug] [Thread-3 (]: On model.job_intelligent.int_skills_extraction: Close
[0m11:25:39.951464 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '413dc72c-d267-4be9-932d-4e94250d521e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028BED4FAC30>]}
[0m11:25:39.952038 [info ] [Thread-3 (]: 7 of 13 OK created sql table model main_silver.int_skills_extraction ........... [[32mOK[0m in 1.86s]
[0m11:25:39.952627 [debug] [Thread-3 (]: Finished running node model.job_intelligent.int_skills_extraction
[0m11:25:39.953504 [debug] [Thread-4 (]: Began running node model.job_intelligent.dim_skills
[0m11:25:39.953917 [info ] [Thread-4 (]: 11 of 13 START sql table model main_gold.dim_skills ............................ [RUN]
[0m11:25:39.954377 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly model.job_intelligent.fact_job_offers, now model.job_intelligent.dim_skills)
[0m11:25:39.954730 [debug] [Thread-4 (]: Began compiling node model.job_intelligent.dim_skills
[0m11:25:39.957810 [debug] [Thread-4 (]: Writing injected SQL for node "model.job_intelligent.dim_skills"
[0m11:25:39.958676 [debug] [Thread-4 (]: Began executing node model.job_intelligent.dim_skills
[0m11:25:39.964424 [debug] [Thread-4 (]: Writing runtime sql for node "model.job_intelligent.dim_skills"
[0m11:25:39.965703 [debug] [Thread-4 (]: Using duckdb connection "model.job_intelligent.dim_skills"
[0m11:25:39.966103 [debug] [Thread-4 (]: On model.job_intelligent.dim_skills: BEGIN
[0m11:25:39.966595 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m11:25:39.967360 [debug] [Thread-4 (]: SQL status: OK in 0.001 seconds
[0m11:25:39.967720 [debug] [Thread-4 (]: Using duckdb connection "model.job_intelligent.dim_skills"
[0m11:25:39.968132 [debug] [Thread-4 (]: On model.job_intelligent.dim_skills: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_skills"} */

  
    
    

    create  table
      "memory"."main_gold"."dim_skills__dbt_tmp"
  
    as (
      -- models/gold/dim_skills.sql
-- Gold layer: Dimension Skills



WITH skills AS (
    SELECT DISTINCT
        skill_name
    FROM "memory"."main_silver"."int_skills_extraction"
    WHERE skill_name IS NOT NULL
),

skill_categorization AS (
    SELECT
        skill_name,
        CASE
            WHEN skill_name IN ('Python', 'Java', 'Scala', 'R') THEN 'Programming Language'
            WHEN skill_name IN ('SQL', 'NoSQL') THEN 'Database'
            WHEN skill_name IN ('Spark', 'Hadoop', 'Hive', 'Kafka') THEN 'Big Data Framework'
            WHEN skill_name IN ('TensorFlow', 'PyTorch', 'Scikit-learn') THEN 'ML/DL Library'
            WHEN skill_name IN ('AWS', 'Azure', 'GCP') THEN 'Cloud Platform'
            WHEN skill_name IN ('Tableau', 'Power BI', 'Looker') THEN 'BI Tool'
            WHEN skill_name IN ('Airflow', 'DBT', 'Kubernetes', 'Docker') THEN 'DataOps/DevOps'
            WHEN skill_name IN ('Pandas', 'NumPy', 'Matplotlib') THEN 'Data Analysis Library'
            WHEN skill_name IN ('Machine Learning', 'Statistics', 'Data Visualization') THEN 'Domain Knowledge'
            ELSE 'Other'
        END as skill_category
    FROM skills
),

ranked_skills AS (
    SELECT
        ROW_NUMBER() OVER (ORDER BY skill_name) as skill_id,
        skill_name,
        skill_category,
        CURRENT_TIMESTAMP() as created_at
    FROM skill_categorization
)

SELECT
    skill_id,
    skill_name,
    skill_category,
    created_at
FROM ranked_skills
ORDER BY skill_id
    );
  
  
[0m11:25:39.970126 [debug] [Thread-4 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_skills"} */

  
    
    

    create  table
      "memory"."main_gold"."dim_skills__dbt_tmp"
  
    as (
      -- models/gold/dim_skills.sql
-- Gold layer: Dimension Skills



WITH skills AS (
    SELECT DISTINCT
        skill_name
    FROM "memory"."main_silver"."int_skills_extraction"
    WHERE skill_name IS NOT NULL
),

skill_categorization AS (
    SELECT
        skill_name,
        CASE
            WHEN skill_name IN ('Python', 'Java', 'Scala', 'R') THEN 'Programming Language'
            WHEN skill_name IN ('SQL', 'NoSQL') THEN 'Database'
            WHEN skill_name IN ('Spark', 'Hadoop', 'Hive', 'Kafka') THEN 'Big Data Framework'
            WHEN skill_name IN ('TensorFlow', 'PyTorch', 'Scikit-learn') THEN 'ML/DL Library'
            WHEN skill_name IN ('AWS', 'Azure', 'GCP') THEN 'Cloud Platform'
            WHEN skill_name IN ('Tableau', 'Power BI', 'Looker') THEN 'BI Tool'
            WHEN skill_name IN ('Airflow', 'DBT', 'Kubernetes', 'Docker') THEN 'DataOps/DevOps'
            WHEN skill_name IN ('Pandas', 'NumPy', 'Matplotlib') THEN 'Data Analysis Library'
            WHEN skill_name IN ('Machine Learning', 'Statistics', 'Data Visualization') THEN 'Domain Knowledge'
            ELSE 'Other'
        END as skill_category
    FROM skills
),

ranked_skills AS (
    SELECT
        ROW_NUMBER() OVER (ORDER BY skill_name) as skill_id,
        skill_name,
        skill_category,
        CURRENT_TIMESTAMP() as created_at
    FROM skill_categorization
)

SELECT
    skill_id,
    skill_name,
    skill_category,
    created_at
FROM ranked_skills
ORDER BY skill_id
    );
  
  
[0m11:25:39.970586 [debug] [Thread-4 (]: DuckDB adapter: Rolling back transaction.
[0m11:25:39.971031 [debug] [Thread-4 (]: On model.job_intelligent.dim_skills: ROLLBACK
[0m11:25:39.975573 [debug] [Thread-4 (]: Failed to rollback 'model.job_intelligent.dim_skills'
[0m11:25:39.975930 [debug] [Thread-4 (]: On model.job_intelligent.dim_skills: Close
[0m11:25:39.979973 [debug] [Thread-4 (]: Runtime Error in model dim_skills (models\gold\dim_skills.sql)
  Catalog Error: Scalar Function with name current_timestamp does not exist!
  Did you mean "current_localtimestamp"?
  
  LINE 46:         CURRENT_TIMESTAMP() as created_at
                   ^
[0m11:25:39.980417 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '413dc72c-d267-4be9-932d-4e94250d521e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028BED4FB590>]}
[0m11:25:39.981017 [error] [Thread-4 (]: 11 of 13 ERROR creating sql table model main_gold.dim_skills ................... [[31mERROR[0m in 0.03s]
[0m11:25:39.981627 [debug] [Thread-4 (]: Finished running node model.job_intelligent.dim_skills
[0m11:25:39.982244 [debug] [Thread-7 (]: Marking all children of 'model.job_intelligent.dim_skills' to be skipped because of status 'error'.  Reason: Runtime Error in model dim_skills (models\gold\dim_skills.sql)
  Catalog Error: Scalar Function with name current_timestamp does not exist!
  Did you mean "current_localtimestamp"?
  
  LINE 46:         CURRENT_TIMESTAMP() as created_at
                   ^.
[0m11:25:39.983084 [debug] [Thread-2 (]: Began running node model.job_intelligent.fact_job_skills
[0m11:25:39.983517 [info ] [Thread-2 (]: 12 of 13 SKIP relation main_gold.fact_job_skills ............................... [[33mSKIP[0m]
[0m11:25:39.984006 [debug] [Thread-2 (]: Finished running node model.job_intelligent.fact_job_skills
[0m11:25:39.984877 [debug] [Thread-1 (]: Began running node model.job_intelligent.agg_skills_demand
[0m11:25:39.985223 [info ] [Thread-1 (]: 13 of 13 SKIP relation main_gold.agg_skills_demand ............................. [[33mSKIP[0m]
[0m11:25:39.985627 [debug] [Thread-1 (]: Finished running node model.job_intelligent.agg_skills_demand
[0m11:25:39.987418 [debug] [MainThread]: Using duckdb connection "master"
[0m11:25:39.987712 [debug] [MainThread]: On master: BEGIN
[0m11:25:39.987966 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m11:25:39.988568 [debug] [MainThread]: SQL status: OK in 0.001 seconds
[0m11:25:39.988838 [debug] [MainThread]: On master: COMMIT
[0m11:25:39.989101 [debug] [MainThread]: Using duckdb connection "master"
[0m11:25:39.989353 [debug] [MainThread]: On master: COMMIT
[0m11:25:39.989857 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m11:25:39.990134 [debug] [MainThread]: On master: Close
[0m11:25:39.990481 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:25:39.990729 [debug] [MainThread]: Connection 'create_memory_main_silver' was properly closed.
[0m11:25:39.990964 [debug] [MainThread]: Connection 'create_memory_main_gold' was properly closed.
[0m11:25:39.991195 [debug] [MainThread]: Connection 'create_memory_main_bronze' was properly closed.
[0m11:25:39.991426 [debug] [MainThread]: Connection 'list_memory_main_bronze' was properly closed.
[0m11:25:39.991654 [debug] [MainThread]: Connection 'list_memory_main_silver' was properly closed.
[0m11:25:39.991883 [debug] [MainThread]: Connection 'list_memory_main_gold' was properly closed.
[0m11:25:39.992112 [debug] [MainThread]: Connection 'model.job_intelligent.dim_location' was properly closed.
[0m11:25:39.992347 [debug] [MainThread]: Connection 'model.job_intelligent.dim_time' was properly closed.
[0m11:25:39.992575 [debug] [MainThread]: Connection 'model.job_intelligent.int_skills_extraction' was properly closed.
[0m11:25:39.992803 [debug] [MainThread]: Connection 'model.job_intelligent.dim_skills' was properly closed.
[0m11:25:39.993144 [info ] [MainThread]: 
[0m11:25:39.993456 [info ] [MainThread]: Finished running 12 table models, 1 view model in 0 hours 0 minutes and 2.47 seconds (2.47s).
[0m11:25:39.995209 [debug] [MainThread]: Command end result
[0m11:25:40.016139 [debug] [MainThread]: Wrote artifact WritableManifest to D:\lab2\dbt_project\target\manifest.json
[0m11:25:40.018330 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\lab2\dbt_project\target\semantic_manifest.json
[0m11:25:40.024091 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\lab2\dbt_project\target\run_results.json
[0m11:25:40.024455 [info ] [MainThread]: 
[0m11:25:40.024854 [info ] [MainThread]: [31mCompleted with 2 errors, 0 partial successes, and 0 warnings:[0m
[0m11:25:40.025176 [info ] [MainThread]: 
[0m11:25:40.025543 [error] [MainThread]: [31mFailure in model fact_job_offers (models\gold\fact_job_offers.sql)[0m
[0m11:25:40.025915 [error] [MainThread]:   Runtime Error in model fact_job_offers (models\gold\fact_job_offers.sql)
  Catalog Error: Scalar Function with name current_timestamp does not exist!
  Did you mean "current_localtimestamp"?
  
  LINE 73:         CURRENT_TIMESTAMP() as created_at,
                   ^
[0m11:25:40.026213 [info ] [MainThread]: 
[0m11:25:40.026562 [info ] [MainThread]:   compiled code at target\compiled\job_intelligent\models\gold\fact_job_offers.sql
[0m11:25:40.026857 [info ] [MainThread]: 
[0m11:25:40.027204 [error] [MainThread]: [31mFailure in model dim_skills (models\gold\dim_skills.sql)[0m
[0m11:25:40.027566 [error] [MainThread]:   Runtime Error in model dim_skills (models\gold\dim_skills.sql)
  Catalog Error: Scalar Function with name current_timestamp does not exist!
  Did you mean "current_localtimestamp"?
  
  LINE 46:         CURRENT_TIMESTAMP() as created_at
                   ^
[0m11:25:40.027862 [info ] [MainThread]: 
[0m11:25:40.028198 [info ] [MainThread]:   compiled code at target\compiled\job_intelligent\models\gold\dim_skills.sql
[0m11:25:40.028490 [info ] [MainThread]: 
[0m11:25:40.028811 [info ] [MainThread]: Done. PASS=7 WARN=0 ERROR=2 SKIP=4 NO-OP=0 TOTAL=13
[0m11:25:40.029626 [debug] [MainThread]: Command `dbt run` failed at 11:25:40.029498 after 4.10 seconds
[0m11:25:40.029966 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028BE9F41250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028BEE609550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028BEB1F7830>]}
[0m11:25:40.030295 [debug] [MainThread]: Flushing usage events
[0m11:25:41.419254 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:32:13.722034 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ED3C455FD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ED3A164190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ED39FD7890>]}


============================== 11:32:13.733200 | 0ef3892f-5dd3-4f39-80e0-5f145d846a83 ==============================
[0m11:32:13.733200 [info ] [MainThread]: Running with dbt=1.10.13
[0m11:32:13.734349 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'target_path': 'None', 'cache_selected_only': 'False', 'warn_error': 'None', 'empty': 'False', 'debug': 'False', 'static_parser': 'True', 'log_cache_events': 'False', 'profiles_dir': 'D:\\lab2\\dbt_project', 'quiet': 'False', 'log_format': 'default', 'use_colors': 'True', 'partial_parse': 'True', 'version_check': 'True', 'log_path': 'D:\\lab2\\dbt_project\\logs', 'introspect': 'True', 'invocation_command': 'dbt run', 'fail_fast': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'no_print': 'None', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'write_json': 'True', 'use_experimental_parser': 'False'}
[0m11:32:14.093525 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '0ef3892f-5dd3-4f39-80e0-5f145d846a83', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ED3C913230>]}
[0m11:32:14.184847 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '0ef3892f-5dd3-4f39-80e0-5f145d846a83', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ED3C4C8380>]}
[0m11:32:14.187584 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m11:32:14.477346 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m11:32:14.600566 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m11:32:14.600928 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m11:32:14.607056 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- seeds
[0m11:32:14.627754 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '0ef3892f-5dd3-4f39-80e0-5f145d846a83', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ED3D7A4850>]}
[0m11:32:14.684710 [debug] [MainThread]: Wrote artifact WritableManifest to D:\lab2\dbt_project\target\manifest.json
[0m11:32:14.686767 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\lab2\dbt_project\target\semantic_manifest.json
[0m11:32:14.718218 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '0ef3892f-5dd3-4f39-80e0-5f145d846a83', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ED3D44B7A0>]}
[0m11:32:14.718678 [info ] [MainThread]: Found 13 models, 456 macros
[0m11:32:14.719016 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0ef3892f-5dd3-4f39-80e0-5f145d846a83', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ED3ED25470>]}
[0m11:32:14.720855 [info ] [MainThread]: 
[0m11:32:14.721205 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m11:32:14.721500 [info ] [MainThread]: 
[0m11:32:14.721939 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m11:32:14.726918 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_memory'
[0m11:32:14.739655 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_memory'
[0m11:32:14.742057 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_memory'
[0m11:32:14.873833 [debug] [ThreadPool]: Using duckdb connection "list_memory"
[0m11:32:14.874497 [debug] [ThreadPool]: Using duckdb connection "list_memory"
[0m11:32:14.875252 [debug] [ThreadPool]: On list_memory: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_memory"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"memory"'
    
  
  
[0m11:32:14.875840 [debug] [ThreadPool]: Using duckdb connection "list_memory"
[0m11:32:14.876543 [debug] [ThreadPool]: On list_memory: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_memory"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"memory"'
    
  
  
[0m11:32:14.877255 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:32:14.878068 [debug] [ThreadPool]: On list_memory: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_memory"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"memory"'
    
  
  
[0m11:32:14.878654 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:32:14.879592 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:32:14.914902 [debug] [ThreadPool]: SQL status: OK in 0.035 seconds
[0m11:32:14.915605 [debug] [ThreadPool]: SQL status: OK in 0.037 seconds
[0m11:32:14.916218 [debug] [ThreadPool]: SQL status: OK in 0.039 seconds
[0m11:32:14.917998 [debug] [ThreadPool]: On list_memory: Close
[0m11:32:14.919506 [debug] [ThreadPool]: On list_memory: Close
[0m11:32:14.921051 [debug] [ThreadPool]: On list_memory: Close
[0m11:32:14.922626 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_memory, now create_memory_main_silver)
[0m11:32:14.923206 [debug] [ThreadPool]: Creating schema "database: "memory"
schema: "main_silver"
"
[0m11:32:14.928922 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_memory, now create_memory_main_gold)
[0m11:32:14.929476 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_memory, now create_memory_main_bronze)
[0m11:32:14.930233 [debug] [ThreadPool]: Creating schema "database: "memory"
schema: "main_gold"
"
[0m11:32:14.932555 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_silver"
[0m11:32:14.933348 [debug] [ThreadPool]: Creating schema "database: "memory"
schema: "main_bronze"
"
[0m11:32:14.935738 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_gold"
[0m11:32:14.936251 [debug] [ThreadPool]: On create_memory_main_silver: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_memory_main_silver"} */

    
        select type from duckdb_databases()
        where lower(database_name)='memory'
        and type='sqlite'
    
  
[0m11:32:14.938154 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_bronze"
[0m11:32:14.938729 [debug] [ThreadPool]: On create_memory_main_gold: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_memory_main_gold"} */

    
        select type from duckdb_databases()
        where lower(database_name)='memory'
        and type='sqlite'
    
  
[0m11:32:14.939158 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:32:14.939589 [debug] [ThreadPool]: On create_memory_main_bronze: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_memory_main_bronze"} */

    
        select type from duckdb_databases()
        where lower(database_name)='memory'
        and type='sqlite'
    
  
[0m11:32:14.940009 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:32:14.940887 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:32:14.941590 [debug] [ThreadPool]: SQL status: OK in 0.002 seconds
[0m11:32:14.942208 [debug] [ThreadPool]: SQL status: OK in 0.002 seconds
[0m11:32:14.943689 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_silver"
[0m11:32:14.945234 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_gold"
[0m11:32:14.945644 [debug] [ThreadPool]: On create_memory_main_silver: BEGIN
[0m11:32:14.946177 [debug] [ThreadPool]: SQL status: OK in 0.005 seconds
[0m11:32:14.946710 [debug] [ThreadPool]: On create_memory_main_gold: BEGIN
[0m11:32:14.948563 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_bronze"
[0m11:32:14.949274 [debug] [ThreadPool]: On create_memory_main_bronze: BEGIN
[0m11:32:14.949666 [debug] [ThreadPool]: SQL status: OK in 0.003 seconds
[0m11:32:14.950031 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_silver"
[0m11:32:14.950563 [debug] [ThreadPool]: On create_memory_main_silver: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_memory_main_silver"} */

    
    
        create schema if not exists "memory"."main_silver"
    
[0m11:32:14.951151 [debug] [ThreadPool]: SQL status: OK in 0.002 seconds
[0m11:32:14.951510 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_bronze"
[0m11:32:14.951865 [debug] [ThreadPool]: On create_memory_main_bronze: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_memory_main_bronze"} */

    
    
        create schema if not exists "memory"."main_bronze"
    
[0m11:32:14.952247 [debug] [ThreadPool]: SQL status: OK in 0.003 seconds
[0m11:32:14.952604 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_gold"
[0m11:32:14.952958 [debug] [ThreadPool]: On create_memory_main_gold: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_memory_main_gold"} */

    
    
        create schema if not exists "memory"."main_gold"
    
[0m11:32:14.953589 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m11:32:14.954530 [debug] [ThreadPool]: On create_memory_main_gold: COMMIT
[0m11:32:14.954904 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_gold"
[0m11:32:14.955242 [debug] [ThreadPool]: On create_memory_main_gold: COMMIT
[0m11:32:14.955666 [debug] [ThreadPool]: SQL status: OK in 0.003 seconds
[0m11:32:14.956614 [debug] [ThreadPool]: On create_memory_main_bronze: COMMIT
[0m11:32:14.956979 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_bronze"
[0m11:32:14.957318 [debug] [ThreadPool]: On create_memory_main_bronze: COMMIT
[0m11:32:14.957685 [debug] [ThreadPool]: SQL status: OK in 0.007 seconds
[0m11:32:14.958644 [debug] [ThreadPool]: On create_memory_main_silver: COMMIT
[0m11:32:14.959004 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_silver"
[0m11:32:14.959338 [debug] [ThreadPool]: On create_memory_main_silver: COMMIT
[0m11:32:14.959998 [debug] [ThreadPool]: SQL status: OK in 0.004 seconds
[0m11:32:14.960553 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m11:32:14.960931 [debug] [ThreadPool]: On create_memory_main_silver: Close
[0m11:32:14.960368 [debug] [ThreadPool]: On create_memory_main_gold: Close
[0m11:32:14.961892 [debug] [ThreadPool]: SQL status: OK in 0.004 seconds
[0m11:32:14.962254 [debug] [ThreadPool]: On create_memory_main_bronze: Close
[0m11:32:14.965510 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_memory_main_silver'
[0m11:32:14.972097 [debug] [ThreadPool]: Using duckdb connection "list_memory_main_silver"
[0m11:32:14.972582 [debug] [ThreadPool]: On list_memory_main_silver: BEGIN
[0m11:32:14.972981 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:32:14.973779 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_memory_main_bronze'
[0m11:32:14.976451 [debug] [ThreadPool]: Using duckdb connection "list_memory_main_bronze"
[0m11:32:14.976898 [debug] [ThreadPool]: On list_memory_main_bronze: BEGIN
[0m11:32:14.977496 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:32:14.978358 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_memory_main_gold'
[0m11:32:14.980769 [debug] [ThreadPool]: Using duckdb connection "list_memory_main_gold"
[0m11:32:14.981292 [debug] [ThreadPool]: On list_memory_main_gold: BEGIN
[0m11:32:14.981729 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:32:14.982503 [debug] [ThreadPool]: SQL status: OK in 0.009 seconds
[0m11:32:14.982939 [debug] [ThreadPool]: Using duckdb connection "list_memory_main_silver"
[0m11:32:14.983389 [debug] [ThreadPool]: On list_memory_main_silver: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_memory_main_silver"} */
select
      'memory' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main_silver'
    and lower(table_catalog) = 'memory'
  
[0m11:32:14.984133 [debug] [ThreadPool]: SQL status: OK in 0.002 seconds
[0m11:32:14.984589 [debug] [ThreadPool]: Using duckdb connection "list_memory_main_gold"
[0m11:32:14.985018 [debug] [ThreadPool]: On list_memory_main_gold: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_memory_main_gold"} */
select
      'memory' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main_gold'
    and lower(table_catalog) = 'memory'
  
[0m11:32:14.985664 [debug] [ThreadPool]: SQL status: OK in 0.008 seconds
[0m11:32:14.986088 [debug] [ThreadPool]: Using duckdb connection "list_memory_main_bronze"
[0m11:32:14.986515 [debug] [ThreadPool]: On list_memory_main_bronze: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_memory_main_bronze"} */
select
      'memory' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main_bronze'
    and lower(table_catalog) = 'memory'
  
[0m11:32:15.008052 [debug] [ThreadPool]: SQL status: OK in 0.021 seconds
[0m11:32:15.008835 [debug] [ThreadPool]: SQL status: OK in 0.023 seconds
[0m11:32:15.009222 [debug] [ThreadPool]: SQL status: OK in 0.025 seconds
[0m11:32:15.010551 [debug] [ThreadPool]: On list_memory_main_silver: ROLLBACK
[0m11:32:15.011893 [debug] [ThreadPool]: On list_memory_main_gold: ROLLBACK
[0m11:32:15.013408 [debug] [ThreadPool]: On list_memory_main_bronze: ROLLBACK
[0m11:32:15.015698 [debug] [ThreadPool]: Failed to rollback 'list_memory_main_gold'
[0m11:32:15.016076 [debug] [ThreadPool]: On list_memory_main_gold: Close
[0m11:32:15.017414 [debug] [ThreadPool]: Failed to rollback 'list_memory_main_silver'
[0m11:32:15.017746 [debug] [ThreadPool]: On list_memory_main_silver: Close
[0m11:32:15.018867 [debug] [ThreadPool]: Failed to rollback 'list_memory_main_bronze'
[0m11:32:15.019239 [debug] [ThreadPool]: On list_memory_main_bronze: Close
[0m11:32:15.020210 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0ef3892f-5dd3-4f39-80e0-5f145d846a83', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ED3EF13380>]}
[0m11:32:15.020680 [debug] [MainThread]: Using duckdb connection "master"
[0m11:32:15.020999 [debug] [MainThread]: On master: BEGIN
[0m11:32:15.021338 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:32:15.021960 [debug] [MainThread]: SQL status: OK in 0.001 seconds
[0m11:32:15.022287 [debug] [MainThread]: On master: COMMIT
[0m11:32:15.022608 [debug] [MainThread]: Using duckdb connection "master"
[0m11:32:15.022933 [debug] [MainThread]: On master: COMMIT
[0m11:32:15.023446 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m11:32:15.023782 [debug] [MainThread]: On master: Close
[0m11:32:15.029869 [debug] [Thread-1 (]: Began running node model.job_intelligent.stg_jobs_raw
[0m11:32:15.030604 [info ] [Thread-1 (]: 1 of 13 START sql view model main_bronze.stg_jobs_raw .......................... [RUN]
[0m11:32:15.031282 [debug] [Thread-1 (]: Acquiring new duckdb connection 'model.job_intelligent.stg_jobs_raw'
[0m11:32:15.031711 [debug] [Thread-1 (]: Began compiling node model.job_intelligent.stg_jobs_raw
[0m11:32:15.040047 [debug] [Thread-1 (]: Writing injected SQL for node "model.job_intelligent.stg_jobs_raw"
[0m11:32:15.040945 [debug] [Thread-1 (]: Began executing node model.job_intelligent.stg_jobs_raw
[0m11:32:15.073550 [debug] [Thread-1 (]: Writing runtime sql for node "model.job_intelligent.stg_jobs_raw"
[0m11:32:15.074549 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.stg_jobs_raw"
[0m11:32:15.074947 [debug] [Thread-1 (]: On model.job_intelligent.stg_jobs_raw: BEGIN
[0m11:32:15.075385 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m11:32:15.076278 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m11:32:15.076668 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.stg_jobs_raw"
[0m11:32:15.077089 [debug] [Thread-1 (]: On model.job_intelligent.stg_jobs_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.stg_jobs_raw"} */

  
  create view "memory"."main_bronze"."stg_jobs_raw__dbt_tmp" as (
    -- models/bronze/stg_jobs_raw.sql
-- Bronze layer: Lecture directe des données brutes depuis final_data.csv
-- Renommage et typage minimal



-- Read directly from CSV file
SELECT
    -- Renommer les colonnes pour cohérence
    title as job_title,
    location,
    postedTime as posted_time,
    publishedAt as published_at,
    jobUrl as job_url,
    companyName as company_name,
    companyUrl as company_url,
    description as job_description,
    contractType as contract_type,
    workType as work_type,
    
    -- Métadonnées de traçabilité
    NOW() as ingestion_timestamp,
    '2026-01-08 10:32:13.573643+00:00' as dbt_run_id

FROM read_csv_auto('D:/lab2/final_data.csv')

WHERE 1=1
    -- Filtrer les lignes vides
    AND title IS NOT NULL
    AND description IS NOT NULL
  );

[0m11:32:15.124592 [debug] [Thread-1 (]: SQL status: OK in 0.047 seconds
[0m11:32:15.130538 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.stg_jobs_raw"
[0m11:32:15.130951 [debug] [Thread-1 (]: On model.job_intelligent.stg_jobs_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.stg_jobs_raw"} */
alter view "memory"."main_bronze"."stg_jobs_raw__dbt_tmp" rename to "stg_jobs_raw"
[0m11:32:15.131644 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m11:32:15.140385 [debug] [Thread-1 (]: On model.job_intelligent.stg_jobs_raw: COMMIT
[0m11:32:15.140817 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.stg_jobs_raw"
[0m11:32:15.141171 [debug] [Thread-1 (]: On model.job_intelligent.stg_jobs_raw: COMMIT
[0m11:32:15.141836 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m11:32:15.147564 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.stg_jobs_raw"
[0m11:32:15.148071 [debug] [Thread-1 (]: On model.job_intelligent.stg_jobs_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.stg_jobs_raw"} */

      drop view if exists "memory"."main_bronze"."stg_jobs_raw__dbt_backup" cascade
    
[0m11:32:15.148903 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m11:32:15.151464 [debug] [Thread-1 (]: On model.job_intelligent.stg_jobs_raw: Close
[0m11:32:15.153961 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0ef3892f-5dd3-4f39-80e0-5f145d846a83', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ED3D7CEED0>]}
[0m11:32:15.154594 [info ] [Thread-1 (]: 1 of 13 OK created sql view model main_bronze.stg_jobs_raw ..................... [[32mOK[0m in 0.12s]
[0m11:32:15.155331 [debug] [Thread-1 (]: Finished running node model.job_intelligent.stg_jobs_raw
[0m11:32:15.156344 [debug] [Thread-2 (]: Began running node model.job_intelligent.int_jobs_cleaned
[0m11:32:15.156879 [info ] [Thread-2 (]: 2 of 13 START sql table model main_silver.int_jobs_cleaned ..................... [RUN]
[0m11:32:15.157464 [debug] [Thread-2 (]: Acquiring new duckdb connection 'model.job_intelligent.int_jobs_cleaned'
[0m11:32:15.157849 [debug] [Thread-2 (]: Began compiling node model.job_intelligent.int_jobs_cleaned
[0m11:32:15.160960 [debug] [Thread-2 (]: Writing injected SQL for node "model.job_intelligent.int_jobs_cleaned"
[0m11:32:15.161738 [debug] [Thread-2 (]: Began executing node model.job_intelligent.int_jobs_cleaned
[0m11:32:15.179932 [debug] [Thread-2 (]: Writing runtime sql for node "model.job_intelligent.int_jobs_cleaned"
[0m11:32:15.181221 [debug] [Thread-2 (]: Using duckdb connection "model.job_intelligent.int_jobs_cleaned"
[0m11:32:15.181919 [debug] [Thread-2 (]: On model.job_intelligent.int_jobs_cleaned: BEGIN
[0m11:32:15.182319 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m11:32:15.183000 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m11:32:15.183351 [debug] [Thread-2 (]: Using duckdb connection "model.job_intelligent.int_jobs_cleaned"
[0m11:32:15.183765 [debug] [Thread-2 (]: On model.job_intelligent.int_jobs_cleaned: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.int_jobs_cleaned"} */

  
    
    

    create  table
      "memory"."main_silver"."int_jobs_cleaned__dbt_tmp"
  
    as (
      -- models/silver/int_jobs_cleaned.sql
-- Silver layer: Nettoyage et normalisation des données brutes



WITH raw_jobs AS (
    SELECT * FROM "memory"."main_bronze"."stg_jobs_raw"
),

cleaned_jobs AS (
    SELECT
        -- Texte: lowercase, trim, remove special characters
        LOWER(TRIM(job_title)) as job_title_cleaned,
        LOWER(TRIM(location)) as location_cleaned,
        LOWER(TRIM(company_name)) as company_name_cleaned,
        LOWER(TRIM(job_description)) as job_description_cleaned,
        LOWER(TRIM(contract_type)) as contract_type_cleaned,
        LOWER(TRIM(work_type)) as work_type_cleaned,
        
        -- URLs as-is
        job_url,
        company_url,
        
        -- Dates
        TRY_CAST(published_at AS DATE) as published_date,
        posted_time,
        
        -- Extract year-month for time-based analysis
        DATE_TRUNC('month', TRY_CAST(published_at AS DATE)) as published_year_month,
        EXTRACT(YEAR FROM TRY_CAST(published_at AS DATE)) as published_year,
        EXTRACT(MONTH FROM TRY_CAST(published_at AS DATE)) as published_month,
        
        -- Métadonnées
        ingestion_timestamp
    FROM raw_jobs
)

SELECT
    *,
    -- Deduplication flag
    ROW_NUMBER() OVER (
        PARTITION BY job_title_cleaned, company_name_cleaned, location_cleaned 
        ORDER BY published_date DESC
    ) as dedup_rank
FROM cleaned_jobs
    );
  
  
[0m11:32:15.318673 [debug] [Thread-2 (]: SQL status: OK in 0.134 seconds
[0m11:32:15.321202 [debug] [Thread-2 (]: Using duckdb connection "model.job_intelligent.int_jobs_cleaned"
[0m11:32:15.321538 [debug] [Thread-2 (]: On model.job_intelligent.int_jobs_cleaned: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.int_jobs_cleaned"} */
alter table "memory"."main_silver"."int_jobs_cleaned__dbt_tmp" rename to "int_jobs_cleaned"
[0m11:32:15.322192 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m11:32:15.327081 [debug] [Thread-2 (]: On model.job_intelligent.int_jobs_cleaned: COMMIT
[0m11:32:15.327477 [debug] [Thread-2 (]: Using duckdb connection "model.job_intelligent.int_jobs_cleaned"
[0m11:32:15.327794 [debug] [Thread-2 (]: On model.job_intelligent.int_jobs_cleaned: COMMIT
[0m11:32:15.331732 [debug] [Thread-2 (]: SQL status: OK in 0.004 seconds
[0m11:32:15.333634 [debug] [Thread-2 (]: Using duckdb connection "model.job_intelligent.int_jobs_cleaned"
[0m11:32:15.333964 [debug] [Thread-2 (]: On model.job_intelligent.int_jobs_cleaned: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.int_jobs_cleaned"} */

      drop table if exists "memory"."main_silver"."int_jobs_cleaned__dbt_backup" cascade
    
[0m11:32:15.334581 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m11:32:15.335729 [debug] [Thread-2 (]: On model.job_intelligent.int_jobs_cleaned: Close
[0m11:32:15.336195 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0ef3892f-5dd3-4f39-80e0-5f145d846a83', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ED3ED3AFC0>]}
[0m11:32:15.336718 [info ] [Thread-2 (]: 2 of 13 OK created sql table model main_silver.int_jobs_cleaned ................ [[32mOK[0m in 0.18s]
[0m11:32:15.337277 [debug] [Thread-2 (]: Finished running node model.job_intelligent.int_jobs_cleaned
[0m11:32:15.338151 [debug] [Thread-3 (]: Began running node model.job_intelligent.int_job_title_normalization
[0m11:32:15.338579 [info ] [Thread-3 (]: 3 of 13 START sql table model main_silver.int_job_title_normalization .......... [RUN]
[0m11:32:15.339058 [debug] [Thread-3 (]: Acquiring new duckdb connection 'model.job_intelligent.int_job_title_normalization'
[0m11:32:15.339391 [debug] [Thread-3 (]: Began compiling node model.job_intelligent.int_job_title_normalization
[0m11:32:15.342576 [debug] [Thread-3 (]: Writing injected SQL for node "model.job_intelligent.int_job_title_normalization"
[0m11:32:15.343346 [debug] [Thread-3 (]: Began executing node model.job_intelligent.int_job_title_normalization
[0m11:32:15.346065 [debug] [Thread-3 (]: Writing runtime sql for node "model.job_intelligent.int_job_title_normalization"
[0m11:32:15.347462 [debug] [Thread-3 (]: Using duckdb connection "model.job_intelligent.int_job_title_normalization"
[0m11:32:15.347986 [debug] [Thread-3 (]: On model.job_intelligent.int_job_title_normalization: BEGIN
[0m11:32:15.348318 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m11:32:15.348961 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m11:32:15.349302 [debug] [Thread-3 (]: Using duckdb connection "model.job_intelligent.int_job_title_normalization"
[0m11:32:15.349723 [debug] [Thread-3 (]: On model.job_intelligent.int_job_title_normalization: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.int_job_title_normalization"} */

  
    
    

    create  table
      "memory"."main_silver"."int_job_title_normalization__dbt_tmp"
  
    as (
      -- models/silver/int_job_title_normalization.sql
-- Silver layer: Normaliser les intitulés de postes



WITH cleaned_jobs AS (
    SELECT * FROM "memory"."main_silver"."int_jobs_cleaned"
),

title_normalized AS (
    SELECT
        *,
        CASE
            WHEN job_title_cleaned LIKE '%data engineer%' THEN 'Data Engineer'
            WHEN job_title_cleaned LIKE '%data scientist%' THEN 'Data Scientist'
            WHEN job_title_cleaned LIKE '%data analyst%' THEN 'Data Analyst'
            WHEN job_title_cleaned LIKE '%analytics engineer%' THEN 'Analytics Engineer'
            WHEN job_title_cleaned LIKE '%ml engineer%' OR job_title_cleaned LIKE '%machine learning%' THEN 'ML Engineer'
            WHEN job_title_cleaned LIKE '%data architect%' THEN 'Data Architect'
            WHEN job_title_cleaned LIKE '%bi developer%' OR job_title_cleaned LIKE '%business intelligence%' THEN 'BI Developer'
            WHEN job_title_cleaned LIKE '%etl%' OR job_title_cleaned LIKE '%pipeline%' THEN 'ETL/Pipeline Engineer'
            WHEN job_title_cleaned LIKE '%data engineer%' THEN 'Data Engineer'
            ELSE 'Other Data Role'
        END as job_category,
        
        CASE
            WHEN contract_type_cleaned LIKE '%cdi%' OR contract_type_cleaned LIKE '%permanent%' THEN 'Permanent'
            WHEN contract_type_cleaned LIKE '%cdd%' OR contract_type_cleaned LIKE '%contract%' THEN 'Contract'
            WHEN contract_type_cleaned LIKE '%stage%' OR contract_type_cleaned LIKE '%internship%' THEN 'Internship'
            WHEN contract_type_cleaned LIKE '%freelance%' THEN 'Freelance'
            ELSE 'Not Specified'
        END as contract_type_normalized,
        
        CASE
            WHEN work_type_cleaned LIKE '%remote%' THEN 'Remote'
            WHEN work_type_cleaned LIKE '%hybrid%' THEN 'Hybrid'
            WHEN work_type_cleaned LIKE '%onsite%' OR work_type_cleaned LIKE '%on-site%' THEN 'On-site'
            ELSE 'Not Specified'
        END as work_type_normalized
    
    FROM cleaned_jobs
    WHERE dedup_rank = 1  -- Keep only first occurrence (most recent)
)

SELECT * FROM title_normalized
    );
  
  
[0m11:32:15.377321 [debug] [Thread-3 (]: SQL status: OK in 0.027 seconds
[0m11:32:15.380619 [debug] [Thread-3 (]: Using duckdb connection "model.job_intelligent.int_job_title_normalization"
[0m11:32:15.381136 [debug] [Thread-3 (]: On model.job_intelligent.int_job_title_normalization: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.int_job_title_normalization"} */
alter table "memory"."main_silver"."int_job_title_normalization__dbt_tmp" rename to "int_job_title_normalization"
[0m11:32:15.382357 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m11:32:15.383992 [debug] [Thread-3 (]: On model.job_intelligent.int_job_title_normalization: COMMIT
[0m11:32:15.384414 [debug] [Thread-3 (]: Using duckdb connection "model.job_intelligent.int_job_title_normalization"
[0m11:32:15.384809 [debug] [Thread-3 (]: On model.job_intelligent.int_job_title_normalization: COMMIT
[0m11:32:15.385486 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m11:32:15.387633 [debug] [Thread-3 (]: Using duckdb connection "model.job_intelligent.int_job_title_normalization"
[0m11:32:15.388049 [debug] [Thread-3 (]: On model.job_intelligent.int_job_title_normalization: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.int_job_title_normalization"} */

      drop table if exists "memory"."main_silver"."int_job_title_normalization__dbt_backup" cascade
    
[0m11:32:15.388716 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m11:32:15.390077 [debug] [Thread-3 (]: On model.job_intelligent.int_job_title_normalization: Close
[0m11:32:15.390624 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0ef3892f-5dd3-4f39-80e0-5f145d846a83', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ED40158F50>]}
[0m11:32:15.391324 [info ] [Thread-3 (]: 3 of 13 OK created sql table model main_silver.int_job_title_normalization ..... [[32mOK[0m in 0.05s]
[0m11:32:15.392026 [debug] [Thread-3 (]: Finished running node model.job_intelligent.int_job_title_normalization
[0m11:32:15.393044 [debug] [Thread-1 (]: Began running node model.job_intelligent.dim_location
[0m11:32:15.393574 [info ] [Thread-1 (]: 5 of 13 START sql table model main_gold.dim_location ........................... [RUN]
[0m11:32:15.394140 [debug] [Thread-3 (]: Began running node model.job_intelligent.int_skills_extraction
[0m11:32:15.394658 [debug] [Thread-4 (]: Began running node model.job_intelligent.dim_company
[0m11:32:15.395250 [debug] [Thread-2 (]: Began running node model.job_intelligent.dim_time
[0m11:32:15.395890 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.job_intelligent.stg_jobs_raw, now model.job_intelligent.dim_location)
[0m11:32:15.396647 [info ] [Thread-3 (]: 7 of 13 START sql table model main_silver.int_skills_extraction ................ [RUN]
[0m11:32:15.397513 [info ] [Thread-4 (]: 4 of 13 START sql table model main_gold.dim_company ............................ [RUN]
[0m11:32:15.398271 [info ] [Thread-2 (]: 6 of 13 START sql table model main_gold.dim_time ............................... [RUN]
[0m11:32:15.398903 [debug] [Thread-1 (]: Began compiling node model.job_intelligent.dim_location
[0m11:32:15.399495 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly model.job_intelligent.int_job_title_normalization, now model.job_intelligent.int_skills_extraction)
[0m11:32:15.400233 [debug] [Thread-4 (]: Acquiring new duckdb connection 'model.job_intelligent.dim_company'
[0m11:32:15.400792 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly model.job_intelligent.int_jobs_cleaned, now model.job_intelligent.dim_time)
[0m11:32:15.404880 [debug] [Thread-1 (]: Writing injected SQL for node "model.job_intelligent.dim_location"
[0m11:32:15.405518 [debug] [Thread-3 (]: Began compiling node model.job_intelligent.int_skills_extraction
[0m11:32:15.406098 [debug] [Thread-4 (]: Began compiling node model.job_intelligent.dim_company
[0m11:32:15.406663 [debug] [Thread-2 (]: Began compiling node model.job_intelligent.dim_time
[0m11:32:15.411896 [debug] [Thread-3 (]: Writing injected SQL for node "model.job_intelligent.int_skills_extraction"
[0m11:32:15.416540 [debug] [Thread-4 (]: Writing injected SQL for node "model.job_intelligent.dim_company"
[0m11:32:15.420838 [debug] [Thread-2 (]: Writing injected SQL for node "model.job_intelligent.dim_time"
[0m11:32:15.421680 [debug] [Thread-1 (]: Began executing node model.job_intelligent.dim_location
[0m11:32:15.427188 [debug] [Thread-1 (]: Writing runtime sql for node "model.job_intelligent.dim_location"
[0m11:32:15.428178 [debug] [Thread-4 (]: Began executing node model.job_intelligent.dim_company
[0m11:32:15.429018 [debug] [Thread-2 (]: Began executing node model.job_intelligent.dim_time
[0m11:32:15.429746 [debug] [Thread-3 (]: Began executing node model.job_intelligent.int_skills_extraction
[0m11:32:15.433654 [debug] [Thread-4 (]: Writing runtime sql for node "model.job_intelligent.dim_company"
[0m11:32:15.441737 [debug] [Thread-2 (]: Writing runtime sql for node "model.job_intelligent.dim_time"
[0m11:32:15.443156 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.dim_location"
[0m11:32:15.447546 [debug] [Thread-3 (]: Writing runtime sql for node "model.job_intelligent.int_skills_extraction"
[0m11:32:15.448710 [debug] [Thread-1 (]: On model.job_intelligent.dim_location: BEGIN
[0m11:32:15.449817 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:32:15.450714 [debug] [Thread-4 (]: Using duckdb connection "model.job_intelligent.dim_company"
[0m11:32:15.452001 [debug] [Thread-3 (]: Using duckdb connection "model.job_intelligent.int_skills_extraction"
[0m11:32:15.452692 [debug] [Thread-4 (]: On model.job_intelligent.dim_company: BEGIN
[0m11:32:15.453405 [debug] [Thread-2 (]: Using duckdb connection "model.job_intelligent.dim_time"
[0m11:32:15.454095 [debug] [Thread-1 (]: SQL status: OK in 0.004 seconds
[0m11:32:15.454721 [debug] [Thread-3 (]: On model.job_intelligent.int_skills_extraction: BEGIN
[0m11:32:15.455377 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m11:32:15.456078 [debug] [Thread-2 (]: On model.job_intelligent.dim_time: BEGIN
[0m11:32:15.456719 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.dim_location"
[0m11:32:15.457360 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m11:32:15.458346 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m11:32:15.459107 [debug] [Thread-4 (]: SQL status: OK in 0.004 seconds
[0m11:32:15.459837 [debug] [Thread-1 (]: On model.job_intelligent.dim_location: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_location"} */

  
    
    

    create  table
      "memory"."main_gold"."dim_location__dbt_tmp"
  
    as (
      -- models/gold/dim_location.sql
-- Gold layer: Dimension Location



WITH jobs AS (
    SELECT DISTINCT
        location_cleaned as location_raw
    FROM "memory"."main_silver"."int_job_title_normalization"
    WHERE location_cleaned IS NOT NULL
),

location_parsed AS (
    SELECT
        location_raw,
        -- Extract city (before comma)
        CASE 
            WHEN POSITION(',' IN location_raw) > 0 
            THEN TRIM(SUBSTRING(location_raw, 1, POSITION(',' IN location_raw) - 1))
            ELSE location_raw
        END as city,
        
        -- Extract country (after comma)
        CASE 
            WHEN POSITION(',' IN location_raw) > 0 
            THEN TRIM(SUBSTRING(location_raw, POSITION(',' IN location_raw) + 1))
            ELSE 'Not Specified'
        END as country,
        
        -- Detect if remote
        CASE 
            WHEN location_raw LIKE '%remote%' 
            THEN 'Remote'
            ELSE 'On-site'
        END as work_location_type
    FROM jobs
),

ranked_locations AS (
    SELECT
        ROW_NUMBER() OVER (ORDER BY location_raw) as location_id,
        location_raw,
        city,
        country,
        work_location_type,
        NOW() as created_at
    FROM location_parsed
)

SELECT
    location_id,
    location_raw,
    city,
    country,
    work_location_type,
    created_at
FROM ranked_locations
ORDER BY location_id
    );
  
  
[0m11:32:15.461215 [debug] [Thread-3 (]: SQL status: OK in 0.004 seconds
[0m11:32:15.461838 [debug] [Thread-4 (]: Using duckdb connection "model.job_intelligent.dim_company"
[0m11:32:15.463183 [debug] [Thread-2 (]: SQL status: OK in 0.005 seconds
[0m11:32:15.463998 [debug] [Thread-3 (]: Using duckdb connection "model.job_intelligent.int_skills_extraction"
[0m11:32:15.464759 [debug] [Thread-4 (]: On model.job_intelligent.dim_company: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_company"} */

  
    
    

    create  table
      "memory"."main_gold"."dim_company__dbt_tmp"
  
    as (
      -- models/gold/dim_company.sql
-- Gold layer: Dimension Company



WITH jobs AS (
    SELECT DISTINCT
        company_name_cleaned,
        company_url
    FROM "memory"."main_silver"."int_job_title_normalization"
    WHERE company_name_cleaned IS NOT NULL
),

ranked_companies AS (
    SELECT
        ROW_NUMBER() OVER (ORDER BY company_name_cleaned) as company_id,
        company_name_cleaned as company_name,
        company_url,
        NOW() as created_at
    FROM jobs
)

SELECT
    company_id,
    company_name,
    company_url,
    created_at
FROM ranked_companies
ORDER BY company_id
    );
  
  
[0m11:32:15.465540 [debug] [Thread-2 (]: Using duckdb connection "model.job_intelligent.dim_time"
[0m11:32:15.466393 [debug] [Thread-3 (]: On model.job_intelligent.int_skills_extraction: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.int_skills_extraction"} */

  
    
    

    create  table
      "memory"."main_silver"."int_skills_extraction__dbt_tmp"
  
    as (
      -- models/silver/int_skills_extraction.sql
-- Silver layer: Extraction des compétences depuis la description



WITH jobs_with_titles AS (
    SELECT * FROM "memory"."main_silver"."int_job_title_normalization"
),

skills_mapping AS (
    -- Définir un mapping de compétences communes Data/Tech
    SELECT
        'Python' as skill_name,
        'python|py |\.py' as skill_pattern
    UNION ALL SELECT 'SQL', 'sql|sql|sql server|postgres|oracle'
    UNION ALL SELECT 'Spark', 'spark|pyspark'
    UNION ALL SELECT 'Hadoop', 'hadoop|hdfs'
    UNION ALL SELECT 'Scala', 'scala'
    UNION ALL SELECT 'Java', '\bjava\b'
    UNION ALL SELECT 'R', '\br\b|r programming'
    UNION ALL SELECT 'Tableau', 'tableau'
    UNION ALL SELECT 'Power BI', 'power bi|powerbi'
    UNION ALL SELECT 'Looker', 'looker'
    UNION ALL SELECT 'AWS', 'aws|amazon web|s3 |ec2|redshift'
    UNION ALL SELECT 'Azure', 'azure|microsoft azure|synapse|cosmos'
    UNION ALL SELECT 'GCP', 'gcp|google cloud|bigquery'
    UNION ALL SELECT 'Airflow', 'airflow'
    UNION ALL SELECT 'DBT', '\bdbt\b|dbt'
    UNION ALL SELECT 'Kubernetes', 'kubernetes|k8s'
    UNION ALL SELECT 'Docker', 'docker'
    UNION ALL SELECT 'Git', 'git|github|gitlab'
    UNION ALL SELECT 'TensorFlow', 'tensorflow'
    UNION ALL SELECT 'PyTorch', 'pytorch'
    UNION ALL SELECT 'Scikit-learn', 'scikit|sklearn'
    UNION ALL SELECT 'Pandas', 'pandas'
    UNION ALL SELECT 'NumPy', 'numpy'
    UNION ALL SELECT 'Machine Learning', 'machine learning|deep learning|ml|artificial intelligence'
    UNION ALL SELECT 'Statistics', 'statistics|statistical|probability'
    UNION ALL SELECT 'Data Visualization', 'data visualization|visualization|charts|graphs'
),

jobs_exploded AS (
    SELECT
        j.*,
        s.skill_name,
        CASE 
            WHEN job_description_cleaned ILIKE '%' || s.skill_pattern || '%' THEN 1 
            ELSE 0 
        END as has_skill
    FROM jobs_with_titles j
    CROSS JOIN skills_mapping s
)

SELECT
    *
FROM jobs_exploded
WHERE has_skill = 1

ORDER BY job_title_cleaned, published_date DESC
    );
  
  
[0m11:32:15.468452 [debug] [Thread-2 (]: On model.job_intelligent.dim_time: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_time"} */

  
    
    

    create  table
      "memory"."main_gold"."dim_time__dbt_tmp"
  
    as (
      -- models/gold/dim_time.sql
-- Gold layer: Dimension Time



-- Generate a date dimension for time-based analysis
WITH date_spine AS (
    SELECT
        published_date,
        EXTRACT(YEAR FROM published_date) as year,
        EXTRACT(MONTH FROM published_date) as month,
        EXTRACT(QUARTER FROM published_date) as quarter,
        EXTRACT(WEEK FROM published_date) as week,
        EXTRACT(DAYOFWEEK FROM published_date) as day_of_week,
        DATE_TRUNC('month', published_date) as month_start,
        DATE_TRUNC('quarter', published_date) as quarter_start,
        DATE_TRUNC('year', published_date) as year_start,
        
        CASE 
            WHEN EXTRACT(MONTH FROM published_date) IN (1,2,3) THEN 'Q1'
            WHEN EXTRACT(MONTH FROM published_date) IN (4,5,6) THEN 'Q2'
            WHEN EXTRACT(MONTH FROM published_date) IN (7,8,9) THEN 'Q3'
            ELSE 'Q4'
        END as quarter_name,
        
        CASE EXTRACT(DAYOFWEEK FROM published_date)
            WHEN 0 THEN 'Sunday'
            WHEN 1 THEN 'Monday'
            WHEN 2 THEN 'Tuesday'
            WHEN 3 THEN 'Wednesday'
            WHEN 4 THEN 'Thursday'
            WHEN 5 THEN 'Friday'
            WHEN 6 THEN 'Saturday'
        END as day_name,
        
        CASE EXTRACT(MONTH FROM published_date)
            WHEN 1 THEN 'January'
            WHEN 2 THEN 'February'
            WHEN 3 THEN 'March'
            WHEN 4 THEN 'April'
            WHEN 5 THEN 'May'
            WHEN 6 THEN 'June'
            WHEN 7 THEN 'July'
            WHEN 8 THEN 'August'
            WHEN 9 THEN 'September'
            WHEN 10 THEN 'October'
            WHEN 11 THEN 'November'
            WHEN 12 THEN 'December'
        END as month_name
        
    FROM (
        SELECT DISTINCT published_date
        FROM "memory"."main_silver"."int_job_title_normalization"
        WHERE published_date IS NOT NULL
    )
)

SELECT
    published_date as date_id,
    year,
    month,
    quarter,
    week,
    day_of_week,
    day_name,
    month_name,
    quarter_name,
    month_start,
    quarter_start,
    year_start,
    NOW() as created_at
FROM date_spine
ORDER BY published_date
    );
  
  
[0m11:32:15.475091 [debug] [Thread-1 (]: SQL status: OK in 0.012 seconds
[0m11:32:15.480301 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.dim_location"
[0m11:32:15.481263 [debug] [Thread-1 (]: On model.job_intelligent.dim_location: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_location"} */
alter table "memory"."main_gold"."dim_location__dbt_tmp" rename to "dim_location"
[0m11:32:15.482384 [debug] [Thread-4 (]: SQL status: OK in 0.015 seconds
[0m11:32:15.485677 [debug] [Thread-4 (]: Using duckdb connection "model.job_intelligent.dim_company"
[0m11:32:15.486150 [debug] [Thread-4 (]: On model.job_intelligent.dim_company: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_company"} */
alter table "memory"."main_gold"."dim_company__dbt_tmp" rename to "dim_company"
[0m11:32:15.486741 [debug] [Thread-1 (]: SQL status: OK in 0.005 seconds
[0m11:32:15.488651 [debug] [Thread-1 (]: On model.job_intelligent.dim_location: COMMIT
[0m11:32:15.489150 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.dim_location"
[0m11:32:15.489558 [debug] [Thread-1 (]: On model.job_intelligent.dim_location: COMMIT
[0m11:32:15.490272 [debug] [Thread-2 (]: SQL status: OK in 0.019 seconds
[0m11:32:15.493199 [debug] [Thread-2 (]: Using duckdb connection "model.job_intelligent.dim_time"
[0m11:32:15.493621 [debug] [Thread-2 (]: On model.job_intelligent.dim_time: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_time"} */
alter table "memory"."main_gold"."dim_time__dbt_tmp" rename to "dim_time"
[0m11:32:15.494262 [debug] [Thread-1 (]: SQL status: OK in 0.004 seconds
[0m11:32:15.497375 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.dim_location"
[0m11:32:15.498006 [debug] [Thread-4 (]: SQL status: OK in 0.011 seconds
[0m11:32:15.498665 [debug] [Thread-1 (]: On model.job_intelligent.dim_location: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_location"} */

      drop table if exists "memory"."main_gold"."dim_location__dbt_backup" cascade
    
[0m11:32:15.499183 [debug] [Thread-2 (]: SQL status: OK in 0.005 seconds
[0m11:32:15.500957 [debug] [Thread-4 (]: On model.job_intelligent.dim_company: COMMIT
[0m11:32:15.502877 [debug] [Thread-2 (]: On model.job_intelligent.dim_time: COMMIT
[0m11:32:15.503430 [debug] [Thread-4 (]: Using duckdb connection "model.job_intelligent.dim_company"
[0m11:32:15.503984 [debug] [Thread-2 (]: Using duckdb connection "model.job_intelligent.dim_time"
[0m11:32:15.504422 [debug] [Thread-1 (]: SQL status: OK in 0.003 seconds
[0m11:32:15.504905 [debug] [Thread-4 (]: On model.job_intelligent.dim_company: COMMIT
[0m11:32:15.505429 [debug] [Thread-2 (]: On model.job_intelligent.dim_time: COMMIT
[0m11:32:15.506910 [debug] [Thread-1 (]: On model.job_intelligent.dim_location: Close
[0m11:32:15.508013 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0ef3892f-5dd3-4f39-80e0-5f145d846a83', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ED3F004550>]}
[0m11:32:15.508713 [info ] [Thread-1 (]: 5 of 13 OK created sql table model main_gold.dim_location ...................... [[32mOK[0m in 0.11s]
[0m11:32:15.509436 [debug] [Thread-1 (]: Finished running node model.job_intelligent.dim_location
[0m11:32:15.510073 [debug] [Thread-4 (]: SQL status: OK in 0.003 seconds
[0m11:32:15.512381 [debug] [Thread-4 (]: Using duckdb connection "model.job_intelligent.dim_company"
[0m11:32:15.513094 [debug] [Thread-4 (]: On model.job_intelligent.dim_company: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_company"} */

      drop table if exists "memory"."main_gold"."dim_company__dbt_backup" cascade
    
[0m11:32:15.513768 [debug] [Thread-2 (]: SQL status: OK in 0.006 seconds
[0m11:32:15.516944 [debug] [Thread-2 (]: Using duckdb connection "model.job_intelligent.dim_time"
[0m11:32:15.517365 [debug] [Thread-2 (]: On model.job_intelligent.dim_time: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_time"} */

      drop table if exists "memory"."main_gold"."dim_time__dbt_backup" cascade
    
[0m11:32:15.518023 [debug] [Thread-4 (]: SQL status: OK in 0.004 seconds
[0m11:32:15.519424 [debug] [Thread-4 (]: On model.job_intelligent.dim_company: Close
[0m11:32:15.519971 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0ef3892f-5dd3-4f39-80e0-5f145d846a83', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ED3F138F30>]}
[0m11:32:15.520617 [info ] [Thread-4 (]: 4 of 13 OK created sql table model main_gold.dim_company ....................... [[32mOK[0m in 0.12s]
[0m11:32:15.521304 [debug] [Thread-4 (]: Finished running node model.job_intelligent.dim_company
[0m11:32:15.521819 [debug] [Thread-2 (]: SQL status: OK in 0.004 seconds
[0m11:32:15.523119 [debug] [Thread-2 (]: On model.job_intelligent.dim_time: Close
[0m11:32:15.523629 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0ef3892f-5dd3-4f39-80e0-5f145d846a83', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ED3D7ED070>]}
[0m11:32:15.524220 [info ] [Thread-2 (]: 6 of 13 OK created sql table model main_gold.dim_time .......................... [[32mOK[0m in 0.12s]
[0m11:32:15.524894 [debug] [Thread-2 (]: Finished running node model.job_intelligent.dim_time
[0m11:32:15.525614 [debug] [Thread-1 (]: Began running node model.job_intelligent.fact_job_offers
[0m11:32:15.526119 [info ] [Thread-1 (]: 8 of 13 START sql table model main_gold.fact_job_offers ........................ [RUN]
[0m11:32:15.526607 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.job_intelligent.dim_location, now model.job_intelligent.fact_job_offers)
[0m11:32:15.526997 [debug] [Thread-1 (]: Began compiling node model.job_intelligent.fact_job_offers
[0m11:32:15.531857 [debug] [Thread-1 (]: Writing injected SQL for node "model.job_intelligent.fact_job_offers"
[0m11:32:15.532814 [debug] [Thread-1 (]: Began executing node model.job_intelligent.fact_job_offers
[0m11:32:15.535815 [debug] [Thread-1 (]: Writing runtime sql for node "model.job_intelligent.fact_job_offers"
[0m11:32:15.536654 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.fact_job_offers"
[0m11:32:15.537071 [debug] [Thread-1 (]: On model.job_intelligent.fact_job_offers: BEGIN
[0m11:32:15.537449 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:32:15.538207 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m11:32:15.538599 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.fact_job_offers"
[0m11:32:15.539103 [debug] [Thread-1 (]: On model.job_intelligent.fact_job_offers: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.fact_job_offers"} */

  
    
    

    create  table
      "memory"."main_gold"."fact_job_offers__dbt_tmp"
  
    as (
      -- models/gold/fact_job_offers.sql
-- Gold layer: Fact Table - Job Offers (Schéma en Étoile)



WITH jobs AS (
    SELECT
        j.*
    FROM "memory"."main_silver"."int_job_title_normalization" j
),

companies AS (
    SELECT * FROM "memory"."main_gold"."dim_company"
),

locations AS (
    SELECT * FROM "memory"."main_gold"."dim_location"
),

times AS (
    SELECT * FROM "memory"."main_gold"."dim_time"
),

fact_table AS (
    SELECT
        -- Surrogate keys
        ROW_NUMBER() OVER (ORDER BY j.job_url, j.company_name_cleaned) as job_offer_id,
        
        -- Foreign keys
        c.company_id,
        l.location_id,
        t.date_id as published_date_id,
        
        -- Job dimensions
        j.job_title_cleaned as job_title,
        j.job_category,
        j.contract_type_normalized as contract_type,
        j.work_type_normalized as work_type,
        
        -- URLs
        j.job_url,
        j.company_url,
        
        -- Description
        j.job_description_cleaned as job_description,
        
        -- Time dimension
        j.published_date,
        j.posted_time,
        j.published_year_month,
        j.published_year,
        j.published_month,
        
        -- Metrics
        LENGTH(j.job_description_cleaned) as description_length,
        (LENGTH(j.job_description_cleaned) - LENGTH(REPLACE(j.job_description_cleaned, ' ', ''))) + 1 as word_count,
        
        -- Flags
        CASE WHEN j.work_type_normalized = 'Remote' THEN 1 ELSE 0 END as is_remote,
        CASE WHEN j.contract_type_normalized = 'Permanent' THEN 1 ELSE 0 END as is_permanent,
        
        -- Metadata
        CURRENT_TIMESTAMP() as created_at,
        j.ingestion_timestamp
        
    FROM jobs j
    LEFT JOIN companies c ON j.company_name_cleaned = c.company_name
    LEFT JOIN locations l ON j.location_cleaned = l.location_raw
    LEFT JOIN times t ON j.published_date = t.date_id
)

SELECT
    job_offer_id,
    company_id,
    location_id,
    published_date_id,
    job_title,
    job_category,
    contract_type,
    work_type,
    job_url,
    company_url,
    job_description,
    published_date,
    posted_time,
    published_year_month,
    published_year,
    published_month,
    description_length,
    word_count,
    is_remote,
    is_permanent,
    created_at,
    ingestion_timestamp
FROM fact_table
ORDER BY published_date DESC, job_offer_id
    );
  
  
[0m11:32:15.543644 [debug] [Thread-1 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.fact_job_offers"} */

  
    
    

    create  table
      "memory"."main_gold"."fact_job_offers__dbt_tmp"
  
    as (
      -- models/gold/fact_job_offers.sql
-- Gold layer: Fact Table - Job Offers (Schéma en Étoile)



WITH jobs AS (
    SELECT
        j.*
    FROM "memory"."main_silver"."int_job_title_normalization" j
),

companies AS (
    SELECT * FROM "memory"."main_gold"."dim_company"
),

locations AS (
    SELECT * FROM "memory"."main_gold"."dim_location"
),

times AS (
    SELECT * FROM "memory"."main_gold"."dim_time"
),

fact_table AS (
    SELECT
        -- Surrogate keys
        ROW_NUMBER() OVER (ORDER BY j.job_url, j.company_name_cleaned) as job_offer_id,
        
        -- Foreign keys
        c.company_id,
        l.location_id,
        t.date_id as published_date_id,
        
        -- Job dimensions
        j.job_title_cleaned as job_title,
        j.job_category,
        j.contract_type_normalized as contract_type,
        j.work_type_normalized as work_type,
        
        -- URLs
        j.job_url,
        j.company_url,
        
        -- Description
        j.job_description_cleaned as job_description,
        
        -- Time dimension
        j.published_date,
        j.posted_time,
        j.published_year_month,
        j.published_year,
        j.published_month,
        
        -- Metrics
        LENGTH(j.job_description_cleaned) as description_length,
        (LENGTH(j.job_description_cleaned) - LENGTH(REPLACE(j.job_description_cleaned, ' ', ''))) + 1 as word_count,
        
        -- Flags
        CASE WHEN j.work_type_normalized = 'Remote' THEN 1 ELSE 0 END as is_remote,
        CASE WHEN j.contract_type_normalized = 'Permanent' THEN 1 ELSE 0 END as is_permanent,
        
        -- Metadata
        CURRENT_TIMESTAMP() as created_at,
        j.ingestion_timestamp
        
    FROM jobs j
    LEFT JOIN companies c ON j.company_name_cleaned = c.company_name
    LEFT JOIN locations l ON j.location_cleaned = l.location_raw
    LEFT JOIN times t ON j.published_date = t.date_id
)

SELECT
    job_offer_id,
    company_id,
    location_id,
    published_date_id,
    job_title,
    job_category,
    contract_type,
    work_type,
    job_url,
    company_url,
    job_description,
    published_date,
    posted_time,
    published_year_month,
    published_year,
    published_month,
    description_length,
    word_count,
    is_remote,
    is_permanent,
    created_at,
    ingestion_timestamp
FROM fact_table
ORDER BY published_date DESC, job_offer_id
    );
  
  
[0m11:32:15.544234 [debug] [Thread-1 (]: DuckDB adapter: Rolling back transaction.
[0m11:32:15.544735 [debug] [Thread-1 (]: On model.job_intelligent.fact_job_offers: ROLLBACK
[0m11:32:15.553581 [debug] [Thread-1 (]: Failed to rollback 'model.job_intelligent.fact_job_offers'
[0m11:32:15.554037 [debug] [Thread-1 (]: On model.job_intelligent.fact_job_offers: Close
[0m11:32:15.558408 [debug] [Thread-1 (]: Runtime Error in model fact_job_offers (models\gold\fact_job_offers.sql)
  Catalog Error: Scalar Function with name current_timestamp does not exist!
  Did you mean "current_localtimestamp"?
  
  LINE 73:         CURRENT_TIMESTAMP() as created_at,
                   ^
[0m11:32:15.558881 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0ef3892f-5dd3-4f39-80e0-5f145d846a83', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ED3F0173B0>]}
[0m11:32:15.559618 [error] [Thread-1 (]: 8 of 13 ERROR creating sql table model main_gold.fact_job_offers ............... [[31mERROR[0m in 0.03s]
[0m11:32:15.560291 [debug] [Thread-1 (]: Finished running node model.job_intelligent.fact_job_offers
[0m11:32:15.560868 [debug] [Thread-7 (]: Marking all children of 'model.job_intelligent.fact_job_offers' to be skipped because of status 'error'.  Reason: Runtime Error in model fact_job_offers (models\gold\fact_job_offers.sql)
  Catalog Error: Scalar Function with name current_timestamp does not exist!
  Did you mean "current_localtimestamp"?
  
  LINE 73:         CURRENT_TIMESTAMP() as created_at,
                   ^.
[0m11:32:15.562136 [debug] [Thread-2 (]: Began running node model.job_intelligent.agg_location_analysis
[0m11:32:15.562835 [debug] [Thread-4 (]: Began running node model.job_intelligent.agg_job_offers_by_category_time
[0m11:32:15.563470 [info ] [Thread-2 (]: 10 of 13 SKIP relation main_gold.agg_location_analysis ......................... [[33mSKIP[0m]
[0m11:32:15.564249 [info ] [Thread-4 (]: 9 of 13 SKIP relation main_gold.agg_job_offers_by_category_time ................ [[33mSKIP[0m]
[0m11:32:15.564976 [debug] [Thread-2 (]: Finished running node model.job_intelligent.agg_location_analysis
[0m11:32:15.565520 [debug] [Thread-4 (]: Finished running node model.job_intelligent.agg_job_offers_by_category_time
[0m11:32:17.420094 [debug] [Thread-3 (]: SQL status: OK in 1.950 seconds
[0m11:32:17.423222 [debug] [Thread-3 (]: Using duckdb connection "model.job_intelligent.int_skills_extraction"
[0m11:32:17.423569 [debug] [Thread-3 (]: On model.job_intelligent.int_skills_extraction: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.int_skills_extraction"} */
alter table "memory"."main_silver"."int_skills_extraction__dbt_tmp" rename to "int_skills_extraction"
[0m11:32:17.424227 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m11:32:17.425504 [debug] [Thread-3 (]: On model.job_intelligent.int_skills_extraction: COMMIT
[0m11:32:17.425855 [debug] [Thread-3 (]: Using duckdb connection "model.job_intelligent.int_skills_extraction"
[0m11:32:17.426165 [debug] [Thread-3 (]: On model.job_intelligent.int_skills_extraction: COMMIT
[0m11:32:17.426796 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m11:32:17.428930 [debug] [Thread-3 (]: Using duckdb connection "model.job_intelligent.int_skills_extraction"
[0m11:32:17.429357 [debug] [Thread-3 (]: On model.job_intelligent.int_skills_extraction: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.int_skills_extraction"} */

      drop table if exists "memory"."main_silver"."int_skills_extraction__dbt_backup" cascade
    
[0m11:32:17.430064 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m11:32:17.431277 [debug] [Thread-3 (]: On model.job_intelligent.int_skills_extraction: Close
[0m11:32:17.431736 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0ef3892f-5dd3-4f39-80e0-5f145d846a83', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ED3F0168D0>]}
[0m11:32:17.432259 [info ] [Thread-3 (]: 7 of 13 OK created sql table model main_silver.int_skills_extraction ........... [[32mOK[0m in 2.03s]
[0m11:32:17.432830 [debug] [Thread-3 (]: Finished running node model.job_intelligent.int_skills_extraction
[0m11:32:17.433518 [debug] [Thread-1 (]: Began running node model.job_intelligent.dim_skills
[0m11:32:17.433940 [info ] [Thread-1 (]: 11 of 13 START sql table model main_gold.dim_skills ............................ [RUN]
[0m11:32:17.434369 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.job_intelligent.fact_job_offers, now model.job_intelligent.dim_skills)
[0m11:32:17.434705 [debug] [Thread-1 (]: Began compiling node model.job_intelligent.dim_skills
[0m11:32:17.437502 [debug] [Thread-1 (]: Writing injected SQL for node "model.job_intelligent.dim_skills"
[0m11:32:17.438489 [debug] [Thread-1 (]: Began executing node model.job_intelligent.dim_skills
[0m11:32:17.441670 [debug] [Thread-1 (]: Writing runtime sql for node "model.job_intelligent.dim_skills"
[0m11:32:17.442498 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.dim_skills"
[0m11:32:17.442918 [debug] [Thread-1 (]: On model.job_intelligent.dim_skills: BEGIN
[0m11:32:17.443262 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:32:17.443956 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m11:32:17.444324 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.dim_skills"
[0m11:32:17.444800 [debug] [Thread-1 (]: On model.job_intelligent.dim_skills: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_skills"} */

  
    
    

    create  table
      "memory"."main_gold"."dim_skills__dbt_tmp"
  
    as (
      -- models/gold/dim_skills.sql
-- Gold layer: Dimension Skills



WITH skills AS (
    SELECT DISTINCT
        skill_name
    FROM "memory"."main_silver"."int_skills_extraction"
    WHERE skill_name IS NOT NULL
),

skill_categorization AS (
    SELECT
        skill_name,
        CASE
            WHEN skill_name IN ('Python', 'Java', 'Scala', 'R') THEN 'Programming Language'
            WHEN skill_name IN ('SQL', 'NoSQL') THEN 'Database'
            WHEN skill_name IN ('Spark', 'Hadoop', 'Hive', 'Kafka') THEN 'Big Data Framework'
            WHEN skill_name IN ('TensorFlow', 'PyTorch', 'Scikit-learn') THEN 'ML/DL Library'
            WHEN skill_name IN ('AWS', 'Azure', 'GCP') THEN 'Cloud Platform'
            WHEN skill_name IN ('Tableau', 'Power BI', 'Looker') THEN 'BI Tool'
            WHEN skill_name IN ('Airflow', 'DBT', 'Kubernetes', 'Docker') THEN 'DataOps/DevOps'
            WHEN skill_name IN ('Pandas', 'NumPy', 'Matplotlib') THEN 'Data Analysis Library'
            WHEN skill_name IN ('Machine Learning', 'Statistics', 'Data Visualization') THEN 'Domain Knowledge'
            ELSE 'Other'
        END as skill_category
    FROM skills
),

ranked_skills AS (
    SELECT
        ROW_NUMBER() OVER (ORDER BY skill_name) as skill_id,
        skill_name,
        skill_category,
        CURRENT_TIMESTAMP() as created_at
    FROM skill_categorization
)

SELECT
    skill_id,
    skill_name,
    skill_category,
    created_at
FROM ranked_skills
ORDER BY skill_id
    );
  
  
[0m11:32:17.447126 [debug] [Thread-1 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_skills"} */

  
    
    

    create  table
      "memory"."main_gold"."dim_skills__dbt_tmp"
  
    as (
      -- models/gold/dim_skills.sql
-- Gold layer: Dimension Skills



WITH skills AS (
    SELECT DISTINCT
        skill_name
    FROM "memory"."main_silver"."int_skills_extraction"
    WHERE skill_name IS NOT NULL
),

skill_categorization AS (
    SELECT
        skill_name,
        CASE
            WHEN skill_name IN ('Python', 'Java', 'Scala', 'R') THEN 'Programming Language'
            WHEN skill_name IN ('SQL', 'NoSQL') THEN 'Database'
            WHEN skill_name IN ('Spark', 'Hadoop', 'Hive', 'Kafka') THEN 'Big Data Framework'
            WHEN skill_name IN ('TensorFlow', 'PyTorch', 'Scikit-learn') THEN 'ML/DL Library'
            WHEN skill_name IN ('AWS', 'Azure', 'GCP') THEN 'Cloud Platform'
            WHEN skill_name IN ('Tableau', 'Power BI', 'Looker') THEN 'BI Tool'
            WHEN skill_name IN ('Airflow', 'DBT', 'Kubernetes', 'Docker') THEN 'DataOps/DevOps'
            WHEN skill_name IN ('Pandas', 'NumPy', 'Matplotlib') THEN 'Data Analysis Library'
            WHEN skill_name IN ('Machine Learning', 'Statistics', 'Data Visualization') THEN 'Domain Knowledge'
            ELSE 'Other'
        END as skill_category
    FROM skills
),

ranked_skills AS (
    SELECT
        ROW_NUMBER() OVER (ORDER BY skill_name) as skill_id,
        skill_name,
        skill_category,
        CURRENT_TIMESTAMP() as created_at
    FROM skill_categorization
)

SELECT
    skill_id,
    skill_name,
    skill_category,
    created_at
FROM ranked_skills
ORDER BY skill_id
    );
  
  
[0m11:32:17.447699 [debug] [Thread-1 (]: DuckDB adapter: Rolling back transaction.
[0m11:32:17.448197 [debug] [Thread-1 (]: On model.job_intelligent.dim_skills: ROLLBACK
[0m11:32:17.453393 [debug] [Thread-1 (]: Failed to rollback 'model.job_intelligent.dim_skills'
[0m11:32:17.453809 [debug] [Thread-1 (]: On model.job_intelligent.dim_skills: Close
[0m11:32:17.458465 [debug] [Thread-1 (]: Runtime Error in model dim_skills (models\gold\dim_skills.sql)
  Catalog Error: Scalar Function with name current_timestamp does not exist!
  Did you mean "current_localtimestamp"?
  
  LINE 46:         CURRENT_TIMESTAMP() as created_at
                   ^
[0m11:32:17.458959 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0ef3892f-5dd3-4f39-80e0-5f145d846a83', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ED401960F0>]}
[0m11:32:17.459632 [error] [Thread-1 (]: 11 of 13 ERROR creating sql table model main_gold.dim_skills ................... [[31mERROR[0m in 0.02s]
[0m11:32:17.460353 [debug] [Thread-1 (]: Finished running node model.job_intelligent.dim_skills
[0m11:32:17.460980 [debug] [Thread-7 (]: Marking all children of 'model.job_intelligent.dim_skills' to be skipped because of status 'error'.  Reason: Runtime Error in model dim_skills (models\gold\dim_skills.sql)
  Catalog Error: Scalar Function with name current_timestamp does not exist!
  Did you mean "current_localtimestamp"?
  
  LINE 46:         CURRENT_TIMESTAMP() as created_at
                   ^.
[0m11:32:17.462058 [debug] [Thread-2 (]: Began running node model.job_intelligent.fact_job_skills
[0m11:32:17.462775 [info ] [Thread-2 (]: 12 of 13 SKIP relation main_gold.fact_job_skills ............................... [[33mSKIP[0m]
[0m11:32:17.463453 [debug] [Thread-2 (]: Finished running node model.job_intelligent.fact_job_skills
[0m11:32:17.464774 [debug] [Thread-4 (]: Began running node model.job_intelligent.agg_skills_demand
[0m11:32:17.465293 [info ] [Thread-4 (]: 13 of 13 SKIP relation main_gold.agg_skills_demand ............................. [[33mSKIP[0m]
[0m11:32:17.465863 [debug] [Thread-4 (]: Finished running node model.job_intelligent.agg_skills_demand
[0m11:32:17.467514 [debug] [MainThread]: Using duckdb connection "master"
[0m11:32:17.467889 [debug] [MainThread]: On master: BEGIN
[0m11:32:17.468213 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m11:32:17.468870 [debug] [MainThread]: SQL status: OK in 0.001 seconds
[0m11:32:17.469215 [debug] [MainThread]: On master: COMMIT
[0m11:32:17.469547 [debug] [MainThread]: Using duckdb connection "master"
[0m11:32:17.469877 [debug] [MainThread]: On master: COMMIT
[0m11:32:17.470395 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m11:32:17.470739 [debug] [MainThread]: On master: Close
[0m11:32:17.471157 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:32:17.471468 [debug] [MainThread]: Connection 'create_memory_main_gold' was properly closed.
[0m11:32:17.471773 [debug] [MainThread]: Connection 'create_memory_main_bronze' was properly closed.
[0m11:32:17.472070 [debug] [MainThread]: Connection 'create_memory_main_silver' was properly closed.
[0m11:32:17.472362 [debug] [MainThread]: Connection 'list_memory_main_silver' was properly closed.
[0m11:32:17.472656 [debug] [MainThread]: Connection 'list_memory_main_bronze' was properly closed.
[0m11:32:17.472948 [debug] [MainThread]: Connection 'list_memory_main_gold' was properly closed.
[0m11:32:17.473239 [debug] [MainThread]: Connection 'model.job_intelligent.dim_skills' was properly closed.
[0m11:32:17.473531 [debug] [MainThread]: Connection 'model.job_intelligent.dim_time' was properly closed.
[0m11:32:17.473860 [debug] [MainThread]: Connection 'model.job_intelligent.int_skills_extraction' was properly closed.
[0m11:32:17.474152 [debug] [MainThread]: Connection 'model.job_intelligent.dim_company' was properly closed.
[0m11:32:17.474573 [info ] [MainThread]: 
[0m11:32:17.474972 [info ] [MainThread]: Finished running 12 table models, 1 view model in 0 hours 0 minutes and 2.75 seconds (2.75s).
[0m11:32:17.477318 [debug] [MainThread]: Command end result
[0m11:32:17.510866 [debug] [MainThread]: Wrote artifact WritableManifest to D:\lab2\dbt_project\target\manifest.json
[0m11:32:17.513959 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\lab2\dbt_project\target\semantic_manifest.json
[0m11:32:17.521414 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\lab2\dbt_project\target\run_results.json
[0m11:32:17.521827 [info ] [MainThread]: 
[0m11:32:17.522260 [info ] [MainThread]: [31mCompleted with 2 errors, 0 partial successes, and 0 warnings:[0m
[0m11:32:17.522664 [info ] [MainThread]: 
[0m11:32:17.523132 [error] [MainThread]: [31mFailure in model fact_job_offers (models\gold\fact_job_offers.sql)[0m
[0m11:32:17.523675 [error] [MainThread]:   Runtime Error in model fact_job_offers (models\gold\fact_job_offers.sql)
  Catalog Error: Scalar Function with name current_timestamp does not exist!
  Did you mean "current_localtimestamp"?
  
  LINE 73:         CURRENT_TIMESTAMP() as created_at,
                   ^
[0m11:32:17.524131 [info ] [MainThread]: 
[0m11:32:17.524596 [info ] [MainThread]:   compiled code at target\compiled\job_intelligent\models\gold\fact_job_offers.sql
[0m11:32:17.524983 [info ] [MainThread]: 
[0m11:32:17.525438 [error] [MainThread]: [31mFailure in model dim_skills (models\gold\dim_skills.sql)[0m
[0m11:32:17.525913 [error] [MainThread]:   Runtime Error in model dim_skills (models\gold\dim_skills.sql)
  Catalog Error: Scalar Function with name current_timestamp does not exist!
  Did you mean "current_localtimestamp"?
  
  LINE 46:         CURRENT_TIMESTAMP() as created_at
                   ^
[0m11:32:17.526285 [info ] [MainThread]: 
[0m11:32:17.526708 [info ] [MainThread]:   compiled code at target\compiled\job_intelligent\models\gold\dim_skills.sql
[0m11:32:17.527078 [info ] [MainThread]: 
[0m11:32:17.527488 [info ] [MainThread]: Done. PASS=7 WARN=0 ERROR=2 SKIP=4 NO-OP=0 TOTAL=13
[0m11:32:17.528621 [debug] [MainThread]: Command `dbt run` failed at 11:32:17.528437 after 4.03 seconds
[0m11:32:17.529218 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ED3D490890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ED3D834830>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ED401D3B30>]}
[0m11:32:17.529703 [debug] [MainThread]: Flushing usage events
[0m11:32:19.138680 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:33:19.132798 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AB6F855FD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AB6D560190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AB6D3D7890>]}


============================== 11:33:19.141424 | defe3f6e-7d02-4437-b76f-d9926019f1cb ==============================
[0m11:33:19.141424 [info ] [MainThread]: Running with dbt=1.10.13
[0m11:33:19.142519 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'use_experimental_parser': 'False', 'cache_selected_only': 'False', 'warn_error': 'None', 'empty': 'None', 'debug': 'False', 'static_parser': 'True', 'log_cache_events': 'False', 'profiles_dir': 'D:\\lab2\\dbt_project', 'quiet': 'False', 'log_format': 'default', 'use_colors': 'True', 'partial_parse': 'True', 'version_check': 'True', 'log_path': 'D:\\lab2\\dbt_project\\logs', 'introspect': 'True', 'invocation_command': 'dbt debug', 'fail_fast': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'no_print': 'None', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'write_json': 'True', 'target_path': 'None'}
[0m11:33:19.188522 [info ] [MainThread]: dbt version: 1.10.13
[0m11:33:19.189103 [info ] [MainThread]: python version: 3.13.2
[0m11:33:19.189573 [info ] [MainThread]: python path: C:\Users\HP\AppData\Local\Programs\Python\Python313\python.exe
[0m11:33:19.190043 [info ] [MainThread]: os info: Windows-11-10.0.26200-SP0
[0m11:33:19.347703 [info ] [MainThread]: Using profiles dir at D:\lab2\dbt_project
[0m11:33:19.348253 [info ] [MainThread]: Using profiles.yml file at D:\lab2\dbt_project\profiles.yml
[0m11:33:19.348763 [info ] [MainThread]: Using dbt_project.yml file at D:\lab2\dbt_project\dbt_project.yml
[0m11:33:19.351992 [info ] [MainThread]: adapter type: duckdb
[0m11:33:19.352473 [info ] [MainThread]: adapter version: 1.10.0
[0m11:33:19.495630 [info ] [MainThread]: Configuration:
[0m11:33:19.496362 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m11:33:19.497089 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m11:33:19.497563 [info ] [MainThread]: Required dependencies:
[0m11:33:19.497938 [debug] [MainThread]: Executing "git --help"
[0m11:33:19.537357 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--no-lazy-fetch]\n           [--no-optional-locks] [--no-advice] [--bare] [--git-dir=<path>]\n           [--work-tree=<path>] [--namespace=<name>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone      Clone a repository into a new directory\n   init       Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add        Add file contents to the index\n   mv         Move or rename a file, a directory, or a symlink\n   restore    Restore working tree files\n   rm         Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect     Use binary search to find the commit that introduced a bug\n   diff       Show changes between commits, commit and working tree, etc\n   grep       Print lines matching a pattern\n   log        Show commit logs\n   show       Show various types of objects\n   status     Show the working tree status\n\ngrow, mark and tweak your common history\n   backfill   Download missing objects in a partial clone\n   branch     List, create, or delete branches\n   commit     Record changes to the repository\n   merge      Join two or more development histories together\n   rebase     Reapply commits on top of another base tip\n   reset      Reset current HEAD to the specified state\n   switch     Switch branches\n   tag        Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch      Download objects and refs from another repository\n   pull       Fetch from and integrate with another repository or a local branch\n   push       Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m11:33:19.537905 [debug] [MainThread]: STDERR: "b''"
[0m11:33:19.538349 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m11:33:19.538738 [info ] [MainThread]: Connection:
[0m11:33:19.539105 [info ] [MainThread]:   database: memory
[0m11:33:19.539449 [info ] [MainThread]:   schema: main
[0m11:33:19.539795 [info ] [MainThread]:   path: :memory:
[0m11:33:19.540130 [info ] [MainThread]:   config_options: None
[0m11:33:19.540469 [info ] [MainThread]:   extensions: None
[0m11:33:19.540808 [info ] [MainThread]:   settings: {}
[0m11:33:19.541144 [info ] [MainThread]:   external_root: .
[0m11:33:19.541478 [info ] [MainThread]:   use_credential_provider: None
[0m11:33:19.541826 [info ] [MainThread]:   attach: None
[0m11:33:19.542161 [info ] [MainThread]:   filesystems: None
[0m11:33:19.542495 [info ] [MainThread]:   remote: None
[0m11:33:19.542830 [info ] [MainThread]:   plugins: None
[0m11:33:19.543165 [info ] [MainThread]:   disable_transactions: False
[0m11:33:19.543778 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m11:33:19.817710 [debug] [MainThread]: Acquiring new duckdb connection 'debug'
[0m11:33:19.884529 [debug] [MainThread]: Using duckdb connection "debug"
[0m11:33:19.884946 [debug] [MainThread]: On debug: select 1 as id
[0m11:33:19.885275 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:33:19.906023 [debug] [MainThread]: SQL status: OK in 0.021 seconds
[0m11:33:19.907309 [debug] [MainThread]: On debug: Close
[0m11:33:19.907804 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m11:33:19.908291 [info ] [MainThread]: [32mAll checks passed![0m
[0m11:33:19.909530 [debug] [MainThread]: Command `dbt debug` succeeded at 11:33:19.909375 after 1.01 seconds
[0m11:33:19.909957 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m11:33:19.910424 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AB71DD9F30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AB71E815B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AB71D50E20>]}
[0m11:33:19.910950 [debug] [MainThread]: Flushing usage events
[0m11:33:27.170781 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:33:30.460140 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A5777F5FD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A5754F4190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A575367890>]}


============================== 11:33:30.465321 | 784e203c-fcf8-4651-9396-5ca79c58d4e2 ==============================
[0m11:33:30.465321 [info ] [MainThread]: Running with dbt=1.10.13
[0m11:33:30.465937 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'write_json': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'empty': 'False', 'debug': 'False', 'static_parser': 'True', 'log_cache_events': 'False', 'profiles_dir': 'D:\\lab2\\dbt_project', 'quiet': 'False', 'log_format': 'default', 'use_colors': 'True', 'partial_parse': 'True', 'version_check': 'True', 'log_path': 'D:\\lab2\\dbt_project\\logs', 'introspect': 'True', 'no_print': 'None', 'fail_fast': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'invocation_command': 'dbt run', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'target_path': 'None', 'use_experimental_parser': 'False'}
[0m11:33:30.835471 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '784e203c-fcf8-4651-9396-5ca79c58d4e2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A577CB3230>]}
[0m11:33:30.931280 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '784e203c-fcf8-4651-9396-5ca79c58d4e2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A577868380>]}
[0m11:33:30.934523 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m11:33:31.194913 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m11:33:31.360478 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 2 files changed.
[0m11:33:31.361131 [debug] [MainThread]: Partial parsing: updated file: job_intelligent://models\gold\fact_job_offers.sql
[0m11:33:31.361587 [debug] [MainThread]: Partial parsing: updated file: job_intelligent://models\gold\dim_skills.sql
[0m11:33:31.719309 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- seeds
[0m11:33:31.725268 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '784e203c-fcf8-4651-9396-5ca79c58d4e2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A578BA6950>]}
[0m11:33:31.789272 [debug] [MainThread]: Wrote artifact WritableManifest to D:\lab2\dbt_project\target\manifest.json
[0m11:33:31.792516 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\lab2\dbt_project\target\semantic_manifest.json
[0m11:33:31.827605 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '784e203c-fcf8-4651-9396-5ca79c58d4e2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A5787EB7A0>]}
[0m11:33:31.828143 [info ] [MainThread]: Found 13 models, 456 macros
[0m11:33:31.828566 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '784e203c-fcf8-4651-9396-5ca79c58d4e2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A579CA27B0>]}
[0m11:33:31.830763 [info ] [MainThread]: 
[0m11:33:31.831187 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m11:33:31.831557 [info ] [MainThread]: 
[0m11:33:31.832093 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m11:33:31.838324 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_memory'
[0m11:33:31.851724 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_memory'
[0m11:33:31.854425 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_memory'
[0m11:33:31.934357 [debug] [ThreadPool]: Using duckdb connection "list_memory"
[0m11:33:31.934803 [debug] [ThreadPool]: On list_memory: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_memory"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"memory"'
    
  
  
[0m11:33:31.935138 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:33:31.935960 [debug] [ThreadPool]: Using duckdb connection "list_memory"
[0m11:33:31.936337 [debug] [ThreadPool]: On list_memory: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_memory"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"memory"'
    
  
  
[0m11:33:31.936686 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:33:31.937101 [debug] [ThreadPool]: Using duckdb connection "list_memory"
[0m11:33:31.937496 [debug] [ThreadPool]: On list_memory: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_memory"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"memory"'
    
  
  
[0m11:33:31.937817 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:33:31.954503 [debug] [ThreadPool]: SQL status: OK in 0.018 seconds
[0m11:33:31.955046 [debug] [ThreadPool]: SQL status: OK in 0.017 seconds
[0m11:33:31.955444 [debug] [ThreadPool]: SQL status: OK in 0.020 seconds
[0m11:33:31.957211 [debug] [ThreadPool]: On list_memory: Close
[0m11:33:31.958892 [debug] [ThreadPool]: On list_memory: Close
[0m11:33:31.960608 [debug] [ThreadPool]: On list_memory: Close
[0m11:33:31.961442 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_memory, now create_memory_main_bronze)
[0m11:33:31.961968 [debug] [ThreadPool]: Creating schema "database: "memory"
schema: "main_bronze"
"
[0m11:33:31.968567 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_bronze"
[0m11:33:31.968980 [debug] [ThreadPool]: On create_memory_main_bronze: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_memory_main_bronze"} */

    
        select type from duckdb_databases()
        where lower(database_name)='memory'
        and type='sqlite'
    
  
[0m11:33:31.969330 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:33:31.969996 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_memory, now create_memory_main_gold)
[0m11:33:31.970551 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_memory, now create_memory_main_silver)
[0m11:33:31.971120 [debug] [ThreadPool]: Creating schema "database: "memory"
schema: "main_gold"
"
[0m11:33:31.971573 [debug] [ThreadPool]: SQL status: OK in 0.002 seconds
[0m11:33:31.972234 [debug] [ThreadPool]: Creating schema "database: "memory"
schema: "main_silver"
"
[0m11:33:31.975088 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_gold"
[0m11:33:31.977061 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_silver"
[0m11:33:31.978534 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_bronze"
[0m11:33:31.979135 [debug] [ThreadPool]: On create_memory_main_gold: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_memory_main_gold"} */

    
        select type from duckdb_databases()
        where lower(database_name)='memory'
        and type='sqlite'
    
  
[0m11:33:31.979634 [debug] [ThreadPool]: On create_memory_main_silver: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_memory_main_silver"} */

    
        select type from duckdb_databases()
        where lower(database_name)='memory'
        and type='sqlite'
    
  
[0m11:33:31.980080 [debug] [ThreadPool]: On create_memory_main_bronze: BEGIN
[0m11:33:31.980507 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:33:31.981003 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:33:31.982101 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m11:33:31.982480 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_bronze"
[0m11:33:31.982837 [debug] [ThreadPool]: On create_memory_main_bronze: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_memory_main_bronze"} */

    
    
        create schema if not exists "memory"."main_bronze"
    
[0m11:33:31.983333 [debug] [ThreadPool]: SQL status: OK in 0.002 seconds
[0m11:33:31.984844 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_silver"
[0m11:33:31.985229 [debug] [ThreadPool]: On create_memory_main_silver: BEGIN
[0m11:33:31.985709 [debug] [ThreadPool]: SQL status: OK in 0.005 seconds
[0m11:33:31.987240 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_gold"
[0m11:33:31.987673 [debug] [ThreadPool]: On create_memory_main_gold: BEGIN
[0m11:33:31.988068 [debug] [ThreadPool]: SQL status: OK in 0.005 seconds
[0m11:33:31.989041 [debug] [ThreadPool]: On create_memory_main_bronze: COMMIT
[0m11:33:31.989500 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_bronze"
[0m11:33:31.990015 [debug] [ThreadPool]: On create_memory_main_bronze: COMMIT
[0m11:33:31.990969 [debug] [ThreadPool]: SQL status: OK in 0.005 seconds
[0m11:33:31.991382 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_silver"
[0m11:33:31.991754 [debug] [ThreadPool]: On create_memory_main_silver: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_memory_main_silver"} */

    
    
        create schema if not exists "memory"."main_silver"
    
[0m11:33:31.992361 [debug] [ThreadPool]: SQL status: OK in 0.002 seconds
[0m11:33:31.992751 [debug] [ThreadPool]: On create_memory_main_bronze: Close
[0m11:33:31.993152 [debug] [ThreadPool]: SQL status: OK in 0.005 seconds
[0m11:33:31.993571 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_gold"
[0m11:33:31.993965 [debug] [ThreadPool]: On create_memory_main_gold: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_memory_main_gold"} */

    
    
        create schema if not exists "memory"."main_gold"
    
[0m11:33:31.994851 [debug] [ThreadPool]: SQL status: OK in 0.003 seconds
[0m11:33:31.995981 [debug] [ThreadPool]: On create_memory_main_silver: COMMIT
[0m11:33:31.996397 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_silver"
[0m11:33:31.996775 [debug] [ThreadPool]: On create_memory_main_silver: COMMIT
[0m11:33:31.997272 [debug] [ThreadPool]: SQL status: OK in 0.003 seconds
[0m11:33:31.998327 [debug] [ThreadPool]: On create_memory_main_gold: COMMIT
[0m11:33:31.998737 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_gold"
[0m11:33:31.999116 [debug] [ThreadPool]: On create_memory_main_gold: COMMIT
[0m11:33:31.999597 [debug] [ThreadPool]: SQL status: OK in 0.002 seconds
[0m11:33:32.000025 [debug] [ThreadPool]: On create_memory_main_silver: Close
[0m11:33:32.000744 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m11:33:32.001141 [debug] [ThreadPool]: On create_memory_main_gold: Close
[0m11:33:32.004135 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_memory_main_bronze'
[0m11:33:32.011149 [debug] [ThreadPool]: Using duckdb connection "list_memory_main_bronze"
[0m11:33:32.011803 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_memory_main_gold'
[0m11:33:32.012311 [debug] [ThreadPool]: On list_memory_main_bronze: BEGIN
[0m11:33:32.014978 [debug] [ThreadPool]: Using duckdb connection "list_memory_main_gold"
[0m11:33:32.015519 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:33:32.016080 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_memory_main_silver'
[0m11:33:32.016563 [debug] [ThreadPool]: On list_memory_main_gold: BEGIN
[0m11:33:32.019085 [debug] [ThreadPool]: Using duckdb connection "list_memory_main_silver"
[0m11:33:32.019678 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:33:32.020112 [debug] [ThreadPool]: SQL status: OK in 0.005 seconds
[0m11:33:32.020595 [debug] [ThreadPool]: On list_memory_main_silver: BEGIN
[0m11:33:32.021428 [debug] [ThreadPool]: Using duckdb connection "list_memory_main_bronze"
[0m11:33:32.021923 [debug] [ThreadPool]: SQL status: OK in 0.002 seconds
[0m11:33:32.022375 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:33:32.023006 [debug] [ThreadPool]: On list_memory_main_bronze: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_memory_main_bronze"} */
select
      'memory' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main_bronze'
    and lower(table_catalog) = 'memory'
  
[0m11:33:32.023657 [debug] [ThreadPool]: Using duckdb connection "list_memory_main_gold"
[0m11:33:32.024853 [debug] [ThreadPool]: On list_memory_main_gold: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_memory_main_gold"} */
select
      'memory' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main_gold'
    and lower(table_catalog) = 'memory'
  
[0m11:33:32.025634 [debug] [ThreadPool]: SQL status: OK in 0.003 seconds
[0m11:33:32.026041 [debug] [ThreadPool]: Using duckdb connection "list_memory_main_silver"
[0m11:33:32.026437 [debug] [ThreadPool]: On list_memory_main_silver: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_memory_main_silver"} */
select
      'memory' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main_silver'
    and lower(table_catalog) = 'memory'
  
[0m11:33:32.045038 [debug] [ThreadPool]: SQL status: OK in 0.020 seconds
[0m11:33:32.046464 [debug] [ThreadPool]: On list_memory_main_bronze: ROLLBACK
[0m11:33:32.046992 [debug] [ThreadPool]: SQL status: OK in 0.022 seconds
[0m11:33:32.047437 [debug] [ThreadPool]: SQL status: OK in 0.021 seconds
[0m11:33:32.048737 [debug] [ThreadPool]: On list_memory_main_gold: ROLLBACK
[0m11:33:32.050558 [debug] [ThreadPool]: On list_memory_main_silver: ROLLBACK
[0m11:33:32.051757 [debug] [ThreadPool]: Failed to rollback 'list_memory_main_bronze'
[0m11:33:32.052109 [debug] [ThreadPool]: On list_memory_main_bronze: Close
[0m11:33:32.053399 [debug] [ThreadPool]: Failed to rollback 'list_memory_main_gold'
[0m11:33:32.053733 [debug] [ThreadPool]: On list_memory_main_gold: Close
[0m11:33:32.054858 [debug] [ThreadPool]: Failed to rollback 'list_memory_main_silver'
[0m11:33:32.055175 [debug] [ThreadPool]: On list_memory_main_silver: Close
[0m11:33:32.056296 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '784e203c-fcf8-4651-9396-5ca79c58d4e2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A57A402680>]}
[0m11:33:32.057117 [debug] [MainThread]: Using duckdb connection "master"
[0m11:33:32.057648 [debug] [MainThread]: On master: BEGIN
[0m11:33:32.058040 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:33:32.058727 [debug] [MainThread]: SQL status: OK in 0.001 seconds
[0m11:33:32.059075 [debug] [MainThread]: On master: COMMIT
[0m11:33:32.059406 [debug] [MainThread]: Using duckdb connection "master"
[0m11:33:32.059718 [debug] [MainThread]: On master: COMMIT
[0m11:33:32.060230 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m11:33:32.060571 [debug] [MainThread]: On master: Close
[0m11:33:32.066059 [debug] [Thread-1 (]: Began running node model.job_intelligent.stg_jobs_raw
[0m11:33:32.066680 [info ] [Thread-1 (]: 1 of 13 START sql view model main_bronze.stg_jobs_raw .......................... [RUN]
[0m11:33:32.067328 [debug] [Thread-1 (]: Acquiring new duckdb connection 'model.job_intelligent.stg_jobs_raw'
[0m11:33:32.067735 [debug] [Thread-1 (]: Began compiling node model.job_intelligent.stg_jobs_raw
[0m11:33:32.076636 [debug] [Thread-1 (]: Writing injected SQL for node "model.job_intelligent.stg_jobs_raw"
[0m11:33:32.077577 [debug] [Thread-1 (]: Began executing node model.job_intelligent.stg_jobs_raw
[0m11:33:32.115904 [debug] [Thread-1 (]: Writing runtime sql for node "model.job_intelligent.stg_jobs_raw"
[0m11:33:32.116974 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.stg_jobs_raw"
[0m11:33:32.117432 [debug] [Thread-1 (]: On model.job_intelligent.stg_jobs_raw: BEGIN
[0m11:33:32.117828 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m11:33:32.118754 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m11:33:32.119167 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.stg_jobs_raw"
[0m11:33:32.119615 [debug] [Thread-1 (]: On model.job_intelligent.stg_jobs_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.stg_jobs_raw"} */

  
  create view "memory"."main_bronze"."stg_jobs_raw__dbt_tmp" as (
    -- models/bronze/stg_jobs_raw.sql
-- Bronze layer: Lecture directe des données brutes depuis final_data.csv
-- Renommage et typage minimal



-- Read directly from CSV file
SELECT
    -- Renommer les colonnes pour cohérence
    title as job_title,
    location,
    postedTime as posted_time,
    publishedAt as published_at,
    jobUrl as job_url,
    companyName as company_name,
    companyUrl as company_url,
    description as job_description,
    contractType as contract_type,
    workType as work_type,
    
    -- Métadonnées de traçabilité
    NOW() as ingestion_timestamp,
    '2026-01-08 10:33:30.353086+00:00' as dbt_run_id

FROM read_csv_auto('D:/lab2/final_data.csv')

WHERE 1=1
    -- Filtrer les lignes vides
    AND title IS NOT NULL
    AND description IS NOT NULL
  );

[0m11:33:32.168432 [debug] [Thread-1 (]: SQL status: OK in 0.048 seconds
[0m11:33:32.174675 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.stg_jobs_raw"
[0m11:33:32.175140 [debug] [Thread-1 (]: On model.job_intelligent.stg_jobs_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.stg_jobs_raw"} */
alter view "memory"."main_bronze"."stg_jobs_raw__dbt_tmp" rename to "stg_jobs_raw"
[0m11:33:32.175946 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m11:33:32.185879 [debug] [Thread-1 (]: On model.job_intelligent.stg_jobs_raw: COMMIT
[0m11:33:32.186281 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.stg_jobs_raw"
[0m11:33:32.186633 [debug] [Thread-1 (]: On model.job_intelligent.stg_jobs_raw: COMMIT
[0m11:33:32.187311 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m11:33:32.192691 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.stg_jobs_raw"
[0m11:33:32.193136 [debug] [Thread-1 (]: On model.job_intelligent.stg_jobs_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.stg_jobs_raw"} */

      drop view if exists "memory"."main_bronze"."stg_jobs_raw__dbt_backup" cascade
    
[0m11:33:32.193900 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m11:33:32.196274 [debug] [Thread-1 (]: On model.job_intelligent.stg_jobs_raw: Close
[0m11:33:32.198712 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '784e203c-fcf8-4651-9396-5ca79c58d4e2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A579D293D0>]}
[0m11:33:32.199479 [info ] [Thread-1 (]: 1 of 13 OK created sql view model main_bronze.stg_jobs_raw ..................... [[32mOK[0m in 0.13s]
[0m11:33:32.200367 [debug] [Thread-1 (]: Finished running node model.job_intelligent.stg_jobs_raw
[0m11:33:32.201397 [debug] [Thread-2 (]: Began running node model.job_intelligent.int_jobs_cleaned
[0m11:33:32.202013 [info ] [Thread-2 (]: 2 of 13 START sql table model main_silver.int_jobs_cleaned ..................... [RUN]
[0m11:33:32.202704 [debug] [Thread-2 (]: Acquiring new duckdb connection 'model.job_intelligent.int_jobs_cleaned'
[0m11:33:32.203166 [debug] [Thread-2 (]: Began compiling node model.job_intelligent.int_jobs_cleaned
[0m11:33:32.207860 [debug] [Thread-2 (]: Writing injected SQL for node "model.job_intelligent.int_jobs_cleaned"
[0m11:33:32.208980 [debug] [Thread-2 (]: Began executing node model.job_intelligent.int_jobs_cleaned
[0m11:33:32.236003 [debug] [Thread-2 (]: Writing runtime sql for node "model.job_intelligent.int_jobs_cleaned"
[0m11:33:32.237378 [debug] [Thread-2 (]: Using duckdb connection "model.job_intelligent.int_jobs_cleaned"
[0m11:33:32.238043 [debug] [Thread-2 (]: On model.job_intelligent.int_jobs_cleaned: BEGIN
[0m11:33:32.238517 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m11:33:32.239328 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m11:33:32.239770 [debug] [Thread-2 (]: Using duckdb connection "model.job_intelligent.int_jobs_cleaned"
[0m11:33:32.240266 [debug] [Thread-2 (]: On model.job_intelligent.int_jobs_cleaned: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.int_jobs_cleaned"} */

  
    
    

    create  table
      "memory"."main_silver"."int_jobs_cleaned__dbt_tmp"
  
    as (
      -- models/silver/int_jobs_cleaned.sql
-- Silver layer: Nettoyage et normalisation des données brutes



WITH raw_jobs AS (
    SELECT * FROM "memory"."main_bronze"."stg_jobs_raw"
),

cleaned_jobs AS (
    SELECT
        -- Texte: lowercase, trim, remove special characters
        LOWER(TRIM(job_title)) as job_title_cleaned,
        LOWER(TRIM(location)) as location_cleaned,
        LOWER(TRIM(company_name)) as company_name_cleaned,
        LOWER(TRIM(job_description)) as job_description_cleaned,
        LOWER(TRIM(contract_type)) as contract_type_cleaned,
        LOWER(TRIM(work_type)) as work_type_cleaned,
        
        -- URLs as-is
        job_url,
        company_url,
        
        -- Dates
        TRY_CAST(published_at AS DATE) as published_date,
        posted_time,
        
        -- Extract year-month for time-based analysis
        DATE_TRUNC('month', TRY_CAST(published_at AS DATE)) as published_year_month,
        EXTRACT(YEAR FROM TRY_CAST(published_at AS DATE)) as published_year,
        EXTRACT(MONTH FROM TRY_CAST(published_at AS DATE)) as published_month,
        
        -- Métadonnées
        ingestion_timestamp
    FROM raw_jobs
)

SELECT
    *,
    -- Deduplication flag
    ROW_NUMBER() OVER (
        PARTITION BY job_title_cleaned, company_name_cleaned, location_cleaned 
        ORDER BY published_date DESC
    ) as dedup_rank
FROM cleaned_jobs
    );
  
  
[0m11:33:32.367996 [debug] [Thread-2 (]: SQL status: OK in 0.127 seconds
[0m11:33:32.370754 [debug] [Thread-2 (]: Using duckdb connection "model.job_intelligent.int_jobs_cleaned"
[0m11:33:32.371098 [debug] [Thread-2 (]: On model.job_intelligent.int_jobs_cleaned: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.int_jobs_cleaned"} */
alter table "memory"."main_silver"."int_jobs_cleaned__dbt_tmp" rename to "int_jobs_cleaned"
[0m11:33:32.371759 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m11:33:32.375947 [debug] [Thread-2 (]: On model.job_intelligent.int_jobs_cleaned: COMMIT
[0m11:33:32.376303 [debug] [Thread-2 (]: Using duckdb connection "model.job_intelligent.int_jobs_cleaned"
[0m11:33:32.376625 [debug] [Thread-2 (]: On model.job_intelligent.int_jobs_cleaned: COMMIT
[0m11:33:32.380731 [debug] [Thread-2 (]: SQL status: OK in 0.004 seconds
[0m11:33:32.382707 [debug] [Thread-2 (]: Using duckdb connection "model.job_intelligent.int_jobs_cleaned"
[0m11:33:32.383055 [debug] [Thread-2 (]: On model.job_intelligent.int_jobs_cleaned: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.int_jobs_cleaned"} */

      drop table if exists "memory"."main_silver"."int_jobs_cleaned__dbt_backup" cascade
    
[0m11:33:32.383680 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m11:33:32.384839 [debug] [Thread-2 (]: On model.job_intelligent.int_jobs_cleaned: Close
[0m11:33:32.385321 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '784e203c-fcf8-4651-9396-5ca79c58d4e2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A57A490050>]}
[0m11:33:32.385851 [info ] [Thread-2 (]: 2 of 13 OK created sql table model main_silver.int_jobs_cleaned ................ [[32mOK[0m in 0.18s]
[0m11:33:32.386408 [debug] [Thread-2 (]: Finished running node model.job_intelligent.int_jobs_cleaned
[0m11:33:32.387110 [debug] [Thread-3 (]: Began running node model.job_intelligent.int_job_title_normalization
[0m11:33:32.387560 [info ] [Thread-3 (]: 3 of 13 START sql table model main_silver.int_job_title_normalization .......... [RUN]
[0m11:33:32.388128 [debug] [Thread-3 (]: Acquiring new duckdb connection 'model.job_intelligent.int_job_title_normalization'
[0m11:33:32.388486 [debug] [Thread-3 (]: Began compiling node model.job_intelligent.int_job_title_normalization
[0m11:33:32.391885 [debug] [Thread-3 (]: Writing injected SQL for node "model.job_intelligent.int_job_title_normalization"
[0m11:33:32.392671 [debug] [Thread-3 (]: Began executing node model.job_intelligent.int_job_title_normalization
[0m11:33:32.395413 [debug] [Thread-3 (]: Writing runtime sql for node "model.job_intelligent.int_job_title_normalization"
[0m11:33:32.396229 [debug] [Thread-3 (]: Using duckdb connection "model.job_intelligent.int_job_title_normalization"
[0m11:33:32.396599 [debug] [Thread-3 (]: On model.job_intelligent.int_job_title_normalization: BEGIN
[0m11:33:32.396932 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m11:33:32.397633 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m11:33:32.397991 [debug] [Thread-3 (]: Using duckdb connection "model.job_intelligent.int_job_title_normalization"
[0m11:33:32.398425 [debug] [Thread-3 (]: On model.job_intelligent.int_job_title_normalization: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.int_job_title_normalization"} */

  
    
    

    create  table
      "memory"."main_silver"."int_job_title_normalization__dbt_tmp"
  
    as (
      -- models/silver/int_job_title_normalization.sql
-- Silver layer: Normaliser les intitulés de postes



WITH cleaned_jobs AS (
    SELECT * FROM "memory"."main_silver"."int_jobs_cleaned"
),

title_normalized AS (
    SELECT
        *,
        CASE
            WHEN job_title_cleaned LIKE '%data engineer%' THEN 'Data Engineer'
            WHEN job_title_cleaned LIKE '%data scientist%' THEN 'Data Scientist'
            WHEN job_title_cleaned LIKE '%data analyst%' THEN 'Data Analyst'
            WHEN job_title_cleaned LIKE '%analytics engineer%' THEN 'Analytics Engineer'
            WHEN job_title_cleaned LIKE '%ml engineer%' OR job_title_cleaned LIKE '%machine learning%' THEN 'ML Engineer'
            WHEN job_title_cleaned LIKE '%data architect%' THEN 'Data Architect'
            WHEN job_title_cleaned LIKE '%bi developer%' OR job_title_cleaned LIKE '%business intelligence%' THEN 'BI Developer'
            WHEN job_title_cleaned LIKE '%etl%' OR job_title_cleaned LIKE '%pipeline%' THEN 'ETL/Pipeline Engineer'
            WHEN job_title_cleaned LIKE '%data engineer%' THEN 'Data Engineer'
            ELSE 'Other Data Role'
        END as job_category,
        
        CASE
            WHEN contract_type_cleaned LIKE '%cdi%' OR contract_type_cleaned LIKE '%permanent%' THEN 'Permanent'
            WHEN contract_type_cleaned LIKE '%cdd%' OR contract_type_cleaned LIKE '%contract%' THEN 'Contract'
            WHEN contract_type_cleaned LIKE '%stage%' OR contract_type_cleaned LIKE '%internship%' THEN 'Internship'
            WHEN contract_type_cleaned LIKE '%freelance%' THEN 'Freelance'
            ELSE 'Not Specified'
        END as contract_type_normalized,
        
        CASE
            WHEN work_type_cleaned LIKE '%remote%' THEN 'Remote'
            WHEN work_type_cleaned LIKE '%hybrid%' THEN 'Hybrid'
            WHEN work_type_cleaned LIKE '%onsite%' OR work_type_cleaned LIKE '%on-site%' THEN 'On-site'
            ELSE 'Not Specified'
        END as work_type_normalized
    
    FROM cleaned_jobs
    WHERE dedup_rank = 1  -- Keep only first occurrence (most recent)
)

SELECT * FROM title_normalized
    );
  
  
[0m11:33:32.425660 [debug] [Thread-3 (]: SQL status: OK in 0.027 seconds
[0m11:33:32.428280 [debug] [Thread-3 (]: Using duckdb connection "model.job_intelligent.int_job_title_normalization"
[0m11:33:32.428653 [debug] [Thread-3 (]: On model.job_intelligent.int_job_title_normalization: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.int_job_title_normalization"} */
alter table "memory"."main_silver"."int_job_title_normalization__dbt_tmp" rename to "int_job_title_normalization"
[0m11:33:32.429357 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m11:33:32.430693 [debug] [Thread-3 (]: On model.job_intelligent.int_job_title_normalization: COMMIT
[0m11:33:32.431066 [debug] [Thread-3 (]: Using duckdb connection "model.job_intelligent.int_job_title_normalization"
[0m11:33:32.431406 [debug] [Thread-3 (]: On model.job_intelligent.int_job_title_normalization: COMMIT
[0m11:33:32.431980 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m11:33:32.434130 [debug] [Thread-3 (]: Using duckdb connection "model.job_intelligent.int_job_title_normalization"
[0m11:33:32.434490 [debug] [Thread-3 (]: On model.job_intelligent.int_job_title_normalization: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.int_job_title_normalization"} */

      drop table if exists "memory"."main_silver"."int_job_title_normalization__dbt_backup" cascade
    
[0m11:33:32.435084 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m11:33:32.436255 [debug] [Thread-3 (]: On model.job_intelligent.int_job_title_normalization: Close
[0m11:33:32.436742 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '784e203c-fcf8-4651-9396-5ca79c58d4e2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A579CEE350>]}
[0m11:33:32.437317 [info ] [Thread-3 (]: 3 of 13 OK created sql table model main_silver.int_job_title_normalization ..... [[32mOK[0m in 0.05s]
[0m11:33:32.437916 [debug] [Thread-3 (]: Finished running node model.job_intelligent.int_job_title_normalization
[0m11:33:32.438644 [debug] [Thread-3 (]: Began running node model.job_intelligent.int_skills_extraction
[0m11:33:32.439260 [debug] [Thread-4 (]: Began running node model.job_intelligent.dim_company
[0m11:33:32.439731 [info ] [Thread-3 (]: 7 of 13 START sql table model main_silver.int_skills_extraction ................ [RUN]
[0m11:33:32.440263 [debug] [Thread-1 (]: Began running node model.job_intelligent.dim_location
[0m11:33:32.440822 [info ] [Thread-4 (]: 4 of 13 START sql table model main_gold.dim_company ............................ [RUN]
[0m11:33:32.441414 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly model.job_intelligent.int_job_title_normalization, now model.job_intelligent.int_skills_extraction)
[0m11:33:32.441864 [debug] [Thread-2 (]: Began running node model.job_intelligent.dim_time
[0m11:33:32.442340 [info ] [Thread-1 (]: 5 of 13 START sql table model main_gold.dim_location ........................... [RUN]
[0m11:33:32.442897 [debug] [Thread-4 (]: Acquiring new duckdb connection 'model.job_intelligent.dim_company'
[0m11:33:32.443296 [debug] [Thread-3 (]: Began compiling node model.job_intelligent.int_skills_extraction
[0m11:33:32.443780 [info ] [Thread-2 (]: 6 of 13 START sql table model main_gold.dim_time ............................... [RUN]
[0m11:33:32.444316 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.job_intelligent.stg_jobs_raw, now model.job_intelligent.dim_location)
[0m11:33:32.444711 [debug] [Thread-4 (]: Began compiling node model.job_intelligent.dim_company
[0m11:33:32.447551 [debug] [Thread-3 (]: Writing injected SQL for node "model.job_intelligent.int_skills_extraction"
[0m11:33:32.448042 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly model.job_intelligent.int_jobs_cleaned, now model.job_intelligent.dim_time)
[0m11:33:32.448495 [debug] [Thread-1 (]: Began compiling node model.job_intelligent.dim_location
[0m11:33:32.451677 [debug] [Thread-4 (]: Writing injected SQL for node "model.job_intelligent.dim_company"
[0m11:33:32.452270 [debug] [Thread-2 (]: Began compiling node model.job_intelligent.dim_time
[0m11:33:32.455294 [debug] [Thread-1 (]: Writing injected SQL for node "model.job_intelligent.dim_location"
[0m11:33:32.458819 [debug] [Thread-2 (]: Writing injected SQL for node "model.job_intelligent.dim_time"
[0m11:33:32.459448 [debug] [Thread-3 (]: Began executing node model.job_intelligent.int_skills_extraction
[0m11:33:32.462837 [debug] [Thread-3 (]: Writing runtime sql for node "model.job_intelligent.int_skills_extraction"
[0m11:33:32.463707 [debug] [Thread-2 (]: Began executing node model.job_intelligent.dim_time
[0m11:33:32.464298 [debug] [Thread-3 (]: Using duckdb connection "model.job_intelligent.int_skills_extraction"
[0m11:33:32.464888 [debug] [Thread-4 (]: Began executing node model.job_intelligent.dim_company
[0m11:33:32.467646 [debug] [Thread-2 (]: Writing runtime sql for node "model.job_intelligent.dim_time"
[0m11:33:32.468285 [debug] [Thread-3 (]: On model.job_intelligent.int_skills_extraction: BEGIN
[0m11:33:32.471372 [debug] [Thread-4 (]: Writing runtime sql for node "model.job_intelligent.dim_company"
[0m11:33:32.471892 [debug] [Thread-1 (]: Began executing node model.job_intelligent.dim_location
[0m11:33:32.472558 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m11:33:32.475705 [debug] [Thread-1 (]: Writing runtime sql for node "model.job_intelligent.dim_location"
[0m11:33:32.476635 [debug] [Thread-2 (]: Using duckdb connection "model.job_intelligent.dim_time"
[0m11:33:32.477272 [debug] [Thread-3 (]: SQL status: OK in 0.005 seconds
[0m11:33:32.477776 [debug] [Thread-2 (]: On model.job_intelligent.dim_time: BEGIN
[0m11:33:32.478295 [debug] [Thread-4 (]: Using duckdb connection "model.job_intelligent.dim_company"
[0m11:33:32.478768 [debug] [Thread-3 (]: Using duckdb connection "model.job_intelligent.int_skills_extraction"
[0m11:33:32.479255 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.dim_location"
[0m11:33:32.479667 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m11:33:32.480160 [debug] [Thread-4 (]: On model.job_intelligent.dim_company: BEGIN
[0m11:33:32.480711 [debug] [Thread-3 (]: On model.job_intelligent.int_skills_extraction: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.int_skills_extraction"} */

  
    
    

    create  table
      "memory"."main_silver"."int_skills_extraction__dbt_tmp"
  
    as (
      -- models/silver/int_skills_extraction.sql
-- Silver layer: Extraction des compétences depuis la description



WITH jobs_with_titles AS (
    SELECT * FROM "memory"."main_silver"."int_job_title_normalization"
),

skills_mapping AS (
    -- Définir un mapping de compétences communes Data/Tech
    SELECT
        'Python' as skill_name,
        'python|py |\.py' as skill_pattern
    UNION ALL SELECT 'SQL', 'sql|sql|sql server|postgres|oracle'
    UNION ALL SELECT 'Spark', 'spark|pyspark'
    UNION ALL SELECT 'Hadoop', 'hadoop|hdfs'
    UNION ALL SELECT 'Scala', 'scala'
    UNION ALL SELECT 'Java', '\bjava\b'
    UNION ALL SELECT 'R', '\br\b|r programming'
    UNION ALL SELECT 'Tableau', 'tableau'
    UNION ALL SELECT 'Power BI', 'power bi|powerbi'
    UNION ALL SELECT 'Looker', 'looker'
    UNION ALL SELECT 'AWS', 'aws|amazon web|s3 |ec2|redshift'
    UNION ALL SELECT 'Azure', 'azure|microsoft azure|synapse|cosmos'
    UNION ALL SELECT 'GCP', 'gcp|google cloud|bigquery'
    UNION ALL SELECT 'Airflow', 'airflow'
    UNION ALL SELECT 'DBT', '\bdbt\b|dbt'
    UNION ALL SELECT 'Kubernetes', 'kubernetes|k8s'
    UNION ALL SELECT 'Docker', 'docker'
    UNION ALL SELECT 'Git', 'git|github|gitlab'
    UNION ALL SELECT 'TensorFlow', 'tensorflow'
    UNION ALL SELECT 'PyTorch', 'pytorch'
    UNION ALL SELECT 'Scikit-learn', 'scikit|sklearn'
    UNION ALL SELECT 'Pandas', 'pandas'
    UNION ALL SELECT 'NumPy', 'numpy'
    UNION ALL SELECT 'Machine Learning', 'machine learning|deep learning|ml|artificial intelligence'
    UNION ALL SELECT 'Statistics', 'statistics|statistical|probability'
    UNION ALL SELECT 'Data Visualization', 'data visualization|visualization|charts|graphs'
),

jobs_exploded AS (
    SELECT
        j.*,
        s.skill_name,
        CASE 
            WHEN job_description_cleaned ILIKE '%' || s.skill_pattern || '%' THEN 1 
            ELSE 0 
        END as has_skill
    FROM jobs_with_titles j
    CROSS JOIN skills_mapping s
)

SELECT
    *
FROM jobs_exploded
WHERE has_skill = 1

ORDER BY job_title_cleaned, published_date DESC
    );
  
  
[0m11:33:32.481301 [debug] [Thread-1 (]: On model.job_intelligent.dim_location: BEGIN
[0m11:33:32.481963 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m11:33:32.482450 [debug] [Thread-2 (]: SQL status: OK in 0.003 seconds
[0m11:33:32.483341 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:33:32.484031 [debug] [Thread-2 (]: Using duckdb connection "model.job_intelligent.dim_time"
[0m11:33:32.484537 [debug] [Thread-4 (]: SQL status: OK in 0.003 seconds
[0m11:33:32.485177 [debug] [Thread-2 (]: On model.job_intelligent.dim_time: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_time"} */

  
    
    

    create  table
      "memory"."main_gold"."dim_time__dbt_tmp"
  
    as (
      -- models/gold/dim_time.sql
-- Gold layer: Dimension Time



-- Generate a date dimension for time-based analysis
WITH date_spine AS (
    SELECT
        published_date,
        EXTRACT(YEAR FROM published_date) as year,
        EXTRACT(MONTH FROM published_date) as month,
        EXTRACT(QUARTER FROM published_date) as quarter,
        EXTRACT(WEEK FROM published_date) as week,
        EXTRACT(DAYOFWEEK FROM published_date) as day_of_week,
        DATE_TRUNC('month', published_date) as month_start,
        DATE_TRUNC('quarter', published_date) as quarter_start,
        DATE_TRUNC('year', published_date) as year_start,
        
        CASE 
            WHEN EXTRACT(MONTH FROM published_date) IN (1,2,3) THEN 'Q1'
            WHEN EXTRACT(MONTH FROM published_date) IN (4,5,6) THEN 'Q2'
            WHEN EXTRACT(MONTH FROM published_date) IN (7,8,9) THEN 'Q3'
            ELSE 'Q4'
        END as quarter_name,
        
        CASE EXTRACT(DAYOFWEEK FROM published_date)
            WHEN 0 THEN 'Sunday'
            WHEN 1 THEN 'Monday'
            WHEN 2 THEN 'Tuesday'
            WHEN 3 THEN 'Wednesday'
            WHEN 4 THEN 'Thursday'
            WHEN 5 THEN 'Friday'
            WHEN 6 THEN 'Saturday'
        END as day_name,
        
        CASE EXTRACT(MONTH FROM published_date)
            WHEN 1 THEN 'January'
            WHEN 2 THEN 'February'
            WHEN 3 THEN 'March'
            WHEN 4 THEN 'April'
            WHEN 5 THEN 'May'
            WHEN 6 THEN 'June'
            WHEN 7 THEN 'July'
            WHEN 8 THEN 'August'
            WHEN 9 THEN 'September'
            WHEN 10 THEN 'October'
            WHEN 11 THEN 'November'
            WHEN 12 THEN 'December'
        END as month_name
        
    FROM (
        SELECT DISTINCT published_date
        FROM "memory"."main_silver"."int_job_title_normalization"
        WHERE published_date IS NOT NULL
    )
)

SELECT
    published_date as date_id,
    year,
    month,
    quarter,
    week,
    day_of_week,
    day_name,
    month_name,
    quarter_name,
    month_start,
    quarter_start,
    year_start,
    NOW() as created_at
FROM date_spine
ORDER BY published_date
    );
  
  
[0m11:33:32.485770 [debug] [Thread-1 (]: SQL status: OK in 0.002 seconds
[0m11:33:32.486206 [debug] [Thread-4 (]: Using duckdb connection "model.job_intelligent.dim_company"
[0m11:33:32.486840 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.dim_location"
[0m11:33:32.487919 [debug] [Thread-4 (]: On model.job_intelligent.dim_company: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_company"} */

  
    
    

    create  table
      "memory"."main_gold"."dim_company__dbt_tmp"
  
    as (
      -- models/gold/dim_company.sql
-- Gold layer: Dimension Company



WITH jobs AS (
    SELECT DISTINCT
        company_name_cleaned,
        company_url
    FROM "memory"."main_silver"."int_job_title_normalization"
    WHERE company_name_cleaned IS NOT NULL
),

ranked_companies AS (
    SELECT
        ROW_NUMBER() OVER (ORDER BY company_name_cleaned) as company_id,
        company_name_cleaned as company_name,
        company_url,
        NOW() as created_at
    FROM jobs
)

SELECT
    company_id,
    company_name,
    company_url,
    created_at
FROM ranked_companies
ORDER BY company_id
    );
  
  
[0m11:33:32.488575 [debug] [Thread-1 (]: On model.job_intelligent.dim_location: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_location"} */

  
    
    

    create  table
      "memory"."main_gold"."dim_location__dbt_tmp"
  
    as (
      -- models/gold/dim_location.sql
-- Gold layer: Dimension Location



WITH jobs AS (
    SELECT DISTINCT
        location_cleaned as location_raw
    FROM "memory"."main_silver"."int_job_title_normalization"
    WHERE location_cleaned IS NOT NULL
),

location_parsed AS (
    SELECT
        location_raw,
        -- Extract city (before comma)
        CASE 
            WHEN POSITION(',' IN location_raw) > 0 
            THEN TRIM(SUBSTRING(location_raw, 1, POSITION(',' IN location_raw) - 1))
            ELSE location_raw
        END as city,
        
        -- Extract country (after comma)
        CASE 
            WHEN POSITION(',' IN location_raw) > 0 
            THEN TRIM(SUBSTRING(location_raw, POSITION(',' IN location_raw) + 1))
            ELSE 'Not Specified'
        END as country,
        
        -- Detect if remote
        CASE 
            WHEN location_raw LIKE '%remote%' 
            THEN 'Remote'
            ELSE 'On-site'
        END as work_location_type
    FROM jobs
),

ranked_locations AS (
    SELECT
        ROW_NUMBER() OVER (ORDER BY location_raw) as location_id,
        location_raw,
        city,
        country,
        work_location_type,
        NOW() as created_at
    FROM location_parsed
)

SELECT
    location_id,
    location_raw,
    city,
    country,
    work_location_type,
    created_at
FROM ranked_locations
ORDER BY location_id
    );
  
  
[0m11:33:32.495731 [debug] [Thread-4 (]: SQL status: OK in 0.006 seconds
[0m11:33:32.499263 [debug] [Thread-4 (]: Using duckdb connection "model.job_intelligent.dim_company"
[0m11:33:32.499702 [debug] [Thread-4 (]: On model.job_intelligent.dim_company: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_company"} */
alter table "memory"."main_gold"."dim_company__dbt_tmp" rename to "dim_company"
[0m11:33:32.500551 [debug] [Thread-4 (]: SQL status: OK in 0.000 seconds
[0m11:33:32.501016 [debug] [Thread-2 (]: SQL status: OK in 0.014 seconds
[0m11:33:32.502609 [debug] [Thread-4 (]: On model.job_intelligent.dim_company: COMMIT
[0m11:33:32.503120 [debug] [Thread-1 (]: SQL status: OK in 0.014 seconds
[0m11:33:32.505593 [debug] [Thread-2 (]: Using duckdb connection "model.job_intelligent.dim_time"
[0m11:33:32.506031 [debug] [Thread-4 (]: Using duckdb connection "model.job_intelligent.dim_company"
[0m11:33:32.508602 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.dim_location"
[0m11:33:32.509124 [debug] [Thread-2 (]: On model.job_intelligent.dim_time: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_time"} */
alter table "memory"."main_gold"."dim_time__dbt_tmp" rename to "dim_time"
[0m11:33:32.509636 [debug] [Thread-4 (]: On model.job_intelligent.dim_company: COMMIT
[0m11:33:32.510146 [debug] [Thread-1 (]: On model.job_intelligent.dim_location: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_location"} */
alter table "memory"."main_gold"."dim_location__dbt_tmp" rename to "dim_location"
[0m11:33:32.511226 [debug] [Thread-4 (]: SQL status: OK in 0.000 seconds
[0m11:33:32.513487 [debug] [Thread-4 (]: Using duckdb connection "model.job_intelligent.dim_company"
[0m11:33:32.513882 [debug] [Thread-4 (]: On model.job_intelligent.dim_company: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_company"} */

      drop table if exists "memory"."main_gold"."dim_company__dbt_backup" cascade
    
[0m11:33:32.514309 [debug] [Thread-2 (]: SQL status: OK in 0.004 seconds
[0m11:33:32.515787 [debug] [Thread-2 (]: On model.job_intelligent.dim_time: COMMIT
[0m11:33:32.516192 [debug] [Thread-2 (]: Using duckdb connection "model.job_intelligent.dim_time"
[0m11:33:32.516555 [debug] [Thread-2 (]: On model.job_intelligent.dim_time: COMMIT
[0m11:33:32.517333 [debug] [Thread-1 (]: SQL status: OK in 0.006 seconds
[0m11:33:32.519529 [debug] [Thread-1 (]: On model.job_intelligent.dim_location: COMMIT
[0m11:33:32.520048 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.dim_location"
[0m11:33:32.520545 [debug] [Thread-1 (]: On model.job_intelligent.dim_location: COMMIT
[0m11:33:32.521228 [debug] [Thread-2 (]: SQL status: OK in 0.004 seconds
[0m11:33:32.523402 [debug] [Thread-2 (]: Using duckdb connection "model.job_intelligent.dim_time"
[0m11:33:32.524031 [debug] [Thread-1 (]: SQL status: OK in 0.003 seconds
[0m11:33:32.524619 [debug] [Thread-4 (]: SQL status: OK in 0.010 seconds
[0m11:33:32.525212 [debug] [Thread-2 (]: On model.job_intelligent.dim_time: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_time"} */

      drop table if exists "memory"."main_gold"."dim_time__dbt_backup" cascade
    
[0m11:33:32.527562 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.dim_location"
[0m11:33:32.528910 [debug] [Thread-4 (]: On model.job_intelligent.dim_company: Close
[0m11:33:32.529558 [debug] [Thread-1 (]: On model.job_intelligent.dim_location: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_location"} */

      drop table if exists "memory"."main_gold"."dim_location__dbt_backup" cascade
    
[0m11:33:32.530210 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m11:33:32.531370 [debug] [Thread-2 (]: On model.job_intelligent.dim_time: Close
[0m11:33:32.531889 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '784e203c-fcf8-4651-9396-5ca79c58d4e2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A57A4BEC50>]}
[0m11:33:32.532918 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '784e203c-fcf8-4651-9396-5ca79c58d4e2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A57A5BDA90>]}
[0m11:33:32.532475 [info ] [Thread-4 (]: 4 of 13 OK created sql table model main_gold.dim_company ....................... [[32mOK[0m in 0.09s]
[0m11:33:32.534243 [debug] [Thread-1 (]: SQL status: OK in 0.004 seconds
[0m11:33:32.535020 [debug] [Thread-4 (]: Finished running node model.job_intelligent.dim_company
[0m11:33:32.533565 [info ] [Thread-2 (]: 6 of 13 OK created sql table model main_gold.dim_time .......................... [[32mOK[0m in 0.08s]
[0m11:33:32.537069 [debug] [Thread-1 (]: On model.job_intelligent.dim_location: Close
[0m11:33:32.538009 [debug] [Thread-2 (]: Finished running node model.job_intelligent.dim_time
[0m11:33:32.538683 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '784e203c-fcf8-4651-9396-5ca79c58d4e2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A57A5DB650>]}
[0m11:33:32.539683 [info ] [Thread-1 (]: 5 of 13 OK created sql table model main_gold.dim_location ...................... [[32mOK[0m in 0.09s]
[0m11:33:32.540419 [debug] [Thread-1 (]: Finished running node model.job_intelligent.dim_location
[0m11:33:32.541137 [debug] [Thread-4 (]: Began running node model.job_intelligent.fact_job_offers
[0m11:33:32.541567 [info ] [Thread-4 (]: 8 of 13 START sql table model main_gold.fact_job_offers ........................ [RUN]
[0m11:33:32.542051 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly model.job_intelligent.dim_company, now model.job_intelligent.fact_job_offers)
[0m11:33:32.542383 [debug] [Thread-4 (]: Began compiling node model.job_intelligent.fact_job_offers
[0m11:33:32.545793 [debug] [Thread-4 (]: Writing injected SQL for node "model.job_intelligent.fact_job_offers"
[0m11:33:32.546597 [debug] [Thread-4 (]: Began executing node model.job_intelligent.fact_job_offers
[0m11:33:32.549168 [debug] [Thread-4 (]: Writing runtime sql for node "model.job_intelligent.fact_job_offers"
[0m11:33:32.550031 [debug] [Thread-4 (]: Using duckdb connection "model.job_intelligent.fact_job_offers"
[0m11:33:32.550389 [debug] [Thread-4 (]: On model.job_intelligent.fact_job_offers: BEGIN
[0m11:33:32.550696 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m11:33:32.551388 [debug] [Thread-4 (]: SQL status: OK in 0.001 seconds
[0m11:33:32.551709 [debug] [Thread-4 (]: Using duckdb connection "model.job_intelligent.fact_job_offers"
[0m11:33:32.552122 [debug] [Thread-4 (]: On model.job_intelligent.fact_job_offers: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.fact_job_offers"} */

  
    
    

    create  table
      "memory"."main_gold"."fact_job_offers__dbt_tmp"
  
    as (
      -- models/gold/fact_job_offers.sql
-- Gold layer: Fact Table - Job Offers (Schéma en Étoile)



WITH jobs AS (
    SELECT
        j.*
    FROM "memory"."main_silver"."int_job_title_normalization" j
),

companies AS (
    SELECT * FROM "memory"."main_gold"."dim_company"
),

locations AS (
    SELECT * FROM "memory"."main_gold"."dim_location"
),

times AS (
    SELECT * FROM "memory"."main_gold"."dim_time"
),

fact_table AS (
    SELECT
        -- Surrogate keys
        ROW_NUMBER() OVER (ORDER BY j.job_url, j.company_name_cleaned) as job_offer_id,
        
        -- Foreign keys
        c.company_id,
        l.location_id,
        t.date_id as published_date_id,
        
        -- Job dimensions
        j.job_title_cleaned as job_title,
        j.job_category,
        j.contract_type_normalized as contract_type,
        j.work_type_normalized as work_type,
        
        -- URLs
        j.job_url,
        j.company_url,
        
        -- Description
        j.job_description_cleaned as job_description,
        
        -- Time dimension
        j.published_date,
        j.posted_time,
        j.published_year_month,
        j.published_year,
        j.published_month,
        
        -- Metrics
        LENGTH(j.job_description_cleaned) as description_length,
        (LENGTH(j.job_description_cleaned) - LENGTH(REPLACE(j.job_description_cleaned, ' ', ''))) + 1 as word_count,
        
        -- Flags
        CASE WHEN j.work_type_normalized = 'Remote' THEN 1 ELSE 0 END as is_remote,
        CASE WHEN j.contract_type_normalized = 'Permanent' THEN 1 ELSE 0 END as is_permanent,
        
        -- Metadata
        NOW() as created_at,
        j.ingestion_timestamp
        
    FROM jobs j
    LEFT JOIN companies c ON j.company_name_cleaned = c.company_name
    LEFT JOIN locations l ON j.location_cleaned = l.location_raw
    LEFT JOIN times t ON j.published_date = t.date_id
)

SELECT
    job_offer_id,
    company_id,
    location_id,
    published_date_id,
    job_title,
    job_category,
    contract_type,
    work_type,
    job_url,
    company_url,
    job_description,
    published_date,
    posted_time,
    published_year_month,
    published_year,
    published_month,
    description_length,
    word_count,
    is_remote,
    is_permanent,
    created_at,
    ingestion_timestamp
FROM fact_table
ORDER BY published_date DESC, job_offer_id
    );
  
  
[0m11:33:32.670742 [debug] [Thread-4 (]: SQL status: OK in 0.118 seconds
[0m11:33:32.673755 [debug] [Thread-4 (]: Using duckdb connection "model.job_intelligent.fact_job_offers"
[0m11:33:32.674156 [debug] [Thread-4 (]: On model.job_intelligent.fact_job_offers: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.fact_job_offers"} */
alter table "memory"."main_gold"."fact_job_offers__dbt_tmp" rename to "fact_job_offers"
[0m11:33:32.674935 [debug] [Thread-4 (]: SQL status: OK in 0.000 seconds
[0m11:33:32.676236 [debug] [Thread-4 (]: On model.job_intelligent.fact_job_offers: COMMIT
[0m11:33:32.676574 [debug] [Thread-4 (]: Using duckdb connection "model.job_intelligent.fact_job_offers"
[0m11:33:32.676892 [debug] [Thread-4 (]: On model.job_intelligent.fact_job_offers: COMMIT
[0m11:33:32.677514 [debug] [Thread-4 (]: SQL status: OK in 0.000 seconds
[0m11:33:32.679255 [debug] [Thread-4 (]: Using duckdb connection "model.job_intelligent.fact_job_offers"
[0m11:33:32.679588 [debug] [Thread-4 (]: On model.job_intelligent.fact_job_offers: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.fact_job_offers"} */

      drop table if exists "memory"."main_gold"."fact_job_offers__dbt_backup" cascade
    
[0m11:33:32.680197 [debug] [Thread-4 (]: SQL status: OK in 0.000 seconds
[0m11:33:32.681541 [debug] [Thread-4 (]: On model.job_intelligent.fact_job_offers: Close
[0m11:33:32.682471 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '784e203c-fcf8-4651-9396-5ca79c58d4e2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A50000D0D0>]}
[0m11:33:32.683166 [info ] [Thread-4 (]: 8 of 13 OK created sql table model main_gold.fact_job_offers ................... [[32mOK[0m in 0.14s]
[0m11:33:32.683771 [debug] [Thread-4 (]: Finished running node model.job_intelligent.fact_job_offers
[0m11:33:32.684517 [debug] [Thread-1 (]: Began running node model.job_intelligent.agg_location_analysis
[0m11:33:32.685136 [info ] [Thread-1 (]: 10 of 13 START sql table model main_gold.agg_location_analysis ................. [RUN]
[0m11:33:32.685743 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.job_intelligent.dim_location, now model.job_intelligent.agg_location_analysis)
[0m11:33:32.686257 [debug] [Thread-1 (]: Began compiling node model.job_intelligent.agg_location_analysis
[0m11:33:32.693534 [debug] [Thread-1 (]: Writing injected SQL for node "model.job_intelligent.agg_location_analysis"
[0m11:33:32.694070 [debug] [Thread-2 (]: Began running node model.job_intelligent.agg_job_offers_by_category_time
[0m11:33:32.694521 [info ] [Thread-2 (]: 9 of 13 START sql table model main_gold.agg_job_offers_by_category_time ........ [RUN]
[0m11:33:32.695028 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly model.job_intelligent.dim_time, now model.job_intelligent.agg_job_offers_by_category_time)
[0m11:33:32.695420 [debug] [Thread-2 (]: Began compiling node model.job_intelligent.agg_job_offers_by_category_time
[0m11:33:32.698065 [debug] [Thread-2 (]: Writing injected SQL for node "model.job_intelligent.agg_job_offers_by_category_time"
[0m11:33:32.699371 [debug] [Thread-2 (]: Began executing node model.job_intelligent.agg_job_offers_by_category_time
[0m11:33:32.699794 [debug] [Thread-1 (]: Began executing node model.job_intelligent.agg_location_analysis
[0m11:33:32.702500 [debug] [Thread-2 (]: Writing runtime sql for node "model.job_intelligent.agg_job_offers_by_category_time"
[0m11:33:32.705234 [debug] [Thread-1 (]: Writing runtime sql for node "model.job_intelligent.agg_location_analysis"
[0m11:33:32.707080 [debug] [Thread-2 (]: Using duckdb connection "model.job_intelligent.agg_job_offers_by_category_time"
[0m11:33:32.707636 [debug] [Thread-2 (]: On model.job_intelligent.agg_job_offers_by_category_time: BEGIN
[0m11:33:32.708122 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m11:33:32.708974 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.agg_location_analysis"
[0m11:33:32.709381 [debug] [Thread-1 (]: On model.job_intelligent.agg_location_analysis: BEGIN
[0m11:33:32.709731 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:33:32.710380 [debug] [Thread-2 (]: SQL status: OK in 0.002 seconds
[0m11:33:32.710764 [debug] [Thread-2 (]: Using duckdb connection "model.job_intelligent.agg_job_offers_by_category_time"
[0m11:33:32.711183 [debug] [Thread-2 (]: On model.job_intelligent.agg_job_offers_by_category_time: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.agg_job_offers_by_category_time"} */

  
    
    

    create  table
      "memory"."main_gold"."agg_job_offers_by_category_time__dbt_tmp"
  
    as (
      -- models/gold/agg_job_offers_by_category_time.sql
-- Gold layer: Aggregate - Job Offers by Category and Time



SELECT
    f.published_year,
    f.published_month,
    f.published_year_month,
    f.job_category,
    f.contract_type,
    f.work_type,
    
    COUNT(DISTINCT f.job_offer_id) as count_job_offers,
    COUNT(DISTINCT f.company_id) as count_companies,
    
    AVG(f.description_length) as avg_description_length,
    AVG(f.word_count) as avg_word_count,
    
    SUM(CASE WHEN f.is_remote = 1 THEN 1 ELSE 0 END) as remote_jobs,
    SUM(CASE WHEN f.is_permanent = 1 THEN 1 ELSE 0 END) as permanent_jobs,
    
    CURRENT_TIMESTAMP() as created_at
    
FROM "memory"."main_gold"."fact_job_offers" f
WHERE f.published_date IS NOT NULL
GROUP BY
    f.published_year,
    f.published_month,
    f.published_year_month,
    f.job_category,
    f.contract_type,
    f.work_type
ORDER BY f.published_year_month DESC, f.job_category
    );
  
  
[0m11:33:32.711909 [debug] [Thread-1 (]: SQL status: OK in 0.002 seconds
[0m11:33:32.712290 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.agg_location_analysis"
[0m11:33:32.712740 [debug] [Thread-1 (]: On model.job_intelligent.agg_location_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.agg_location_analysis"} */

  
    
    

    create  table
      "memory"."main_gold"."agg_location_analysis__dbt_tmp"
  
    as (
      -- models/gold/agg_location_analysis.sql
-- Gold layer: Aggregate - Location Analysis



SELECT
    dl.location_id,
    dl.location_raw,
    dl.city,
    dl.country,
    dl.work_location_type,
    
    COUNT(DISTINCT f.job_offer_id) as count_job_offers,
    COUNT(DISTINCT f.company_id) as count_companies,
    
    -- Distribution by job category
    COUNT(DISTINCT CASE WHEN f.job_category = 'Data Engineer' THEN f.job_offer_id END) as data_engineer_count,
    COUNT(DISTINCT CASE WHEN f.job_category = 'Data Scientist' THEN f.job_offer_id END) as data_scientist_count,
    COUNT(DISTINCT CASE WHEN f.job_category = 'Data Analyst' THEN f.job_offer_id END) as data_analyst_count,
    COUNT(DISTINCT CASE WHEN f.job_category = 'ML Engineer' THEN f.job_offer_id END) as ml_engineer_count,
    
    -- Remote percentage
    ROUND(
        100.0 * SUM(CASE WHEN f.is_remote = 1 THEN 1 ELSE 0 END) / COUNT(DISTINCT f.job_offer_id),
        2
    ) as pct_remote,
    
    CURRENT_TIMESTAMP() as created_at
    
FROM "memory"."main_gold"."dim_location" dl
LEFT JOIN "memory"."main_gold"."fact_job_offers" f ON dl.location_id = f.location_id
GROUP BY
    dl.location_id,
    dl.location_raw,
    dl.city,
    dl.country,
    dl.work_location_type
ORDER BY count_job_offers DESC
    );
  
  
[0m11:33:32.714553 [debug] [Thread-2 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.agg_job_offers_by_category_time"} */

  
    
    

    create  table
      "memory"."main_gold"."agg_job_offers_by_category_time__dbt_tmp"
  
    as (
      -- models/gold/agg_job_offers_by_category_time.sql
-- Gold layer: Aggregate - Job Offers by Category and Time



SELECT
    f.published_year,
    f.published_month,
    f.published_year_month,
    f.job_category,
    f.contract_type,
    f.work_type,
    
    COUNT(DISTINCT f.job_offer_id) as count_job_offers,
    COUNT(DISTINCT f.company_id) as count_companies,
    
    AVG(f.description_length) as avg_description_length,
    AVG(f.word_count) as avg_word_count,
    
    SUM(CASE WHEN f.is_remote = 1 THEN 1 ELSE 0 END) as remote_jobs,
    SUM(CASE WHEN f.is_permanent = 1 THEN 1 ELSE 0 END) as permanent_jobs,
    
    CURRENT_TIMESTAMP() as created_at
    
FROM "memory"."main_gold"."fact_job_offers" f
WHERE f.published_date IS NOT NULL
GROUP BY
    f.published_year,
    f.published_month,
    f.published_year_month,
    f.job_category,
    f.contract_type,
    f.work_type
ORDER BY f.published_year_month DESC, f.job_category
    );
  
  
[0m11:33:32.714992 [debug] [Thread-2 (]: DuckDB adapter: Rolling back transaction.
[0m11:33:32.715443 [debug] [Thread-2 (]: On model.job_intelligent.agg_job_offers_by_category_time: ROLLBACK
[0m11:33:32.716066 [debug] [Thread-1 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.agg_location_analysis"} */

  
    
    

    create  table
      "memory"."main_gold"."agg_location_analysis__dbt_tmp"
  
    as (
      -- models/gold/agg_location_analysis.sql
-- Gold layer: Aggregate - Location Analysis



SELECT
    dl.location_id,
    dl.location_raw,
    dl.city,
    dl.country,
    dl.work_location_type,
    
    COUNT(DISTINCT f.job_offer_id) as count_job_offers,
    COUNT(DISTINCT f.company_id) as count_companies,
    
    -- Distribution by job category
    COUNT(DISTINCT CASE WHEN f.job_category = 'Data Engineer' THEN f.job_offer_id END) as data_engineer_count,
    COUNT(DISTINCT CASE WHEN f.job_category = 'Data Scientist' THEN f.job_offer_id END) as data_scientist_count,
    COUNT(DISTINCT CASE WHEN f.job_category = 'Data Analyst' THEN f.job_offer_id END) as data_analyst_count,
    COUNT(DISTINCT CASE WHEN f.job_category = 'ML Engineer' THEN f.job_offer_id END) as ml_engineer_count,
    
    -- Remote percentage
    ROUND(
        100.0 * SUM(CASE WHEN f.is_remote = 1 THEN 1 ELSE 0 END) / COUNT(DISTINCT f.job_offer_id),
        2
    ) as pct_remote,
    
    CURRENT_TIMESTAMP() as created_at
    
FROM "memory"."main_gold"."dim_location" dl
LEFT JOIN "memory"."main_gold"."fact_job_offers" f ON dl.location_id = f.location_id
GROUP BY
    dl.location_id,
    dl.location_raw,
    dl.city,
    dl.country,
    dl.work_location_type
ORDER BY count_job_offers DESC
    );
  
  
[0m11:33:32.716509 [debug] [Thread-1 (]: DuckDB adapter: Rolling back transaction.
[0m11:33:32.716945 [debug] [Thread-1 (]: On model.job_intelligent.agg_location_analysis: ROLLBACK
[0m11:33:32.728291 [debug] [Thread-1 (]: Failed to rollback 'model.job_intelligent.agg_location_analysis'
[0m11:33:32.728704 [debug] [Thread-1 (]: On model.job_intelligent.agg_location_analysis: Close
[0m11:33:32.732280 [debug] [Thread-2 (]: Failed to rollback 'model.job_intelligent.agg_job_offers_by_category_time'
[0m11:33:32.732675 [debug] [Thread-2 (]: On model.job_intelligent.agg_job_offers_by_category_time: Close
[0m11:33:32.736509 [debug] [Thread-1 (]: Runtime Error in model agg_location_analysis (models\gold\agg_location_analysis.sql)
  Catalog Error: Scalar Function with name current_timestamp does not exist!
  Did you mean "current_localtimestamp"?
  
  LINE 38:     CURRENT_TIMESTAMP() as created_at
               ^
[0m11:33:32.736936 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '784e203c-fcf8-4651-9396-5ca79c58d4e2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A57A045E50>]}
[0m11:33:32.737497 [error] [Thread-1 (]: 10 of 13 ERROR creating sql table model main_gold.agg_location_analysis ........ [[31mERROR[0m in 0.05s]
[0m11:33:32.738099 [debug] [Thread-1 (]: Finished running node model.job_intelligent.agg_location_analysis
[0m11:33:32.738697 [debug] [Thread-7 (]: Marking all children of 'model.job_intelligent.agg_location_analysis' to be skipped because of status 'error'.  Reason: Runtime Error in model agg_location_analysis (models\gold\agg_location_analysis.sql)
  Catalog Error: Scalar Function with name current_timestamp does not exist!
  Did you mean "current_localtimestamp"?
  
  LINE 38:     CURRENT_TIMESTAMP() as created_at
               ^.
[0m11:33:32.742258 [debug] [Thread-2 (]: Runtime Error in model agg_job_offers_by_category_time (models\gold\agg_job_offers_by_category_time.sql)
  Catalog Error: Scalar Function with name current_timestamp does not exist!
  Did you mean "current_localtimestamp"?
  
  LINE 33:     CURRENT_TIMESTAMP() as created_at
               ^
[0m11:33:32.742691 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '784e203c-fcf8-4651-9396-5ca79c58d4e2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A57A045E50>]}
[0m11:33:32.743291 [error] [Thread-2 (]: 9 of 13 ERROR creating sql table model main_gold.agg_job_offers_by_category_time  [[31mERROR[0m in 0.05s]
[0m11:33:32.743913 [debug] [Thread-2 (]: Finished running node model.job_intelligent.agg_job_offers_by_category_time
[0m11:33:32.744444 [debug] [Thread-7 (]: Marking all children of 'model.job_intelligent.agg_job_offers_by_category_time' to be skipped because of status 'error'.  Reason: Runtime Error in model agg_job_offers_by_category_time (models\gold\agg_job_offers_by_category_time.sql)
  Catalog Error: Scalar Function with name current_timestamp does not exist!
  Did you mean "current_localtimestamp"?
  
  LINE 33:     CURRENT_TIMESTAMP() as created_at
               ^.
[0m11:33:34.306863 [debug] [Thread-3 (]: SQL status: OK in 1.824 seconds
[0m11:33:34.309365 [debug] [Thread-3 (]: Using duckdb connection "model.job_intelligent.int_skills_extraction"
[0m11:33:34.309711 [debug] [Thread-3 (]: On model.job_intelligent.int_skills_extraction: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.int_skills_extraction"} */
alter table "memory"."main_silver"."int_skills_extraction__dbt_tmp" rename to "int_skills_extraction"
[0m11:33:34.310381 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m11:33:34.311650 [debug] [Thread-3 (]: On model.job_intelligent.int_skills_extraction: COMMIT
[0m11:33:34.311987 [debug] [Thread-3 (]: Using duckdb connection "model.job_intelligent.int_skills_extraction"
[0m11:33:34.312344 [debug] [Thread-3 (]: On model.job_intelligent.int_skills_extraction: COMMIT
[0m11:33:34.312993 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m11:33:34.315182 [debug] [Thread-3 (]: Using duckdb connection "model.job_intelligent.int_skills_extraction"
[0m11:33:34.315519 [debug] [Thread-3 (]: On model.job_intelligent.int_skills_extraction: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.int_skills_extraction"} */

      drop table if exists "memory"."main_silver"."int_skills_extraction__dbt_backup" cascade
    
[0m11:33:34.316148 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m11:33:34.317246 [debug] [Thread-3 (]: On model.job_intelligent.int_skills_extraction: Close
[0m11:33:34.317699 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '784e203c-fcf8-4651-9396-5ca79c58d4e2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A57A61C230>]}
[0m11:33:34.318243 [info ] [Thread-3 (]: 7 of 13 OK created sql table model main_silver.int_skills_extraction ........... [[32mOK[0m in 1.88s]
[0m11:33:34.318799 [debug] [Thread-3 (]: Finished running node model.job_intelligent.int_skills_extraction
[0m11:33:34.319585 [debug] [Thread-4 (]: Began running node model.job_intelligent.dim_skills
[0m11:33:34.320010 [info ] [Thread-4 (]: 11 of 13 START sql table model main_gold.dim_skills ............................ [RUN]
[0m11:33:34.320444 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly model.job_intelligent.fact_job_offers, now model.job_intelligent.dim_skills)
[0m11:33:34.320807 [debug] [Thread-4 (]: Began compiling node model.job_intelligent.dim_skills
[0m11:33:34.323646 [debug] [Thread-4 (]: Writing injected SQL for node "model.job_intelligent.dim_skills"
[0m11:33:34.324570 [debug] [Thread-4 (]: Began executing node model.job_intelligent.dim_skills
[0m11:33:34.327350 [debug] [Thread-4 (]: Writing runtime sql for node "model.job_intelligent.dim_skills"
[0m11:33:34.328183 [debug] [Thread-4 (]: Using duckdb connection "model.job_intelligent.dim_skills"
[0m11:33:34.328541 [debug] [Thread-4 (]: On model.job_intelligent.dim_skills: BEGIN
[0m11:33:34.328844 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m11:33:34.329459 [debug] [Thread-4 (]: SQL status: OK in 0.001 seconds
[0m11:33:34.329780 [debug] [Thread-4 (]: Using duckdb connection "model.job_intelligent.dim_skills"
[0m11:33:34.330150 [debug] [Thread-4 (]: On model.job_intelligent.dim_skills: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_skills"} */

  
    
    

    create  table
      "memory"."main_gold"."dim_skills__dbt_tmp"
  
    as (
      -- models/gold/dim_skills.sql
-- Gold layer: Dimension Skills



WITH skills AS (
    SELECT DISTINCT
        skill_name
    FROM "memory"."main_silver"."int_skills_extraction"
    WHERE skill_name IS NOT NULL
),

skill_categorization AS (
    SELECT
        skill_name,
        CASE
            WHEN skill_name IN ('Python', 'Java', 'Scala', 'R') THEN 'Programming Language'
            WHEN skill_name IN ('SQL', 'NoSQL') THEN 'Database'
            WHEN skill_name IN ('Spark', 'Hadoop', 'Hive', 'Kafka') THEN 'Big Data Framework'
            WHEN skill_name IN ('TensorFlow', 'PyTorch', 'Scikit-learn') THEN 'ML/DL Library'
            WHEN skill_name IN ('AWS', 'Azure', 'GCP') THEN 'Cloud Platform'
            WHEN skill_name IN ('Tableau', 'Power BI', 'Looker') THEN 'BI Tool'
            WHEN skill_name IN ('Airflow', 'DBT', 'Kubernetes', 'Docker') THEN 'DataOps/DevOps'
            WHEN skill_name IN ('Pandas', 'NumPy', 'Matplotlib') THEN 'Data Analysis Library'
            WHEN skill_name IN ('Machine Learning', 'Statistics', 'Data Visualization') THEN 'Domain Knowledge'
            ELSE 'Other'
        END as skill_category
    FROM skills
),

ranked_skills AS (
    SELECT
        ROW_NUMBER() OVER (ORDER BY skill_name) as skill_id,
        skill_name,
        skill_category,
        NOW() as created_at
    FROM skill_categorization
)

SELECT
    skill_id,
    skill_name,
    skill_category,
    created_at
FROM ranked_skills
ORDER BY skill_id
    );
  
  
[0m11:33:34.336456 [debug] [Thread-4 (]: SQL status: OK in 0.006 seconds
[0m11:33:34.339564 [debug] [Thread-4 (]: Using duckdb connection "model.job_intelligent.dim_skills"
[0m11:33:34.339939 [debug] [Thread-4 (]: On model.job_intelligent.dim_skills: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_skills"} */
alter table "memory"."main_gold"."dim_skills__dbt_tmp" rename to "dim_skills"
[0m11:33:34.340690 [debug] [Thread-4 (]: SQL status: OK in 0.000 seconds
[0m11:33:34.342068 [debug] [Thread-4 (]: On model.job_intelligent.dim_skills: COMMIT
[0m11:33:34.342430 [debug] [Thread-4 (]: Using duckdb connection "model.job_intelligent.dim_skills"
[0m11:33:34.342765 [debug] [Thread-4 (]: On model.job_intelligent.dim_skills: COMMIT
[0m11:33:34.343386 [debug] [Thread-4 (]: SQL status: OK in 0.000 seconds
[0m11:33:34.345064 [debug] [Thread-4 (]: Using duckdb connection "model.job_intelligent.dim_skills"
[0m11:33:34.345391 [debug] [Thread-4 (]: On model.job_intelligent.dim_skills: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_skills"} */

      drop table if exists "memory"."main_gold"."dim_skills__dbt_backup" cascade
    
[0m11:33:34.345961 [debug] [Thread-4 (]: SQL status: OK in 0.000 seconds
[0m11:33:34.347036 [debug] [Thread-4 (]: On model.job_intelligent.dim_skills: Close
[0m11:33:34.347484 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '784e203c-fcf8-4651-9396-5ca79c58d4e2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A57A5DBD10>]}
[0m11:33:34.348010 [info ] [Thread-4 (]: 11 of 13 OK created sql table model main_gold.dim_skills ....................... [[32mOK[0m in 0.03s]
[0m11:33:34.348563 [debug] [Thread-4 (]: Finished running node model.job_intelligent.dim_skills
[0m11:33:34.349531 [debug] [Thread-1 (]: Began running node model.job_intelligent.fact_job_skills
[0m11:33:34.349956 [info ] [Thread-1 (]: 12 of 13 START sql table model main_gold.fact_job_skills ....................... [RUN]
[0m11:33:34.350373 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.job_intelligent.agg_location_analysis, now model.job_intelligent.fact_job_skills)
[0m11:33:34.350715 [debug] [Thread-1 (]: Began compiling node model.job_intelligent.fact_job_skills
[0m11:33:34.353741 [debug] [Thread-1 (]: Writing injected SQL for node "model.job_intelligent.fact_job_skills"
[0m11:33:34.354779 [debug] [Thread-1 (]: Began executing node model.job_intelligent.fact_job_skills
[0m11:33:34.358089 [debug] [Thread-1 (]: Writing runtime sql for node "model.job_intelligent.fact_job_skills"
[0m11:33:34.358993 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.fact_job_skills"
[0m11:33:34.359370 [debug] [Thread-1 (]: On model.job_intelligent.fact_job_skills: BEGIN
[0m11:33:34.359679 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:33:34.360349 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m11:33:34.360668 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.fact_job_skills"
[0m11:33:34.361022 [debug] [Thread-1 (]: On model.job_intelligent.fact_job_skills: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.fact_job_skills"} */

  
    
    

    create  table
      "memory"."main_gold"."fact_job_skills__dbt_tmp"
  
    as (
      -- models/gold/fact_job_skills.sql
-- Gold layer: Bridge Table - Job Skills



WITH skills_raw AS (
    SELECT DISTINCT
        job_url,
        company_name_cleaned,
        skill_name
    FROM "memory"."main_silver"."int_skills_extraction"
),

jobs AS (
    SELECT
        job_offer_id,
        job_url
    FROM "memory"."main_gold"."fact_job_offers"
),

skills_dim AS (
    SELECT
        skill_id,
        skill_name
    FROM "memory"."main_gold"."dim_skills"
),

fact_table AS (
    SELECT
        ROW_NUMBER() OVER (ORDER BY s.job_url, sd.skill_id) as job_skill_id,
        j.job_offer_id,
        sd.skill_id,
        s.skill_name,
        CURRENT_TIMESTAMP() as created_at
    FROM skills_raw s
    LEFT JOIN jobs j ON s.job_url = j.job_url
    LEFT JOIN skills_dim sd ON s.skill_name = sd.skill_name
)

SELECT
    job_skill_id,
    job_offer_id,
    skill_id,
    skill_name,
    created_at
FROM fact_table
WHERE job_offer_id IS NOT NULL
ORDER BY job_offer_id, skill_id
    );
  
  
[0m11:33:34.362683 [debug] [Thread-1 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.fact_job_skills"} */

  
    
    

    create  table
      "memory"."main_gold"."fact_job_skills__dbt_tmp"
  
    as (
      -- models/gold/fact_job_skills.sql
-- Gold layer: Bridge Table - Job Skills



WITH skills_raw AS (
    SELECT DISTINCT
        job_url,
        company_name_cleaned,
        skill_name
    FROM "memory"."main_silver"."int_skills_extraction"
),

jobs AS (
    SELECT
        job_offer_id,
        job_url
    FROM "memory"."main_gold"."fact_job_offers"
),

skills_dim AS (
    SELECT
        skill_id,
        skill_name
    FROM "memory"."main_gold"."dim_skills"
),

fact_table AS (
    SELECT
        ROW_NUMBER() OVER (ORDER BY s.job_url, sd.skill_id) as job_skill_id,
        j.job_offer_id,
        sd.skill_id,
        s.skill_name,
        CURRENT_TIMESTAMP() as created_at
    FROM skills_raw s
    LEFT JOIN jobs j ON s.job_url = j.job_url
    LEFT JOIN skills_dim sd ON s.skill_name = sd.skill_name
)

SELECT
    job_skill_id,
    job_offer_id,
    skill_id,
    skill_name,
    created_at
FROM fact_table
WHERE job_offer_id IS NOT NULL
ORDER BY job_offer_id, skill_id
    );
  
  
[0m11:33:34.363066 [debug] [Thread-1 (]: DuckDB adapter: Rolling back transaction.
[0m11:33:34.363460 [debug] [Thread-1 (]: On model.job_intelligent.fact_job_skills: ROLLBACK
[0m11:33:34.367716 [debug] [Thread-1 (]: Failed to rollback 'model.job_intelligent.fact_job_skills'
[0m11:33:34.368062 [debug] [Thread-1 (]: On model.job_intelligent.fact_job_skills: Close
[0m11:33:34.371702 [debug] [Thread-1 (]: Runtime Error in model fact_job_skills (models\gold\fact_job_skills.sql)
  Catalog Error: Scalar Function with name current_timestamp does not exist!
  Did you mean "current_localtimestamp"?
  
  LINE 44:         CURRENT_TIMESTAMP() as created_at
                   ^
[0m11:33:34.372122 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '784e203c-fcf8-4651-9396-5ca79c58d4e2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A500081610>]}
[0m11:33:34.372665 [error] [Thread-1 (]: 12 of 13 ERROR creating sql table model main_gold.fact_job_skills .............. [[31mERROR[0m in 0.02s]
[0m11:33:34.373227 [debug] [Thread-1 (]: Finished running node model.job_intelligent.fact_job_skills
[0m11:33:34.373803 [debug] [Thread-7 (]: Marking all children of 'model.job_intelligent.fact_job_skills' to be skipped because of status 'error'.  Reason: Runtime Error in model fact_job_skills (models\gold\fact_job_skills.sql)
  Catalog Error: Scalar Function with name current_timestamp does not exist!
  Did you mean "current_localtimestamp"?
  
  LINE 44:         CURRENT_TIMESTAMP() as created_at
                   ^.
[0m11:33:34.374534 [debug] [Thread-2 (]: Began running node model.job_intelligent.agg_skills_demand
[0m11:33:34.374937 [info ] [Thread-2 (]: 13 of 13 SKIP relation main_gold.agg_skills_demand ............................. [[33mSKIP[0m]
[0m11:33:34.375487 [debug] [Thread-2 (]: Finished running node model.job_intelligent.agg_skills_demand
[0m11:33:34.377038 [debug] [MainThread]: Using duckdb connection "master"
[0m11:33:34.377343 [debug] [MainThread]: On master: BEGIN
[0m11:33:34.377616 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m11:33:34.378215 [debug] [MainThread]: SQL status: OK in 0.001 seconds
[0m11:33:34.378508 [debug] [MainThread]: On master: COMMIT
[0m11:33:34.378793 [debug] [MainThread]: Using duckdb connection "master"
[0m11:33:34.379072 [debug] [MainThread]: On master: COMMIT
[0m11:33:34.379522 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m11:33:34.379816 [debug] [MainThread]: On master: Close
[0m11:33:34.380177 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:33:34.380442 [debug] [MainThread]: Connection 'create_memory_main_silver' was properly closed.
[0m11:33:34.380703 [debug] [MainThread]: Connection 'create_memory_main_bronze' was properly closed.
[0m11:33:34.380994 [debug] [MainThread]: Connection 'create_memory_main_gold' was properly closed.
[0m11:33:34.381247 [debug] [MainThread]: Connection 'list_memory_main_bronze' was properly closed.
[0m11:33:34.381497 [debug] [MainThread]: Connection 'list_memory_main_gold' was properly closed.
[0m11:33:34.381743 [debug] [MainThread]: Connection 'list_memory_main_silver' was properly closed.
[0m11:33:34.381999 [debug] [MainThread]: Connection 'model.job_intelligent.fact_job_skills' was properly closed.
[0m11:33:34.382246 [debug] [MainThread]: Connection 'model.job_intelligent.agg_job_offers_by_category_time' was properly closed.
[0m11:33:34.382494 [debug] [MainThread]: Connection 'model.job_intelligent.int_skills_extraction' was properly closed.
[0m11:33:34.382738 [debug] [MainThread]: Connection 'model.job_intelligent.dim_skills' was properly closed.
[0m11:33:34.383117 [info ] [MainThread]: 
[0m11:33:34.383454 [info ] [MainThread]: Finished running 12 table models, 1 view model in 0 hours 0 minutes and 2.55 seconds (2.55s).
[0m11:33:34.385551 [debug] [MainThread]: Command end result
[0m11:33:34.404462 [debug] [MainThread]: Wrote artifact WritableManifest to D:\lab2\dbt_project\target\manifest.json
[0m11:33:34.407443 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\lab2\dbt_project\target\semantic_manifest.json
[0m11:33:34.414172 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\lab2\dbt_project\target\run_results.json
[0m11:33:34.414524 [info ] [MainThread]: 
[0m11:33:34.414904 [info ] [MainThread]: [31mCompleted with 3 errors, 0 partial successes, and 0 warnings:[0m
[0m11:33:34.415244 [info ] [MainThread]: 
[0m11:33:34.415670 [error] [MainThread]: [31mFailure in model agg_location_analysis (models\gold\agg_location_analysis.sql)[0m
[0m11:33:34.416075 [error] [MainThread]:   Runtime Error in model agg_location_analysis (models\gold\agg_location_analysis.sql)
  Catalog Error: Scalar Function with name current_timestamp does not exist!
  Did you mean "current_localtimestamp"?
  
  LINE 38:     CURRENT_TIMESTAMP() as created_at
               ^
[0m11:33:34.416374 [info ] [MainThread]: 
[0m11:33:34.416716 [info ] [MainThread]:   compiled code at target\compiled\job_intelligent\models\gold\agg_location_analysis.sql
[0m11:33:34.417035 [info ] [MainThread]: 
[0m11:33:34.417419 [error] [MainThread]: [31mFailure in model agg_job_offers_by_category_time (models\gold\agg_job_offers_by_category_time.sql)[0m
[0m11:33:34.417788 [error] [MainThread]:   Runtime Error in model agg_job_offers_by_category_time (models\gold\agg_job_offers_by_category_time.sql)
  Catalog Error: Scalar Function with name current_timestamp does not exist!
  Did you mean "current_localtimestamp"?
  
  LINE 33:     CURRENT_TIMESTAMP() as created_at
               ^
[0m11:33:34.418087 [info ] [MainThread]: 
[0m11:33:34.418449 [info ] [MainThread]:   compiled code at target\compiled\job_intelligent\models\gold\agg_job_offers_by_category_time.sql
[0m11:33:34.418804 [info ] [MainThread]: 
[0m11:33:34.419150 [error] [MainThread]: [31mFailure in model fact_job_skills (models\gold\fact_job_skills.sql)[0m
[0m11:33:34.419510 [error] [MainThread]:   Runtime Error in model fact_job_skills (models\gold\fact_job_skills.sql)
  Catalog Error: Scalar Function with name current_timestamp does not exist!
  Did you mean "current_localtimestamp"?
  
  LINE 44:         CURRENT_TIMESTAMP() as created_at
                   ^
[0m11:33:34.419820 [info ] [MainThread]: 
[0m11:33:34.420185 [info ] [MainThread]:   compiled code at target\compiled\job_intelligent\models\gold\fact_job_skills.sql
[0m11:33:34.420476 [info ] [MainThread]: 
[0m11:33:34.420793 [info ] [MainThread]: Done. PASS=9 WARN=0 ERROR=3 SKIP=1 NO-OP=0 TOTAL=13
[0m11:33:34.421762 [debug] [MainThread]: Command `dbt run` failed at 11:33:34.421582 after 4.16 seconds
[0m11:33:34.422427 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A577C69010>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A5770AF6B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A50000C0B0>]}
[0m11:33:34.422971 [debug] [MainThread]: Flushing usage events
[0m11:33:44.626857 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:34:10.375127 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020EB4B5DFD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020EB2904190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020EB2787890>]}


============================== 11:34:10.385805 | 6b81b1d3-1bf4-4e1f-a930-48c85c27ee03 ==============================
[0m11:34:10.385805 [info ] [MainThread]: Running with dbt=1.10.13
[0m11:34:10.387383 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'use_experimental_parser': 'False', 'cache_selected_only': 'False', 'warn_error': 'None', 'profiles_dir': 'D:\\lab2\\dbt_project', 'debug': 'False', 'static_parser': 'True', 'log_cache_events': 'False', 'empty': 'False', 'quiet': 'False', 'log_format': 'default', 'use_colors': 'True', 'partial_parse': 'True', 'version_check': 'True', 'log_path': 'D:\\lab2\\dbt_project\\logs', 'introspect': 'True', 'no_print': 'None', 'fail_fast': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'invocation_command': 'dbt run', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'target_path': 'None', 'write_json': 'True'}
[0m11:34:10.742301 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '6b81b1d3-1bf4-4e1f-a930-48c85c27ee03', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020EB501F230>]}
[0m11:34:10.824138 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '6b81b1d3-1bf4-4e1f-a930-48c85c27ee03', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020EB4BD8380>]}
[0m11:34:10.826924 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m11:34:11.089864 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m11:34:11.247419 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m11:34:11.247814 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m11:34:11.254207 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- seeds
[0m11:34:11.273868 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '6b81b1d3-1bf4-4e1f-a930-48c85c27ee03', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020EB5E54850>]}
[0m11:34:11.329206 [debug] [MainThread]: Wrote artifact WritableManifest to D:\lab2\dbt_project\target\manifest.json
[0m11:34:11.331498 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\lab2\dbt_project\target\semantic_manifest.json
[0m11:34:11.366633 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '6b81b1d3-1bf4-4e1f-a930-48c85c27ee03', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020EB5B5B7A0>]}
[0m11:34:11.367160 [info ] [MainThread]: Found 13 models, 456 macros
[0m11:34:11.367582 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6b81b1d3-1bf4-4e1f-a930-48c85c27ee03', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020EB73D5010>]}
[0m11:34:11.370059 [info ] [MainThread]: 
[0m11:34:11.370588 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m11:34:11.370962 [info ] [MainThread]: 
[0m11:34:11.371512 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m11:34:11.378242 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_memory'
[0m11:34:11.395470 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_memory'
[0m11:34:11.399598 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_memory'
[0m11:34:11.544140 [debug] [ThreadPool]: Using duckdb connection "list_memory"
[0m11:34:11.544838 [debug] [ThreadPool]: Using duckdb connection "list_memory"
[0m11:34:11.545505 [debug] [ThreadPool]: On list_memory: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_memory"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"memory"'
    
  
  
[0m11:34:11.546083 [debug] [ThreadPool]: Using duckdb connection "list_memory"
[0m11:34:11.546718 [debug] [ThreadPool]: On list_memory: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_memory"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"memory"'
    
  
  
[0m11:34:11.547411 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:34:11.548133 [debug] [ThreadPool]: On list_memory: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_memory"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"memory"'
    
  
  
[0m11:34:11.548973 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:34:11.550145 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:34:11.573582 [debug] [ThreadPool]: SQL status: OK in 0.023 seconds
[0m11:34:11.574074 [debug] [ThreadPool]: SQL status: OK in 0.025 seconds
[0m11:34:11.575675 [debug] [ThreadPool]: On list_memory: Close
[0m11:34:11.576350 [debug] [ThreadPool]: SQL status: OK in 0.029 seconds
[0m11:34:11.577603 [debug] [ThreadPool]: On list_memory: Close
[0m11:34:11.579585 [debug] [ThreadPool]: On list_memory: Close
[0m11:34:11.580473 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_memory, now create_memory_main_gold)
[0m11:34:11.581014 [debug] [ThreadPool]: Creating schema "database: "memory"
schema: "main_gold"
"
[0m11:34:11.588030 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_gold"
[0m11:34:11.588494 [debug] [ThreadPool]: On create_memory_main_gold: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_memory_main_gold"} */

    
        select type from duckdb_databases()
        where lower(database_name)='memory'
        and type='sqlite'
    
  
[0m11:34:11.588851 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:34:11.589567 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_memory, now create_memory_main_bronze)
[0m11:34:11.590153 [debug] [ThreadPool]: Creating schema "database: "memory"
schema: "main_bronze"
"
[0m11:34:11.592526 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_bronze"
[0m11:34:11.592959 [debug] [ThreadPool]: On create_memory_main_bronze: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_memory_main_bronze"} */

    
        select type from duckdb_databases()
        where lower(database_name)='memory'
        and type='sqlite'
    
  
[0m11:34:11.593440 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_memory, now create_memory_main_silver)
[0m11:34:11.593849 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:34:11.594361 [debug] [ThreadPool]: SQL status: OK in 0.005 seconds
[0m11:34:11.594914 [debug] [ThreadPool]: Creating schema "database: "memory"
schema: "main_silver"
"
[0m11:34:11.596669 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_gold"
[0m11:34:11.598664 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_silver"
[0m11:34:11.599266 [debug] [ThreadPool]: SQL status: OK in 0.005 seconds
[0m11:34:11.599774 [debug] [ThreadPool]: On create_memory_main_gold: BEGIN
[0m11:34:11.600211 [debug] [ThreadPool]: On create_memory_main_silver: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_memory_main_silver"} */

    
        select type from duckdb_databases()
        where lower(database_name)='memory'
        and type='sqlite'
    
  
[0m11:34:11.601875 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_bronze"
[0m11:34:11.602464 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:34:11.602993 [debug] [ThreadPool]: SQL status: OK in 0.002 seconds
[0m11:34:11.603413 [debug] [ThreadPool]: On create_memory_main_bronze: BEGIN
[0m11:34:11.604230 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_gold"
[0m11:34:11.604865 [debug] [ThreadPool]: On create_memory_main_gold: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_memory_main_gold"} */

    
    
        create schema if not exists "memory"."main_gold"
    
[0m11:34:11.605454 [debug] [ThreadPool]: SQL status: OK in 0.003 seconds
[0m11:34:11.606936 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_silver"
[0m11:34:11.607333 [debug] [ThreadPool]: On create_memory_main_silver: BEGIN
[0m11:34:11.607863 [debug] [ThreadPool]: SQL status: OK in 0.003 seconds
[0m11:34:11.608216 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_bronze"
[0m11:34:11.608565 [debug] [ThreadPool]: On create_memory_main_bronze: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_memory_main_bronze"} */

    
    
        create schema if not exists "memory"."main_bronze"
    
[0m11:34:11.609001 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m11:34:11.609359 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_silver"
[0m11:34:11.609699 [debug] [ThreadPool]: On create_memory_main_silver: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_memory_main_silver"} */

    
    
        create schema if not exists "memory"."main_silver"
    
[0m11:34:11.610150 [debug] [ThreadPool]: SQL status: OK in 0.005 seconds
[0m11:34:11.611076 [debug] [ThreadPool]: On create_memory_main_gold: COMMIT
[0m11:34:11.611491 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_gold"
[0m11:34:11.611863 [debug] [ThreadPool]: On create_memory_main_gold: COMMIT
[0m11:34:11.612341 [debug] [ThreadPool]: SQL status: OK in 0.003 seconds
[0m11:34:11.613253 [debug] [ThreadPool]: On create_memory_main_bronze: COMMIT
[0m11:34:11.613613 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_bronze"
[0m11:34:11.613957 [debug] [ThreadPool]: On create_memory_main_bronze: COMMIT
[0m11:34:11.614430 [debug] [ThreadPool]: SQL status: OK in 0.004 seconds
[0m11:34:11.615490 [debug] [ThreadPool]: SQL status: OK in 0.003 seconds
[0m11:34:11.615869 [debug] [ThreadPool]: On create_memory_main_gold: Close
[0m11:34:11.616211 [debug] [ThreadPool]: On create_memory_main_silver: COMMIT
[0m11:34:11.616598 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_silver"
[0m11:34:11.616942 [debug] [ThreadPool]: On create_memory_main_silver: COMMIT
[0m11:34:11.617712 [debug] [ThreadPool]: SQL status: OK in 0.003 seconds
[0m11:34:11.618075 [debug] [ThreadPool]: On create_memory_main_bronze: Close
[0m11:34:11.619097 [debug] [ThreadPool]: SQL status: OK in 0.002 seconds
[0m11:34:11.620059 [debug] [ThreadPool]: On create_memory_main_silver: Close
[0m11:34:11.625421 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_memory_main_bronze'
[0m11:34:11.638961 [debug] [ThreadPool]: Using duckdb connection "list_memory_main_bronze"
[0m11:34:11.640241 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_memory_main_silver'
[0m11:34:11.641133 [debug] [ThreadPool]: On list_memory_main_bronze: BEGIN
[0m11:34:11.646376 [debug] [ThreadPool]: Using duckdb connection "list_memory_main_silver"
[0m11:34:11.647302 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:34:11.648533 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_memory_main_gold'
[0m11:34:11.649717 [debug] [ThreadPool]: On list_memory_main_silver: BEGIN
[0m11:34:11.654507 [debug] [ThreadPool]: Using duckdb connection "list_memory_main_gold"
[0m11:34:11.655491 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:34:11.656468 [debug] [ThreadPool]: On list_memory_main_gold: BEGIN
[0m11:34:11.657336 [debug] [ThreadPool]: SQL status: OK in 0.010 seconds
[0m11:34:11.658771 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:34:11.659790 [debug] [ThreadPool]: Using duckdb connection "list_memory_main_bronze"
[0m11:34:11.660640 [debug] [ThreadPool]: SQL status: OK in 0.005 seconds
[0m11:34:11.662174 [debug] [ThreadPool]: On list_memory_main_bronze: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_memory_main_bronze"} */
select
      'memory' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main_bronze'
    and lower(table_catalog) = 'memory'
  
[0m11:34:11.663927 [debug] [ThreadPool]: Using duckdb connection "list_memory_main_silver"
[0m11:34:11.663226 [debug] [ThreadPool]: SQL status: OK in 0.004 seconds
[0m11:34:11.665739 [debug] [ThreadPool]: On list_memory_main_silver: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_memory_main_silver"} */
select
      'memory' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main_silver'
    and lower(table_catalog) = 'memory'
  
[0m11:34:11.666768 [debug] [ThreadPool]: Using duckdb connection "list_memory_main_gold"
[0m11:34:11.668181 [debug] [ThreadPool]: On list_memory_main_gold: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_memory_main_gold"} */
select
      'memory' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main_gold'
    and lower(table_catalog) = 'memory'
  
[0m11:34:11.699597 [debug] [ThreadPool]: SQL status: OK in 0.032 seconds
[0m11:34:11.700200 [debug] [ThreadPool]: SQL status: OK in 0.031 seconds
[0m11:34:11.701665 [debug] [ThreadPool]: On list_memory_main_gold: ROLLBACK
[0m11:34:11.703388 [debug] [ThreadPool]: On list_memory_main_silver: ROLLBACK
[0m11:34:11.703894 [debug] [ThreadPool]: SQL status: OK in 0.039 seconds
[0m11:34:11.705642 [debug] [ThreadPool]: On list_memory_main_bronze: ROLLBACK
[0m11:34:11.707747 [debug] [ThreadPool]: Failed to rollback 'list_memory_main_silver'
[0m11:34:11.708113 [debug] [ThreadPool]: On list_memory_main_silver: Close
[0m11:34:11.709467 [debug] [ThreadPool]: Failed to rollback 'list_memory_main_gold'
[0m11:34:11.709824 [debug] [ThreadPool]: On list_memory_main_gold: Close
[0m11:34:11.710988 [debug] [ThreadPool]: Failed to rollback 'list_memory_main_bronze'
[0m11:34:11.711320 [debug] [ThreadPool]: On list_memory_main_bronze: Close
[0m11:34:11.712304 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6b81b1d3-1bf4-4e1f-a930-48c85c27ee03', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020EB75C3A00>]}
[0m11:34:11.712766 [debug] [MainThread]: Using duckdb connection "master"
[0m11:34:11.713085 [debug] [MainThread]: On master: BEGIN
[0m11:34:11.713385 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:34:11.714013 [debug] [MainThread]: SQL status: OK in 0.001 seconds
[0m11:34:11.714336 [debug] [MainThread]: On master: COMMIT
[0m11:34:11.714681 [debug] [MainThread]: Using duckdb connection "master"
[0m11:34:11.714982 [debug] [MainThread]: On master: COMMIT
[0m11:34:11.715491 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m11:34:11.715818 [debug] [MainThread]: On master: Close
[0m11:34:11.721930 [debug] [Thread-1 (]: Began running node model.job_intelligent.stg_jobs_raw
[0m11:34:11.722579 [info ] [Thread-1 (]: 1 of 13 START sql view model main_bronze.stg_jobs_raw .......................... [RUN]
[0m11:34:11.723211 [debug] [Thread-1 (]: Acquiring new duckdb connection 'model.job_intelligent.stg_jobs_raw'
[0m11:34:11.723641 [debug] [Thread-1 (]: Began compiling node model.job_intelligent.stg_jobs_raw
[0m11:34:11.731963 [debug] [Thread-1 (]: Writing injected SQL for node "model.job_intelligent.stg_jobs_raw"
[0m11:34:11.732817 [debug] [Thread-1 (]: Began executing node model.job_intelligent.stg_jobs_raw
[0m11:34:11.766489 [debug] [Thread-1 (]: Writing runtime sql for node "model.job_intelligent.stg_jobs_raw"
[0m11:34:11.767478 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.stg_jobs_raw"
[0m11:34:11.767890 [debug] [Thread-1 (]: On model.job_intelligent.stg_jobs_raw: BEGIN
[0m11:34:11.768251 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m11:34:11.769209 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m11:34:11.769694 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.stg_jobs_raw"
[0m11:34:11.770189 [debug] [Thread-1 (]: On model.job_intelligent.stg_jobs_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.stg_jobs_raw"} */

  
  create view "memory"."main_bronze"."stg_jobs_raw__dbt_tmp" as (
    -- models/bronze/stg_jobs_raw.sql
-- Bronze layer: Lecture directe des données brutes depuis final_data.csv
-- Renommage et typage minimal



-- Read directly from CSV file
SELECT
    -- Renommer les colonnes pour cohérence
    title as job_title,
    location,
    postedTime as posted_time,
    publishedAt as published_at,
    jobUrl as job_url,
    companyName as company_name,
    companyUrl as company_url,
    description as job_description,
    contractType as contract_type,
    workType as work_type,
    
    -- Métadonnées de traçabilité
    NOW() as ingestion_timestamp,
    '2026-01-08 10:34:10.247190+00:00' as dbt_run_id

FROM read_csv_auto('D:/lab2/final_data.csv')

WHERE 1=1
    -- Filtrer les lignes vides
    AND title IS NOT NULL
    AND description IS NOT NULL
  );

[0m11:34:11.819889 [debug] [Thread-1 (]: SQL status: OK in 0.049 seconds
[0m11:34:11.825983 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.stg_jobs_raw"
[0m11:34:11.826411 [debug] [Thread-1 (]: On model.job_intelligent.stg_jobs_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.stg_jobs_raw"} */
alter view "memory"."main_bronze"."stg_jobs_raw__dbt_tmp" rename to "stg_jobs_raw"
[0m11:34:11.827217 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m11:34:11.837142 [debug] [Thread-1 (]: On model.job_intelligent.stg_jobs_raw: COMMIT
[0m11:34:11.837616 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.stg_jobs_raw"
[0m11:34:11.837981 [debug] [Thread-1 (]: On model.job_intelligent.stg_jobs_raw: COMMIT
[0m11:34:11.838687 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m11:34:11.844166 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.stg_jobs_raw"
[0m11:34:11.844531 [debug] [Thread-1 (]: On model.job_intelligent.stg_jobs_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.stg_jobs_raw"} */

      drop view if exists "memory"."main_bronze"."stg_jobs_raw__dbt_backup" cascade
    
[0m11:34:11.845152 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m11:34:11.847298 [debug] [Thread-1 (]: On model.job_intelligent.stg_jobs_raw: Close
[0m11:34:11.849664 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6b81b1d3-1bf4-4e1f-a930-48c85c27ee03', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020EB5E7DB50>]}
[0m11:34:11.850272 [info ] [Thread-1 (]: 1 of 13 OK created sql view model main_bronze.stg_jobs_raw ..................... [[32mOK[0m in 0.12s]
[0m11:34:11.850874 [debug] [Thread-1 (]: Finished running node model.job_intelligent.stg_jobs_raw
[0m11:34:11.851727 [debug] [Thread-2 (]: Began running node model.job_intelligent.int_jobs_cleaned
[0m11:34:11.852190 [info ] [Thread-2 (]: 2 of 13 START sql table model main_silver.int_jobs_cleaned ..................... [RUN]
[0m11:34:11.853144 [debug] [Thread-2 (]: Acquiring new duckdb connection 'model.job_intelligent.int_jobs_cleaned'
[0m11:34:11.853615 [debug] [Thread-2 (]: Began compiling node model.job_intelligent.int_jobs_cleaned
[0m11:34:11.856780 [debug] [Thread-2 (]: Writing injected SQL for node "model.job_intelligent.int_jobs_cleaned"
[0m11:34:11.857598 [debug] [Thread-2 (]: Began executing node model.job_intelligent.int_jobs_cleaned
[0m11:34:11.877940 [debug] [Thread-2 (]: Writing runtime sql for node "model.job_intelligent.int_jobs_cleaned"
[0m11:34:11.879114 [debug] [Thread-2 (]: Using duckdb connection "model.job_intelligent.int_jobs_cleaned"
[0m11:34:11.879731 [debug] [Thread-2 (]: On model.job_intelligent.int_jobs_cleaned: BEGIN
[0m11:34:11.880131 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m11:34:11.880878 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m11:34:11.881277 [debug] [Thread-2 (]: Using duckdb connection "model.job_intelligent.int_jobs_cleaned"
[0m11:34:11.881743 [debug] [Thread-2 (]: On model.job_intelligent.int_jobs_cleaned: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.int_jobs_cleaned"} */

  
    
    

    create  table
      "memory"."main_silver"."int_jobs_cleaned__dbt_tmp"
  
    as (
      -- models/silver/int_jobs_cleaned.sql
-- Silver layer: Nettoyage et normalisation des données brutes



WITH raw_jobs AS (
    SELECT * FROM "memory"."main_bronze"."stg_jobs_raw"
),

cleaned_jobs AS (
    SELECT
        -- Texte: lowercase, trim, remove special characters
        LOWER(TRIM(job_title)) as job_title_cleaned,
        LOWER(TRIM(location)) as location_cleaned,
        LOWER(TRIM(company_name)) as company_name_cleaned,
        LOWER(TRIM(job_description)) as job_description_cleaned,
        LOWER(TRIM(contract_type)) as contract_type_cleaned,
        LOWER(TRIM(work_type)) as work_type_cleaned,
        
        -- URLs as-is
        job_url,
        company_url,
        
        -- Dates
        TRY_CAST(published_at AS DATE) as published_date,
        posted_time,
        
        -- Extract year-month for time-based analysis
        DATE_TRUNC('month', TRY_CAST(published_at AS DATE)) as published_year_month,
        EXTRACT(YEAR FROM TRY_CAST(published_at AS DATE)) as published_year,
        EXTRACT(MONTH FROM TRY_CAST(published_at AS DATE)) as published_month,
        
        -- Métadonnées
        ingestion_timestamp
    FROM raw_jobs
)

SELECT
    *,
    -- Deduplication flag
    ROW_NUMBER() OVER (
        PARTITION BY job_title_cleaned, company_name_cleaned, location_cleaned 
        ORDER BY published_date DESC
    ) as dedup_rank
FROM cleaned_jobs
    );
  
  
[0m11:34:12.017577 [debug] [Thread-2 (]: SQL status: OK in 0.135 seconds
[0m11:34:12.020847 [debug] [Thread-2 (]: Using duckdb connection "model.job_intelligent.int_jobs_cleaned"
[0m11:34:12.021336 [debug] [Thread-2 (]: On model.job_intelligent.int_jobs_cleaned: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.int_jobs_cleaned"} */
alter table "memory"."main_silver"."int_jobs_cleaned__dbt_tmp" rename to "int_jobs_cleaned"
[0m11:34:12.022206 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m11:34:12.027367 [debug] [Thread-2 (]: On model.job_intelligent.int_jobs_cleaned: COMMIT
[0m11:34:12.027809 [debug] [Thread-2 (]: Using duckdb connection "model.job_intelligent.int_jobs_cleaned"
[0m11:34:12.028204 [debug] [Thread-2 (]: On model.job_intelligent.int_jobs_cleaned: COMMIT
[0m11:34:12.032627 [debug] [Thread-2 (]: SQL status: OK in 0.004 seconds
[0m11:34:12.034905 [debug] [Thread-2 (]: Using duckdb connection "model.job_intelligent.int_jobs_cleaned"
[0m11:34:12.035327 [debug] [Thread-2 (]: On model.job_intelligent.int_jobs_cleaned: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.int_jobs_cleaned"} */

      drop table if exists "memory"."main_silver"."int_jobs_cleaned__dbt_backup" cascade
    
[0m11:34:12.036156 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m11:34:12.037906 [debug] [Thread-2 (]: On model.job_intelligent.int_jobs_cleaned: Close
[0m11:34:12.038546 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6b81b1d3-1bf4-4e1f-a930-48c85c27ee03', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020EB73ED230>]}
[0m11:34:12.039213 [info ] [Thread-2 (]: 2 of 13 OK created sql table model main_silver.int_jobs_cleaned ................ [[32mOK[0m in 0.19s]
[0m11:34:12.039931 [debug] [Thread-2 (]: Finished running node model.job_intelligent.int_jobs_cleaned
[0m11:34:12.040916 [debug] [Thread-3 (]: Began running node model.job_intelligent.int_job_title_normalization
[0m11:34:12.041466 [info ] [Thread-3 (]: 3 of 13 START sql table model main_silver.int_job_title_normalization .......... [RUN]
[0m11:34:12.042079 [debug] [Thread-3 (]: Acquiring new duckdb connection 'model.job_intelligent.int_job_title_normalization'
[0m11:34:12.042496 [debug] [Thread-3 (]: Began compiling node model.job_intelligent.int_job_title_normalization
[0m11:34:12.047039 [debug] [Thread-3 (]: Writing injected SQL for node "model.job_intelligent.int_job_title_normalization"
[0m11:34:12.048165 [debug] [Thread-3 (]: Began executing node model.job_intelligent.int_job_title_normalization
[0m11:34:12.051769 [debug] [Thread-3 (]: Writing runtime sql for node "model.job_intelligent.int_job_title_normalization"
[0m11:34:12.053204 [debug] [Thread-3 (]: Using duckdb connection "model.job_intelligent.int_job_title_normalization"
[0m11:34:12.054133 [debug] [Thread-3 (]: On model.job_intelligent.int_job_title_normalization: BEGIN
[0m11:34:12.054858 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m11:34:12.056011 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m11:34:12.056459 [debug] [Thread-3 (]: Using duckdb connection "model.job_intelligent.int_job_title_normalization"
[0m11:34:12.057036 [debug] [Thread-3 (]: On model.job_intelligent.int_job_title_normalization: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.int_job_title_normalization"} */

  
    
    

    create  table
      "memory"."main_silver"."int_job_title_normalization__dbt_tmp"
  
    as (
      -- models/silver/int_job_title_normalization.sql
-- Silver layer: Normaliser les intitulés de postes



WITH cleaned_jobs AS (
    SELECT * FROM "memory"."main_silver"."int_jobs_cleaned"
),

title_normalized AS (
    SELECT
        *,
        CASE
            WHEN job_title_cleaned LIKE '%data engineer%' THEN 'Data Engineer'
            WHEN job_title_cleaned LIKE '%data scientist%' THEN 'Data Scientist'
            WHEN job_title_cleaned LIKE '%data analyst%' THEN 'Data Analyst'
            WHEN job_title_cleaned LIKE '%analytics engineer%' THEN 'Analytics Engineer'
            WHEN job_title_cleaned LIKE '%ml engineer%' OR job_title_cleaned LIKE '%machine learning%' THEN 'ML Engineer'
            WHEN job_title_cleaned LIKE '%data architect%' THEN 'Data Architect'
            WHEN job_title_cleaned LIKE '%bi developer%' OR job_title_cleaned LIKE '%business intelligence%' THEN 'BI Developer'
            WHEN job_title_cleaned LIKE '%etl%' OR job_title_cleaned LIKE '%pipeline%' THEN 'ETL/Pipeline Engineer'
            WHEN job_title_cleaned LIKE '%data engineer%' THEN 'Data Engineer'
            ELSE 'Other Data Role'
        END as job_category,
        
        CASE
            WHEN contract_type_cleaned LIKE '%cdi%' OR contract_type_cleaned LIKE '%permanent%' THEN 'Permanent'
            WHEN contract_type_cleaned LIKE '%cdd%' OR contract_type_cleaned LIKE '%contract%' THEN 'Contract'
            WHEN contract_type_cleaned LIKE '%stage%' OR contract_type_cleaned LIKE '%internship%' THEN 'Internship'
            WHEN contract_type_cleaned LIKE '%freelance%' THEN 'Freelance'
            ELSE 'Not Specified'
        END as contract_type_normalized,
        
        CASE
            WHEN work_type_cleaned LIKE '%remote%' THEN 'Remote'
            WHEN work_type_cleaned LIKE '%hybrid%' THEN 'Hybrid'
            WHEN work_type_cleaned LIKE '%onsite%' OR work_type_cleaned LIKE '%on-site%' THEN 'On-site'
            ELSE 'Not Specified'
        END as work_type_normalized
    
    FROM cleaned_jobs
    WHERE dedup_rank = 1  -- Keep only first occurrence (most recent)
)

SELECT * FROM title_normalized
    );
  
  
[0m11:34:12.090307 [debug] [Thread-3 (]: SQL status: OK in 0.033 seconds
[0m11:34:12.093308 [debug] [Thread-3 (]: Using duckdb connection "model.job_intelligent.int_job_title_normalization"
[0m11:34:12.093744 [debug] [Thread-3 (]: On model.job_intelligent.int_job_title_normalization: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.int_job_title_normalization"} */
alter table "memory"."main_silver"."int_job_title_normalization__dbt_tmp" rename to "int_job_title_normalization"
[0m11:34:12.094865 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m11:34:12.096435 [debug] [Thread-3 (]: On model.job_intelligent.int_job_title_normalization: COMMIT
[0m11:34:12.096853 [debug] [Thread-3 (]: Using duckdb connection "model.job_intelligent.int_job_title_normalization"
[0m11:34:12.097248 [debug] [Thread-3 (]: On model.job_intelligent.int_job_title_normalization: COMMIT
[0m11:34:12.097956 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m11:34:12.100061 [debug] [Thread-3 (]: Using duckdb connection "model.job_intelligent.int_job_title_normalization"
[0m11:34:12.100503 [debug] [Thread-3 (]: On model.job_intelligent.int_job_title_normalization: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.int_job_title_normalization"} */

      drop table if exists "memory"."main_silver"."int_job_title_normalization__dbt_backup" cascade
    
[0m11:34:12.101251 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m11:34:12.102946 [debug] [Thread-3 (]: On model.job_intelligent.int_job_title_normalization: Close
[0m11:34:12.103811 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6b81b1d3-1bf4-4e1f-a930-48c85c27ee03', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020EB8848FF0>]}
[0m11:34:12.104508 [info ] [Thread-3 (]: 3 of 13 OK created sql table model main_silver.int_job_title_normalization ..... [[32mOK[0m in 0.06s]
[0m11:34:12.105211 [debug] [Thread-3 (]: Finished running node model.job_intelligent.int_job_title_normalization
[0m11:34:12.106295 [debug] [Thread-1 (]: Began running node model.job_intelligent.dim_location
[0m11:34:12.106839 [info ] [Thread-1 (]: 5 of 13 START sql table model main_gold.dim_location ........................... [RUN]
[0m11:34:12.107406 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.job_intelligent.stg_jobs_raw, now model.job_intelligent.dim_location)
[0m11:34:12.107824 [debug] [Thread-1 (]: Began compiling node model.job_intelligent.dim_location
[0m11:34:12.111877 [debug] [Thread-1 (]: Writing injected SQL for node "model.job_intelligent.dim_location"
[0m11:34:12.112578 [debug] [Thread-3 (]: Began running node model.job_intelligent.int_skills_extraction
[0m11:34:12.113105 [info ] [Thread-3 (]: 7 of 13 START sql table model main_silver.int_skills_extraction ................ [RUN]
[0m11:34:12.113622 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly model.job_intelligent.int_job_title_normalization, now model.job_intelligent.int_skills_extraction)
[0m11:34:12.114036 [debug] [Thread-3 (]: Began compiling node model.job_intelligent.int_skills_extraction
[0m11:34:12.117847 [debug] [Thread-3 (]: Writing injected SQL for node "model.job_intelligent.int_skills_extraction"
[0m11:34:12.118474 [debug] [Thread-4 (]: Began running node model.job_intelligent.dim_company
[0m11:34:12.119044 [debug] [Thread-2 (]: Began running node model.job_intelligent.dim_time
[0m11:34:12.119778 [info ] [Thread-4 (]: 4 of 13 START sql table model main_gold.dim_company ............................ [RUN]
[0m11:34:12.120711 [info ] [Thread-2 (]: 6 of 13 START sql table model main_gold.dim_time ............................... [RUN]
[0m11:34:12.121549 [debug] [Thread-4 (]: Acquiring new duckdb connection 'model.job_intelligent.dim_company'
[0m11:34:12.122156 [debug] [Thread-3 (]: Began executing node model.job_intelligent.int_skills_extraction
[0m11:34:12.122742 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly model.job_intelligent.int_jobs_cleaned, now model.job_intelligent.dim_time)
[0m11:34:12.123272 [debug] [Thread-4 (]: Began compiling node model.job_intelligent.dim_company
[0m11:34:12.123753 [debug] [Thread-1 (]: Began executing node model.job_intelligent.dim_location
[0m11:34:12.126937 [debug] [Thread-3 (]: Writing runtime sql for node "model.job_intelligent.int_skills_extraction"
[0m11:34:12.127497 [debug] [Thread-2 (]: Began compiling node model.job_intelligent.dim_time
[0m11:34:12.131323 [debug] [Thread-4 (]: Writing injected SQL for node "model.job_intelligent.dim_company"
[0m11:34:12.134689 [debug] [Thread-1 (]: Writing runtime sql for node "model.job_intelligent.dim_location"
[0m11:34:12.139431 [debug] [Thread-2 (]: Writing injected SQL for node "model.job_intelligent.dim_time"
[0m11:34:12.141026 [debug] [Thread-3 (]: Using duckdb connection "model.job_intelligent.int_skills_extraction"
[0m11:34:12.141594 [debug] [Thread-3 (]: On model.job_intelligent.int_skills_extraction: BEGIN
[0m11:34:12.142087 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m11:34:12.142933 [debug] [Thread-4 (]: Began executing node model.job_intelligent.dim_company
[0m11:34:12.152567 [debug] [Thread-4 (]: Writing runtime sql for node "model.job_intelligent.dim_company"
[0m11:34:12.153574 [debug] [Thread-3 (]: SQL status: OK in 0.011 seconds
[0m11:34:12.154901 [debug] [Thread-3 (]: Using duckdb connection "model.job_intelligent.int_skills_extraction"
[0m11:34:12.155852 [debug] [Thread-2 (]: Began executing node model.job_intelligent.dim_time
[0m11:34:12.156796 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.dim_location"
[0m11:34:12.157875 [debug] [Thread-3 (]: On model.job_intelligent.int_skills_extraction: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.int_skills_extraction"} */

  
    
    

    create  table
      "memory"."main_silver"."int_skills_extraction__dbt_tmp"
  
    as (
      -- models/silver/int_skills_extraction.sql
-- Silver layer: Extraction des compétences depuis la description



WITH jobs_with_titles AS (
    SELECT * FROM "memory"."main_silver"."int_job_title_normalization"
),

skills_mapping AS (
    -- Définir un mapping de compétences communes Data/Tech
    SELECT
        'Python' as skill_name,
        'python|py |\.py' as skill_pattern
    UNION ALL SELECT 'SQL', 'sql|sql|sql server|postgres|oracle'
    UNION ALL SELECT 'Spark', 'spark|pyspark'
    UNION ALL SELECT 'Hadoop', 'hadoop|hdfs'
    UNION ALL SELECT 'Scala', 'scala'
    UNION ALL SELECT 'Java', '\bjava\b'
    UNION ALL SELECT 'R', '\br\b|r programming'
    UNION ALL SELECT 'Tableau', 'tableau'
    UNION ALL SELECT 'Power BI', 'power bi|powerbi'
    UNION ALL SELECT 'Looker', 'looker'
    UNION ALL SELECT 'AWS', 'aws|amazon web|s3 |ec2|redshift'
    UNION ALL SELECT 'Azure', 'azure|microsoft azure|synapse|cosmos'
    UNION ALL SELECT 'GCP', 'gcp|google cloud|bigquery'
    UNION ALL SELECT 'Airflow', 'airflow'
    UNION ALL SELECT 'DBT', '\bdbt\b|dbt'
    UNION ALL SELECT 'Kubernetes', 'kubernetes|k8s'
    UNION ALL SELECT 'Docker', 'docker'
    UNION ALL SELECT 'Git', 'git|github|gitlab'
    UNION ALL SELECT 'TensorFlow', 'tensorflow'
    UNION ALL SELECT 'PyTorch', 'pytorch'
    UNION ALL SELECT 'Scikit-learn', 'scikit|sklearn'
    UNION ALL SELECT 'Pandas', 'pandas'
    UNION ALL SELECT 'NumPy', 'numpy'
    UNION ALL SELECT 'Machine Learning', 'machine learning|deep learning|ml|artificial intelligence'
    UNION ALL SELECT 'Statistics', 'statistics|statistical|probability'
    UNION ALL SELECT 'Data Visualization', 'data visualization|visualization|charts|graphs'
),

jobs_exploded AS (
    SELECT
        j.*,
        s.skill_name,
        CASE 
            WHEN job_description_cleaned ILIKE '%' || s.skill_pattern || '%' THEN 1 
            ELSE 0 
        END as has_skill
    FROM jobs_with_titles j
    CROSS JOIN skills_mapping s
)

SELECT
    *
FROM jobs_exploded
WHERE has_skill = 1

ORDER BY job_title_cleaned, published_date DESC
    );
  
  
[0m11:34:12.164676 [debug] [Thread-1 (]: On model.job_intelligent.dim_location: BEGIN
[0m11:34:12.177511 [debug] [Thread-2 (]: Writing runtime sql for node "model.job_intelligent.dim_time"
[0m11:34:12.178567 [debug] [Thread-4 (]: Using duckdb connection "model.job_intelligent.dim_company"
[0m11:34:12.180122 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:34:12.181099 [debug] [Thread-4 (]: On model.job_intelligent.dim_company: BEGIN
[0m11:34:12.182151 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m11:34:12.183248 [debug] [Thread-1 (]: SQL status: OK in 0.003 seconds
[0m11:34:12.184092 [debug] [Thread-2 (]: Using duckdb connection "model.job_intelligent.dim_time"
[0m11:34:12.184808 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.dim_location"
[0m11:34:12.185640 [debug] [Thread-4 (]: SQL status: OK in 0.003 seconds
[0m11:34:12.186514 [debug] [Thread-2 (]: On model.job_intelligent.dim_time: BEGIN
[0m11:34:12.187468 [debug] [Thread-1 (]: On model.job_intelligent.dim_location: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_location"} */

  
    
    

    create  table
      "memory"."main_gold"."dim_location__dbt_tmp"
  
    as (
      -- models/gold/dim_location.sql
-- Gold layer: Dimension Location



WITH jobs AS (
    SELECT DISTINCT
        location_cleaned as location_raw
    FROM "memory"."main_silver"."int_job_title_normalization"
    WHERE location_cleaned IS NOT NULL
),

location_parsed AS (
    SELECT
        location_raw,
        -- Extract city (before comma)
        CASE 
            WHEN POSITION(',' IN location_raw) > 0 
            THEN TRIM(SUBSTRING(location_raw, 1, POSITION(',' IN location_raw) - 1))
            ELSE location_raw
        END as city,
        
        -- Extract country (after comma)
        CASE 
            WHEN POSITION(',' IN location_raw) > 0 
            THEN TRIM(SUBSTRING(location_raw, POSITION(',' IN location_raw) + 1))
            ELSE 'Not Specified'
        END as country,
        
        -- Detect if remote
        CASE 
            WHEN location_raw LIKE '%remote%' 
            THEN 'Remote'
            ELSE 'On-site'
        END as work_location_type
    FROM jobs
),

ranked_locations AS (
    SELECT
        ROW_NUMBER() OVER (ORDER BY location_raw) as location_id,
        location_raw,
        city,
        country,
        work_location_type,
        NOW() as created_at
    FROM location_parsed
)

SELECT
    location_id,
    location_raw,
    city,
    country,
    work_location_type,
    created_at
FROM ranked_locations
ORDER BY location_id
    );
  
  
[0m11:34:12.188461 [debug] [Thread-4 (]: Using duckdb connection "model.job_intelligent.dim_company"
[0m11:34:12.189232 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m11:34:12.190654 [debug] [Thread-4 (]: On model.job_intelligent.dim_company: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_company"} */

  
    
    

    create  table
      "memory"."main_gold"."dim_company__dbt_tmp"
  
    as (
      -- models/gold/dim_company.sql
-- Gold layer: Dimension Company



WITH jobs AS (
    SELECT DISTINCT
        company_name_cleaned,
        company_url
    FROM "memory"."main_silver"."int_job_title_normalization"
    WHERE company_name_cleaned IS NOT NULL
),

ranked_companies AS (
    SELECT
        ROW_NUMBER() OVER (ORDER BY company_name_cleaned) as company_id,
        company_name_cleaned as company_name,
        company_url,
        NOW() as created_at
    FROM jobs
)

SELECT
    company_id,
    company_name,
    company_url,
    created_at
FROM ranked_companies
ORDER BY company_id
    );
  
  
[0m11:34:12.192207 [debug] [Thread-2 (]: SQL status: OK in 0.003 seconds
[0m11:34:12.192984 [debug] [Thread-2 (]: Using duckdb connection "model.job_intelligent.dim_time"
[0m11:34:12.193924 [debug] [Thread-2 (]: On model.job_intelligent.dim_time: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_time"} */

  
    
    

    create  table
      "memory"."main_gold"."dim_time__dbt_tmp"
  
    as (
      -- models/gold/dim_time.sql
-- Gold layer: Dimension Time



-- Generate a date dimension for time-based analysis
WITH date_spine AS (
    SELECT
        published_date,
        EXTRACT(YEAR FROM published_date) as year,
        EXTRACT(MONTH FROM published_date) as month,
        EXTRACT(QUARTER FROM published_date) as quarter,
        EXTRACT(WEEK FROM published_date) as week,
        EXTRACT(DAYOFWEEK FROM published_date) as day_of_week,
        DATE_TRUNC('month', published_date) as month_start,
        DATE_TRUNC('quarter', published_date) as quarter_start,
        DATE_TRUNC('year', published_date) as year_start,
        
        CASE 
            WHEN EXTRACT(MONTH FROM published_date) IN (1,2,3) THEN 'Q1'
            WHEN EXTRACT(MONTH FROM published_date) IN (4,5,6) THEN 'Q2'
            WHEN EXTRACT(MONTH FROM published_date) IN (7,8,9) THEN 'Q3'
            ELSE 'Q4'
        END as quarter_name,
        
        CASE EXTRACT(DAYOFWEEK FROM published_date)
            WHEN 0 THEN 'Sunday'
            WHEN 1 THEN 'Monday'
            WHEN 2 THEN 'Tuesday'
            WHEN 3 THEN 'Wednesday'
            WHEN 4 THEN 'Thursday'
            WHEN 5 THEN 'Friday'
            WHEN 6 THEN 'Saturday'
        END as day_name,
        
        CASE EXTRACT(MONTH FROM published_date)
            WHEN 1 THEN 'January'
            WHEN 2 THEN 'February'
            WHEN 3 THEN 'March'
            WHEN 4 THEN 'April'
            WHEN 5 THEN 'May'
            WHEN 6 THEN 'June'
            WHEN 7 THEN 'July'
            WHEN 8 THEN 'August'
            WHEN 9 THEN 'September'
            WHEN 10 THEN 'October'
            WHEN 11 THEN 'November'
            WHEN 12 THEN 'December'
        END as month_name
        
    FROM (
        SELECT DISTINCT published_date
        FROM "memory"."main_silver"."int_job_title_normalization"
        WHERE published_date IS NOT NULL
    )
)

SELECT
    published_date as date_id,
    year,
    month,
    quarter,
    week,
    day_of_week,
    day_name,
    month_name,
    quarter_name,
    month_start,
    quarter_start,
    year_start,
    NOW() as created_at
FROM date_spine
ORDER BY published_date
    );
  
  
[0m11:34:12.199273 [debug] [Thread-1 (]: SQL status: OK in 0.009 seconds
[0m11:34:12.205897 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.dim_location"
[0m11:34:12.206816 [debug] [Thread-4 (]: SQL status: OK in 0.015 seconds
[0m11:34:12.207667 [debug] [Thread-1 (]: On model.job_intelligent.dim_location: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_location"} */
alter table "memory"."main_gold"."dim_location__dbt_tmp" rename to "dim_location"
[0m11:34:12.211751 [debug] [Thread-4 (]: Using duckdb connection "model.job_intelligent.dim_company"
[0m11:34:12.212748 [debug] [Thread-4 (]: On model.job_intelligent.dim_company: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_company"} */
alter table "memory"."main_gold"."dim_company__dbt_tmp" rename to "dim_company"
[0m11:34:12.213613 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m11:34:12.215319 [debug] [Thread-1 (]: On model.job_intelligent.dim_location: COMMIT
[0m11:34:12.215766 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.dim_location"
[0m11:34:12.216371 [debug] [Thread-2 (]: SQL status: OK in 0.021 seconds
[0m11:34:12.216955 [debug] [Thread-1 (]: On model.job_intelligent.dim_location: COMMIT
[0m11:34:12.217510 [debug] [Thread-4 (]: SQL status: OK in 0.004 seconds
[0m11:34:12.222303 [debug] [Thread-2 (]: Using duckdb connection "model.job_intelligent.dim_time"
[0m11:34:12.225150 [debug] [Thread-4 (]: On model.job_intelligent.dim_company: COMMIT
[0m11:34:12.225894 [debug] [Thread-2 (]: On model.job_intelligent.dim_time: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_time"} */
alter table "memory"."main_gold"."dim_time__dbt_tmp" rename to "dim_time"
[0m11:34:12.226430 [debug] [Thread-4 (]: Using duckdb connection "model.job_intelligent.dim_company"
[0m11:34:12.226954 [debug] [Thread-1 (]: SQL status: OK in 0.004 seconds
[0m11:34:12.227502 [debug] [Thread-4 (]: On model.job_intelligent.dim_company: COMMIT
[0m11:34:12.230355 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.dim_location"
[0m11:34:12.231246 [debug] [Thread-1 (]: On model.job_intelligent.dim_location: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_location"} */

      drop table if exists "memory"."main_gold"."dim_location__dbt_backup" cascade
    
[0m11:34:12.231906 [debug] [Thread-2 (]: SQL status: OK in 0.004 seconds
[0m11:34:12.233617 [debug] [Thread-2 (]: On model.job_intelligent.dim_time: COMMIT
[0m11:34:12.234137 [debug] [Thread-2 (]: Using duckdb connection "model.job_intelligent.dim_time"
[0m11:34:12.234592 [debug] [Thread-2 (]: On model.job_intelligent.dim_time: COMMIT
[0m11:34:12.235241 [debug] [Thread-4 (]: SQL status: OK in 0.004 seconds
[0m11:34:12.235984 [debug] [Thread-1 (]: SQL status: OK in 0.004 seconds
[0m11:34:12.238649 [debug] [Thread-4 (]: Using duckdb connection "model.job_intelligent.dim_company"
[0m11:34:12.239171 [debug] [Thread-2 (]: SQL status: OK in 0.004 seconds
[0m11:34:12.240798 [debug] [Thread-1 (]: On model.job_intelligent.dim_location: Close
[0m11:34:12.241305 [debug] [Thread-4 (]: On model.job_intelligent.dim_company: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_company"} */

      drop table if exists "memory"."main_gold"."dim_company__dbt_backup" cascade
    
[0m11:34:12.244110 [debug] [Thread-2 (]: Using duckdb connection "model.job_intelligent.dim_time"
[0m11:34:12.244895 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6b81b1d3-1bf4-4e1f-a930-48c85c27ee03', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020EB76B1DD0>]}
[0m11:34:12.245645 [debug] [Thread-2 (]: On model.job_intelligent.dim_time: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_time"} */

      drop table if exists "memory"."main_gold"."dim_time__dbt_backup" cascade
    
[0m11:34:12.246910 [debug] [Thread-4 (]: SQL status: OK in 0.001 seconds
[0m11:34:12.246458 [info ] [Thread-1 (]: 5 of 13 OK created sql table model main_gold.dim_location ...................... [[32mOK[0m in 0.14s]
[0m11:34:12.249400 [debug] [Thread-4 (]: On model.job_intelligent.dim_company: Close
[0m11:34:12.250089 [debug] [Thread-2 (]: SQL status: OK in 0.003 seconds
[0m11:34:12.250907 [debug] [Thread-1 (]: Finished running node model.job_intelligent.dim_location
[0m11:34:12.251657 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6b81b1d3-1bf4-4e1f-a930-48c85c27ee03', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020EB8851550>]}
[0m11:34:12.253595 [debug] [Thread-2 (]: On model.job_intelligent.dim_time: Close
[0m11:34:12.254709 [info ] [Thread-4 (]: 4 of 13 OK created sql table model main_gold.dim_company ....................... [[32mOK[0m in 0.13s]
[0m11:34:12.255569 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6b81b1d3-1bf4-4e1f-a930-48c85c27ee03', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020EB5BFF1D0>]}
[0m11:34:12.256453 [debug] [Thread-4 (]: Finished running node model.job_intelligent.dim_company
[0m11:34:12.257234 [info ] [Thread-2 (]: 6 of 13 OK created sql table model main_gold.dim_time .......................... [[32mOK[0m in 0.13s]
[0m11:34:12.258324 [debug] [Thread-2 (]: Finished running node model.job_intelligent.dim_time
[0m11:34:12.259002 [debug] [Thread-1 (]: Began running node model.job_intelligent.fact_job_offers
[0m11:34:12.259541 [info ] [Thread-1 (]: 8 of 13 START sql table model main_gold.fact_job_offers ........................ [RUN]
[0m11:34:12.260090 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.job_intelligent.dim_location, now model.job_intelligent.fact_job_offers)
[0m11:34:12.260511 [debug] [Thread-1 (]: Began compiling node model.job_intelligent.fact_job_offers
[0m11:34:12.264770 [debug] [Thread-1 (]: Writing injected SQL for node "model.job_intelligent.fact_job_offers"
[0m11:34:12.265706 [debug] [Thread-1 (]: Began executing node model.job_intelligent.fact_job_offers
[0m11:34:12.270463 [debug] [Thread-1 (]: Writing runtime sql for node "model.job_intelligent.fact_job_offers"
[0m11:34:12.271543 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.fact_job_offers"
[0m11:34:12.272012 [debug] [Thread-1 (]: On model.job_intelligent.fact_job_offers: BEGIN
[0m11:34:12.272422 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:34:12.273241 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m11:34:12.273672 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.fact_job_offers"
[0m11:34:12.274217 [debug] [Thread-1 (]: On model.job_intelligent.fact_job_offers: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.fact_job_offers"} */

  
    
    

    create  table
      "memory"."main_gold"."fact_job_offers__dbt_tmp"
  
    as (
      -- models/gold/fact_job_offers.sql
-- Gold layer: Fact Table - Job Offers (Schéma en Étoile)



WITH jobs AS (
    SELECT
        j.*
    FROM "memory"."main_silver"."int_job_title_normalization" j
),

companies AS (
    SELECT * FROM "memory"."main_gold"."dim_company"
),

locations AS (
    SELECT * FROM "memory"."main_gold"."dim_location"
),

times AS (
    SELECT * FROM "memory"."main_gold"."dim_time"
),

fact_table AS (
    SELECT
        -- Surrogate keys
        ROW_NUMBER() OVER (ORDER BY j.job_url, j.company_name_cleaned) as job_offer_id,
        
        -- Foreign keys
        c.company_id,
        l.location_id,
        t.date_id as published_date_id,
        
        -- Job dimensions
        j.job_title_cleaned as job_title,
        j.job_category,
        j.contract_type_normalized as contract_type,
        j.work_type_normalized as work_type,
        
        -- URLs
        j.job_url,
        j.company_url,
        
        -- Description
        j.job_description_cleaned as job_description,
        
        -- Time dimension
        j.published_date,
        j.posted_time,
        j.published_year_month,
        j.published_year,
        j.published_month,
        
        -- Metrics
        LENGTH(j.job_description_cleaned) as description_length,
        (LENGTH(j.job_description_cleaned) - LENGTH(REPLACE(j.job_description_cleaned, ' ', ''))) + 1 as word_count,
        
        -- Flags
        CASE WHEN j.work_type_normalized = 'Remote' THEN 1 ELSE 0 END as is_remote,
        CASE WHEN j.contract_type_normalized = 'Permanent' THEN 1 ELSE 0 END as is_permanent,
        
        -- Metadata
        NOW() as created_at,
        j.ingestion_timestamp
        
    FROM jobs j
    LEFT JOIN companies c ON j.company_name_cleaned = c.company_name
    LEFT JOIN locations l ON j.location_cleaned = l.location_raw
    LEFT JOIN times t ON j.published_date = t.date_id
)

SELECT
    job_offer_id,
    company_id,
    location_id,
    published_date_id,
    job_title,
    job_category,
    contract_type,
    work_type,
    job_url,
    company_url,
    job_description,
    published_date,
    posted_time,
    published_year_month,
    published_year,
    published_month,
    description_length,
    word_count,
    is_remote,
    is_permanent,
    created_at,
    ingestion_timestamp
FROM fact_table
ORDER BY published_date DESC, job_offer_id
    );
  
  
[0m11:34:12.416105 [debug] [Thread-1 (]: SQL status: OK in 0.141 seconds
[0m11:34:12.420278 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.fact_job_offers"
[0m11:34:12.420731 [debug] [Thread-1 (]: On model.job_intelligent.fact_job_offers: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.fact_job_offers"} */
alter table "memory"."main_gold"."fact_job_offers__dbt_tmp" rename to "fact_job_offers"
[0m11:34:12.421537 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m11:34:12.422896 [debug] [Thread-1 (]: On model.job_intelligent.fact_job_offers: COMMIT
[0m11:34:12.423237 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.fact_job_offers"
[0m11:34:12.423551 [debug] [Thread-1 (]: On model.job_intelligent.fact_job_offers: COMMIT
[0m11:34:12.424184 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m11:34:12.425947 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.fact_job_offers"
[0m11:34:12.426285 [debug] [Thread-1 (]: On model.job_intelligent.fact_job_offers: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.fact_job_offers"} */

      drop table if exists "memory"."main_gold"."fact_job_offers__dbt_backup" cascade
    
[0m11:34:12.426881 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m11:34:12.427992 [debug] [Thread-1 (]: On model.job_intelligent.fact_job_offers: Close
[0m11:34:12.428455 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6b81b1d3-1bf4-4e1f-a930-48c85c27ee03', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020EB76C6E10>]}
[0m11:34:12.428976 [info ] [Thread-1 (]: 8 of 13 OK created sql table model main_gold.fact_job_offers ................... [[32mOK[0m in 0.17s]
[0m11:34:12.429616 [debug] [Thread-1 (]: Finished running node model.job_intelligent.fact_job_offers
[0m11:34:12.430347 [debug] [Thread-4 (]: Began running node model.job_intelligent.agg_job_offers_by_category_time
[0m11:34:12.430856 [info ] [Thread-4 (]: 9 of 13 START sql table model main_gold.agg_job_offers_by_category_time ........ [RUN]
[0m11:34:12.431385 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly model.job_intelligent.dim_company, now model.job_intelligent.agg_job_offers_by_category_time)
[0m11:34:12.431776 [debug] [Thread-4 (]: Began compiling node model.job_intelligent.agg_job_offers_by_category_time
[0m11:34:12.434787 [debug] [Thread-4 (]: Writing injected SQL for node "model.job_intelligent.agg_job_offers_by_category_time"
[0m11:34:12.435324 [debug] [Thread-2 (]: Began running node model.job_intelligent.agg_location_analysis
[0m11:34:12.436166 [info ] [Thread-2 (]: 10 of 13 START sql table model main_gold.agg_location_analysis ................. [RUN]
[0m11:34:12.437156 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly model.job_intelligent.dim_time, now model.job_intelligent.agg_location_analysis)
[0m11:34:12.437749 [debug] [Thread-2 (]: Began compiling node model.job_intelligent.agg_location_analysis
[0m11:34:12.442315 [debug] [Thread-2 (]: Writing injected SQL for node "model.job_intelligent.agg_location_analysis"
[0m11:34:12.443079 [debug] [Thread-4 (]: Began executing node model.job_intelligent.agg_job_offers_by_category_time
[0m11:34:12.445554 [debug] [Thread-4 (]: Writing runtime sql for node "model.job_intelligent.agg_job_offers_by_category_time"
[0m11:34:12.446343 [debug] [Thread-2 (]: Began executing node model.job_intelligent.agg_location_analysis
[0m11:34:12.449123 [debug] [Thread-2 (]: Writing runtime sql for node "model.job_intelligent.agg_location_analysis"
[0m11:34:12.449864 [debug] [Thread-4 (]: Using duckdb connection "model.job_intelligent.agg_job_offers_by_category_time"
[0m11:34:12.450312 [debug] [Thread-4 (]: On model.job_intelligent.agg_job_offers_by_category_time: BEGIN
[0m11:34:12.450734 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m11:34:12.451552 [debug] [Thread-2 (]: Using duckdb connection "model.job_intelligent.agg_location_analysis"
[0m11:34:12.452094 [debug] [Thread-2 (]: On model.job_intelligent.agg_location_analysis: BEGIN
[0m11:34:12.452655 [debug] [Thread-4 (]: SQL status: OK in 0.002 seconds
[0m11:34:12.453054 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m11:34:12.453573 [debug] [Thread-4 (]: Using duckdb connection "model.job_intelligent.agg_job_offers_by_category_time"
[0m11:34:12.454401 [debug] [Thread-4 (]: On model.job_intelligent.agg_job_offers_by_category_time: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.agg_job_offers_by_category_time"} */

  
    
    

    create  table
      "memory"."main_gold"."agg_job_offers_by_category_time__dbt_tmp"
  
    as (
      -- models/gold/agg_job_offers_by_category_time.sql
-- Gold layer: Aggregate - Job Offers by Category and Time



SELECT
    f.published_year,
    f.published_month,
    f.published_year_month,
    f.job_category,
    f.contract_type,
    f.work_type,
    
    COUNT(DISTINCT f.job_offer_id) as count_job_offers,
    COUNT(DISTINCT f.company_id) as count_companies,
    
    AVG(f.description_length) as avg_description_length,
    AVG(f.word_count) as avg_word_count,
    
    SUM(CASE WHEN f.is_remote = 1 THEN 1 ELSE 0 END) as remote_jobs,
    SUM(CASE WHEN f.is_permanent = 1 THEN 1 ELSE 0 END) as permanent_jobs,
    
    CURRENT_TIMESTAMP() as created_at
    
FROM "memory"."main_gold"."fact_job_offers" f
WHERE f.published_date IS NOT NULL
GROUP BY
    f.published_year,
    f.published_month,
    f.published_year_month,
    f.job_category,
    f.contract_type,
    f.work_type
ORDER BY f.published_year_month DESC, f.job_category
    );
  
  
[0m11:34:12.455126 [debug] [Thread-2 (]: SQL status: OK in 0.002 seconds
[0m11:34:12.455534 [debug] [Thread-2 (]: Using duckdb connection "model.job_intelligent.agg_location_analysis"
[0m11:34:12.455940 [debug] [Thread-2 (]: On model.job_intelligent.agg_location_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.agg_location_analysis"} */

  
    
    

    create  table
      "memory"."main_gold"."agg_location_analysis__dbt_tmp"
  
    as (
      -- models/gold/agg_location_analysis.sql
-- Gold layer: Aggregate - Location Analysis



SELECT
    dl.location_id,
    dl.location_raw,
    dl.city,
    dl.country,
    dl.work_location_type,
    
    COUNT(DISTINCT f.job_offer_id) as count_job_offers,
    COUNT(DISTINCT f.company_id) as count_companies,
    
    -- Distribution by job category
    COUNT(DISTINCT CASE WHEN f.job_category = 'Data Engineer' THEN f.job_offer_id END) as data_engineer_count,
    COUNT(DISTINCT CASE WHEN f.job_category = 'Data Scientist' THEN f.job_offer_id END) as data_scientist_count,
    COUNT(DISTINCT CASE WHEN f.job_category = 'Data Analyst' THEN f.job_offer_id END) as data_analyst_count,
    COUNT(DISTINCT CASE WHEN f.job_category = 'ML Engineer' THEN f.job_offer_id END) as ml_engineer_count,
    
    -- Remote percentage
    ROUND(
        100.0 * SUM(CASE WHEN f.is_remote = 1 THEN 1 ELSE 0 END) / COUNT(DISTINCT f.job_offer_id),
        2
    ) as pct_remote,
    
    CURRENT_TIMESTAMP() as created_at
    
FROM "memory"."main_gold"."dim_location" dl
LEFT JOIN "memory"."main_gold"."fact_job_offers" f ON dl.location_id = f.location_id
GROUP BY
    dl.location_id,
    dl.location_raw,
    dl.city,
    dl.country,
    dl.work_location_type
ORDER BY count_job_offers DESC
    );
  
  
[0m11:34:12.457570 [debug] [Thread-4 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.agg_job_offers_by_category_time"} */

  
    
    

    create  table
      "memory"."main_gold"."agg_job_offers_by_category_time__dbt_tmp"
  
    as (
      -- models/gold/agg_job_offers_by_category_time.sql
-- Gold layer: Aggregate - Job Offers by Category and Time



SELECT
    f.published_year,
    f.published_month,
    f.published_year_month,
    f.job_category,
    f.contract_type,
    f.work_type,
    
    COUNT(DISTINCT f.job_offer_id) as count_job_offers,
    COUNT(DISTINCT f.company_id) as count_companies,
    
    AVG(f.description_length) as avg_description_length,
    AVG(f.word_count) as avg_word_count,
    
    SUM(CASE WHEN f.is_remote = 1 THEN 1 ELSE 0 END) as remote_jobs,
    SUM(CASE WHEN f.is_permanent = 1 THEN 1 ELSE 0 END) as permanent_jobs,
    
    CURRENT_TIMESTAMP() as created_at
    
FROM "memory"."main_gold"."fact_job_offers" f
WHERE f.published_date IS NOT NULL
GROUP BY
    f.published_year,
    f.published_month,
    f.published_year_month,
    f.job_category,
    f.contract_type,
    f.work_type
ORDER BY f.published_year_month DESC, f.job_category
    );
  
  
[0m11:34:12.458166 [debug] [Thread-2 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.agg_location_analysis"} */

  
    
    

    create  table
      "memory"."main_gold"."agg_location_analysis__dbt_tmp"
  
    as (
      -- models/gold/agg_location_analysis.sql
-- Gold layer: Aggregate - Location Analysis



SELECT
    dl.location_id,
    dl.location_raw,
    dl.city,
    dl.country,
    dl.work_location_type,
    
    COUNT(DISTINCT f.job_offer_id) as count_job_offers,
    COUNT(DISTINCT f.company_id) as count_companies,
    
    -- Distribution by job category
    COUNT(DISTINCT CASE WHEN f.job_category = 'Data Engineer' THEN f.job_offer_id END) as data_engineer_count,
    COUNT(DISTINCT CASE WHEN f.job_category = 'Data Scientist' THEN f.job_offer_id END) as data_scientist_count,
    COUNT(DISTINCT CASE WHEN f.job_category = 'Data Analyst' THEN f.job_offer_id END) as data_analyst_count,
    COUNT(DISTINCT CASE WHEN f.job_category = 'ML Engineer' THEN f.job_offer_id END) as ml_engineer_count,
    
    -- Remote percentage
    ROUND(
        100.0 * SUM(CASE WHEN f.is_remote = 1 THEN 1 ELSE 0 END) / COUNT(DISTINCT f.job_offer_id),
        2
    ) as pct_remote,
    
    CURRENT_TIMESTAMP() as created_at
    
FROM "memory"."main_gold"."dim_location" dl
LEFT JOIN "memory"."main_gold"."fact_job_offers" f ON dl.location_id = f.location_id
GROUP BY
    dl.location_id,
    dl.location_raw,
    dl.city,
    dl.country,
    dl.work_location_type
ORDER BY count_job_offers DESC
    );
  
  
[0m11:34:12.458659 [debug] [Thread-4 (]: DuckDB adapter: Rolling back transaction.
[0m11:34:12.459120 [debug] [Thread-2 (]: DuckDB adapter: Rolling back transaction.
[0m11:34:12.459675 [debug] [Thread-4 (]: On model.job_intelligent.agg_job_offers_by_category_time: ROLLBACK
[0m11:34:12.460178 [debug] [Thread-2 (]: On model.job_intelligent.agg_location_analysis: ROLLBACK
[0m11:34:12.472421 [debug] [Thread-4 (]: Failed to rollback 'model.job_intelligent.agg_job_offers_by_category_time'
[0m11:34:12.472836 [debug] [Thread-4 (]: On model.job_intelligent.agg_job_offers_by_category_time: Close
[0m11:34:12.475522 [debug] [Thread-2 (]: Failed to rollback 'model.job_intelligent.agg_location_analysis'
[0m11:34:12.475858 [debug] [Thread-2 (]: On model.job_intelligent.agg_location_analysis: Close
[0m11:34:12.481258 [debug] [Thread-4 (]: Runtime Error in model agg_job_offers_by_category_time (models\gold\agg_job_offers_by_category_time.sql)
  Catalog Error: Scalar Function with name current_timestamp does not exist!
  Did you mean "current_localtimestamp"?
  
  LINE 33:     CURRENT_TIMESTAMP() as created_at
               ^
[0m11:34:12.481689 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6b81b1d3-1bf4-4e1f-a930-48c85c27ee03', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020EB8837CB0>]}
[0m11:34:12.482236 [error] [Thread-4 (]: 9 of 13 ERROR creating sql table model main_gold.agg_job_offers_by_category_time  [[31mERROR[0m in 0.05s]
[0m11:34:12.482944 [debug] [Thread-4 (]: Finished running node model.job_intelligent.agg_job_offers_by_category_time
[0m11:34:12.483567 [debug] [Thread-7 (]: Marking all children of 'model.job_intelligent.agg_job_offers_by_category_time' to be skipped because of status 'error'.  Reason: Runtime Error in model agg_job_offers_by_category_time (models\gold\agg_job_offers_by_category_time.sql)
  Catalog Error: Scalar Function with name current_timestamp does not exist!
  Did you mean "current_localtimestamp"?
  
  LINE 33:     CURRENT_TIMESTAMP() as created_at
               ^.
[0m11:34:12.487442 [debug] [Thread-2 (]: Runtime Error in model agg_location_analysis (models\gold\agg_location_analysis.sql)
  Catalog Error: Scalar Function with name current_timestamp does not exist!
  Did you mean "current_localtimestamp"?
  
  LINE 38:     CURRENT_TIMESTAMP() as created_at
               ^
[0m11:34:12.487982 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6b81b1d3-1bf4-4e1f-a930-48c85c27ee03', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020EB88004D0>]}
[0m11:34:12.488583 [error] [Thread-2 (]: 10 of 13 ERROR creating sql table model main_gold.agg_location_analysis ........ [[31mERROR[0m in 0.05s]
[0m11:34:12.489161 [debug] [Thread-2 (]: Finished running node model.job_intelligent.agg_location_analysis
[0m11:34:12.489694 [debug] [Thread-7 (]: Marking all children of 'model.job_intelligent.agg_location_analysis' to be skipped because of status 'error'.  Reason: Runtime Error in model agg_location_analysis (models\gold\agg_location_analysis.sql)
  Catalog Error: Scalar Function with name current_timestamp does not exist!
  Did you mean "current_localtimestamp"?
  
  LINE 38:     CURRENT_TIMESTAMP() as created_at
               ^.
[0m11:34:14.166661 [debug] [Thread-3 (]: SQL status: OK in 1.987 seconds
[0m11:34:14.169594 [debug] [Thread-3 (]: Using duckdb connection "model.job_intelligent.int_skills_extraction"
[0m11:34:14.170046 [debug] [Thread-3 (]: On model.job_intelligent.int_skills_extraction: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.int_skills_extraction"} */
alter table "memory"."main_silver"."int_skills_extraction__dbt_tmp" rename to "int_skills_extraction"
[0m11:34:14.170838 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m11:34:14.172326 [debug] [Thread-3 (]: On model.job_intelligent.int_skills_extraction: COMMIT
[0m11:34:14.172727 [debug] [Thread-3 (]: Using duckdb connection "model.job_intelligent.int_skills_extraction"
[0m11:34:14.173091 [debug] [Thread-3 (]: On model.job_intelligent.int_skills_extraction: COMMIT
[0m11:34:14.173900 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m11:34:14.176569 [debug] [Thread-3 (]: Using duckdb connection "model.job_intelligent.int_skills_extraction"
[0m11:34:14.176981 [debug] [Thread-3 (]: On model.job_intelligent.int_skills_extraction: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.int_skills_extraction"} */

      drop table if exists "memory"."main_silver"."int_skills_extraction__dbt_backup" cascade
    
[0m11:34:14.177649 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m11:34:14.179076 [debug] [Thread-3 (]: On model.job_intelligent.int_skills_extraction: Close
[0m11:34:14.179620 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6b81b1d3-1bf4-4e1f-a930-48c85c27ee03', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020EB88378F0>]}
[0m11:34:14.180258 [info ] [Thread-3 (]: 7 of 13 OK created sql table model main_silver.int_skills_extraction ........... [[32mOK[0m in 2.07s]
[0m11:34:14.180935 [debug] [Thread-3 (]: Finished running node model.job_intelligent.int_skills_extraction
[0m11:34:14.181804 [debug] [Thread-1 (]: Began running node model.job_intelligent.dim_skills
[0m11:34:14.182301 [info ] [Thread-1 (]: 11 of 13 START sql table model main_gold.dim_skills ............................ [RUN]
[0m11:34:14.182808 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.job_intelligent.fact_job_offers, now model.job_intelligent.dim_skills)
[0m11:34:14.183193 [debug] [Thread-1 (]: Began compiling node model.job_intelligent.dim_skills
[0m11:34:14.186923 [debug] [Thread-1 (]: Writing injected SQL for node "model.job_intelligent.dim_skills"
[0m11:34:14.187886 [debug] [Thread-1 (]: Began executing node model.job_intelligent.dim_skills
[0m11:34:14.191390 [debug] [Thread-1 (]: Writing runtime sql for node "model.job_intelligent.dim_skills"
[0m11:34:14.192413 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.dim_skills"
[0m11:34:14.192854 [debug] [Thread-1 (]: On model.job_intelligent.dim_skills: BEGIN
[0m11:34:14.193241 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:34:14.194100 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m11:34:14.194503 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.dim_skills"
[0m11:34:14.194972 [debug] [Thread-1 (]: On model.job_intelligent.dim_skills: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_skills"} */

  
    
    

    create  table
      "memory"."main_gold"."dim_skills__dbt_tmp"
  
    as (
      -- models/gold/dim_skills.sql
-- Gold layer: Dimension Skills



WITH skills AS (
    SELECT DISTINCT
        skill_name
    FROM "memory"."main_silver"."int_skills_extraction"
    WHERE skill_name IS NOT NULL
),

skill_categorization AS (
    SELECT
        skill_name,
        CASE
            WHEN skill_name IN ('Python', 'Java', 'Scala', 'R') THEN 'Programming Language'
            WHEN skill_name IN ('SQL', 'NoSQL') THEN 'Database'
            WHEN skill_name IN ('Spark', 'Hadoop', 'Hive', 'Kafka') THEN 'Big Data Framework'
            WHEN skill_name IN ('TensorFlow', 'PyTorch', 'Scikit-learn') THEN 'ML/DL Library'
            WHEN skill_name IN ('AWS', 'Azure', 'GCP') THEN 'Cloud Platform'
            WHEN skill_name IN ('Tableau', 'Power BI', 'Looker') THEN 'BI Tool'
            WHEN skill_name IN ('Airflow', 'DBT', 'Kubernetes', 'Docker') THEN 'DataOps/DevOps'
            WHEN skill_name IN ('Pandas', 'NumPy', 'Matplotlib') THEN 'Data Analysis Library'
            WHEN skill_name IN ('Machine Learning', 'Statistics', 'Data Visualization') THEN 'Domain Knowledge'
            ELSE 'Other'
        END as skill_category
    FROM skills
),

ranked_skills AS (
    SELECT
        ROW_NUMBER() OVER (ORDER BY skill_name) as skill_id,
        skill_name,
        skill_category,
        NOW() as created_at
    FROM skill_categorization
)

SELECT
    skill_id,
    skill_name,
    skill_category,
    created_at
FROM ranked_skills
ORDER BY skill_id
    );
  
  
[0m11:34:14.199777 [debug] [Thread-1 (]: SQL status: OK in 0.004 seconds
[0m11:34:14.204072 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.dim_skills"
[0m11:34:14.204540 [debug] [Thread-1 (]: On model.job_intelligent.dim_skills: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_skills"} */
alter table "memory"."main_gold"."dim_skills__dbt_tmp" rename to "dim_skills"
[0m11:34:14.205352 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m11:34:14.206956 [debug] [Thread-1 (]: On model.job_intelligent.dim_skills: COMMIT
[0m11:34:14.207380 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.dim_skills"
[0m11:34:14.207775 [debug] [Thread-1 (]: On model.job_intelligent.dim_skills: COMMIT
[0m11:34:14.208444 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m11:34:14.210622 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.dim_skills"
[0m11:34:14.211037 [debug] [Thread-1 (]: On model.job_intelligent.dim_skills: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_skills"} */

      drop table if exists "memory"."main_gold"."dim_skills__dbt_backup" cascade
    
[0m11:34:14.211718 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m11:34:14.213125 [debug] [Thread-1 (]: On model.job_intelligent.dim_skills: Close
[0m11:34:14.213683 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6b81b1d3-1bf4-4e1f-a930-48c85c27ee03', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020EB88A9610>]}
[0m11:34:14.214342 [info ] [Thread-1 (]: 11 of 13 OK created sql table model main_gold.dim_skills ....................... [[32mOK[0m in 0.03s]
[0m11:34:14.215047 [debug] [Thread-1 (]: Finished running node model.job_intelligent.dim_skills
[0m11:34:14.215924 [debug] [Thread-4 (]: Began running node model.job_intelligent.fact_job_skills
[0m11:34:14.216452 [info ] [Thread-4 (]: 12 of 13 START sql table model main_gold.fact_job_skills ....................... [RUN]
[0m11:34:14.217022 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly model.job_intelligent.agg_job_offers_by_category_time, now model.job_intelligent.fact_job_skills)
[0m11:34:14.217608 [debug] [Thread-4 (]: Began compiling node model.job_intelligent.fact_job_skills
[0m11:34:14.222329 [debug] [Thread-4 (]: Writing injected SQL for node "model.job_intelligent.fact_job_skills"
[0m11:34:14.223257 [debug] [Thread-4 (]: Began executing node model.job_intelligent.fact_job_skills
[0m11:34:14.226362 [debug] [Thread-4 (]: Writing runtime sql for node "model.job_intelligent.fact_job_skills"
[0m11:34:14.227239 [debug] [Thread-4 (]: Using duckdb connection "model.job_intelligent.fact_job_skills"
[0m11:34:14.227660 [debug] [Thread-4 (]: On model.job_intelligent.fact_job_skills: BEGIN
[0m11:34:14.228049 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m11:34:14.228787 [debug] [Thread-4 (]: SQL status: OK in 0.001 seconds
[0m11:34:14.229192 [debug] [Thread-4 (]: Using duckdb connection "model.job_intelligent.fact_job_skills"
[0m11:34:14.229638 [debug] [Thread-4 (]: On model.job_intelligent.fact_job_skills: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.fact_job_skills"} */

  
    
    

    create  table
      "memory"."main_gold"."fact_job_skills__dbt_tmp"
  
    as (
      -- models/gold/fact_job_skills.sql
-- Gold layer: Bridge Table - Job Skills



WITH skills_raw AS (
    SELECT DISTINCT
        job_url,
        company_name_cleaned,
        skill_name
    FROM "memory"."main_silver"."int_skills_extraction"
),

jobs AS (
    SELECT
        job_offer_id,
        job_url
    FROM "memory"."main_gold"."fact_job_offers"
),

skills_dim AS (
    SELECT
        skill_id,
        skill_name
    FROM "memory"."main_gold"."dim_skills"
),

fact_table AS (
    SELECT
        ROW_NUMBER() OVER (ORDER BY s.job_url, sd.skill_id) as job_skill_id,
        j.job_offer_id,
        sd.skill_id,
        s.skill_name,
        CURRENT_TIMESTAMP() as created_at
    FROM skills_raw s
    LEFT JOIN jobs j ON s.job_url = j.job_url
    LEFT JOIN skills_dim sd ON s.skill_name = sd.skill_name
)

SELECT
    job_skill_id,
    job_offer_id,
    skill_id,
    skill_name,
    created_at
FROM fact_table
WHERE job_offer_id IS NOT NULL
ORDER BY job_offer_id, skill_id
    );
  
  
[0m11:34:14.231625 [debug] [Thread-4 (]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.fact_job_skills"} */

  
    
    

    create  table
      "memory"."main_gold"."fact_job_skills__dbt_tmp"
  
    as (
      -- models/gold/fact_job_skills.sql
-- Gold layer: Bridge Table - Job Skills



WITH skills_raw AS (
    SELECT DISTINCT
        job_url,
        company_name_cleaned,
        skill_name
    FROM "memory"."main_silver"."int_skills_extraction"
),

jobs AS (
    SELECT
        job_offer_id,
        job_url
    FROM "memory"."main_gold"."fact_job_offers"
),

skills_dim AS (
    SELECT
        skill_id,
        skill_name
    FROM "memory"."main_gold"."dim_skills"
),

fact_table AS (
    SELECT
        ROW_NUMBER() OVER (ORDER BY s.job_url, sd.skill_id) as job_skill_id,
        j.job_offer_id,
        sd.skill_id,
        s.skill_name,
        CURRENT_TIMESTAMP() as created_at
    FROM skills_raw s
    LEFT JOIN jobs j ON s.job_url = j.job_url
    LEFT JOIN skills_dim sd ON s.skill_name = sd.skill_name
)

SELECT
    job_skill_id,
    job_offer_id,
    skill_id,
    skill_name,
    created_at
FROM fact_table
WHERE job_offer_id IS NOT NULL
ORDER BY job_offer_id, skill_id
    );
  
  
[0m11:34:14.232103 [debug] [Thread-4 (]: DuckDB adapter: Rolling back transaction.
[0m11:34:14.232626 [debug] [Thread-4 (]: On model.job_intelligent.fact_job_skills: ROLLBACK
[0m11:34:14.238421 [debug] [Thread-4 (]: Failed to rollback 'model.job_intelligent.fact_job_skills'
[0m11:34:14.238924 [debug] [Thread-4 (]: On model.job_intelligent.fact_job_skills: Close
[0m11:34:14.243671 [debug] [Thread-4 (]: Runtime Error in model fact_job_skills (models\gold\fact_job_skills.sql)
  Catalog Error: Scalar Function with name current_timestamp does not exist!
  Did you mean "current_localtimestamp"?
  
  LINE 44:         CURRENT_TIMESTAMP() as created_at
                   ^
[0m11:34:14.244161 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6b81b1d3-1bf4-4e1f-a930-48c85c27ee03', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020EB76C6E70>]}
[0m11:34:14.244822 [error] [Thread-4 (]: 12 of 13 ERROR creating sql table model main_gold.fact_job_skills .............. [[31mERROR[0m in 0.03s]
[0m11:34:14.245515 [debug] [Thread-4 (]: Finished running node model.job_intelligent.fact_job_skills
[0m11:34:14.246236 [debug] [Thread-7 (]: Marking all children of 'model.job_intelligent.fact_job_skills' to be skipped because of status 'error'.  Reason: Runtime Error in model fact_job_skills (models\gold\fact_job_skills.sql)
  Catalog Error: Scalar Function with name current_timestamp does not exist!
  Did you mean "current_localtimestamp"?
  
  LINE 44:         CURRENT_TIMESTAMP() as created_at
                   ^.
[0m11:34:14.247035 [debug] [Thread-2 (]: Began running node model.job_intelligent.agg_skills_demand
[0m11:34:14.247476 [info ] [Thread-2 (]: 13 of 13 SKIP relation main_gold.agg_skills_demand ............................. [[33mSKIP[0m]
[0m11:34:14.248003 [debug] [Thread-2 (]: Finished running node model.job_intelligent.agg_skills_demand
[0m11:34:14.249560 [debug] [MainThread]: Using duckdb connection "master"
[0m11:34:14.249947 [debug] [MainThread]: On master: BEGIN
[0m11:34:14.250288 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m11:34:14.251130 [debug] [MainThread]: SQL status: OK in 0.001 seconds
[0m11:34:14.251663 [debug] [MainThread]: On master: COMMIT
[0m11:34:14.252253 [debug] [MainThread]: Using duckdb connection "master"
[0m11:34:14.252687 [debug] [MainThread]: On master: COMMIT
[0m11:34:14.253297 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m11:34:14.253675 [debug] [MainThread]: On master: Close
[0m11:34:14.254184 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:34:14.254539 [debug] [MainThread]: Connection 'create_memory_main_gold' was properly closed.
[0m11:34:14.254874 [debug] [MainThread]: Connection 'create_memory_main_silver' was properly closed.
[0m11:34:14.255203 [debug] [MainThread]: Connection 'create_memory_main_bronze' was properly closed.
[0m11:34:14.255530 [debug] [MainThread]: Connection 'list_memory_main_bronze' was properly closed.
[0m11:34:14.255860 [debug] [MainThread]: Connection 'list_memory_main_silver' was properly closed.
[0m11:34:14.256199 [debug] [MainThread]: Connection 'list_memory_main_gold' was properly closed.
[0m11:34:14.256532 [debug] [MainThread]: Connection 'model.job_intelligent.dim_skills' was properly closed.
[0m11:34:14.256864 [debug] [MainThread]: Connection 'model.job_intelligent.agg_location_analysis' was properly closed.
[0m11:34:14.257193 [debug] [MainThread]: Connection 'model.job_intelligent.int_skills_extraction' was properly closed.
[0m11:34:14.257518 [debug] [MainThread]: Connection 'model.job_intelligent.fact_job_skills' was properly closed.
[0m11:34:14.258008 [info ] [MainThread]: 
[0m11:34:14.258459 [info ] [MainThread]: Finished running 12 table models, 1 view model in 0 hours 0 minutes and 2.89 seconds (2.89s).
[0m11:34:14.261401 [debug] [MainThread]: Command end result
[0m11:34:14.290083 [debug] [MainThread]: Wrote artifact WritableManifest to D:\lab2\dbt_project\target\manifest.json
[0m11:34:14.292480 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\lab2\dbt_project\target\semantic_manifest.json
[0m11:34:14.299372 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\lab2\dbt_project\target\run_results.json
[0m11:34:14.299756 [info ] [MainThread]: 
[0m11:34:14.300182 [info ] [MainThread]: [31mCompleted with 3 errors, 0 partial successes, and 0 warnings:[0m
[0m11:34:14.300564 [info ] [MainThread]: 
[0m11:34:14.301042 [error] [MainThread]: [31mFailure in model agg_job_offers_by_category_time (models\gold\agg_job_offers_by_category_time.sql)[0m
[0m11:34:14.301588 [error] [MainThread]:   Runtime Error in model agg_job_offers_by_category_time (models\gold\agg_job_offers_by_category_time.sql)
  Catalog Error: Scalar Function with name current_timestamp does not exist!
  Did you mean "current_localtimestamp"?
  
  LINE 33:     CURRENT_TIMESTAMP() as created_at
               ^
[0m11:34:14.302021 [info ] [MainThread]: 
[0m11:34:14.302509 [info ] [MainThread]:   compiled code at target\compiled\job_intelligent\models\gold\agg_job_offers_by_category_time.sql
[0m11:34:14.302894 [info ] [MainThread]: 
[0m11:34:14.303335 [error] [MainThread]: [31mFailure in model agg_location_analysis (models\gold\agg_location_analysis.sql)[0m
[0m11:34:14.303786 [error] [MainThread]:   Runtime Error in model agg_location_analysis (models\gold\agg_location_analysis.sql)
  Catalog Error: Scalar Function with name current_timestamp does not exist!
  Did you mean "current_localtimestamp"?
  
  LINE 38:     CURRENT_TIMESTAMP() as created_at
               ^
[0m11:34:14.304162 [info ] [MainThread]: 
[0m11:34:14.304583 [info ] [MainThread]:   compiled code at target\compiled\job_intelligent\models\gold\agg_location_analysis.sql
[0m11:34:14.304938 [info ] [MainThread]: 
[0m11:34:14.305364 [error] [MainThread]: [31mFailure in model fact_job_skills (models\gold\fact_job_skills.sql)[0m
[0m11:34:14.305811 [error] [MainThread]:   Runtime Error in model fact_job_skills (models\gold\fact_job_skills.sql)
  Catalog Error: Scalar Function with name current_timestamp does not exist!
  Did you mean "current_localtimestamp"?
  
  LINE 44:         CURRENT_TIMESTAMP() as created_at
                   ^
[0m11:34:14.306171 [info ] [MainThread]: 
[0m11:34:14.306574 [info ] [MainThread]:   compiled code at target\compiled\job_intelligent\models\gold\fact_job_skills.sql
[0m11:34:14.306923 [info ] [MainThread]: 
[0m11:34:14.307295 [info ] [MainThread]: Done. PASS=9 WARN=0 ERROR=3 SKIP=1 NO-OP=0 TOTAL=13
[0m11:34:14.308256 [debug] [MainThread]: Command `dbt run` failed at 11:34:14.308135 after 4.14 seconds
[0m11:34:14.308645 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020EB72F5FD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020EB88E3D70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020EB88E0470>]}
[0m11:34:14.309028 [debug] [MainThread]: Flushing usage events
[0m11:34:21.427869 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:36:00.000682 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC44E95FD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC42B94190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC42A07890>]}


============================== 11:36:00.008119 | 24edca5d-105f-454c-9db3-6a4a2aaa594b ==============================
[0m11:36:00.008119 [info ] [MainThread]: Running with dbt=1.10.13
[0m11:36:00.009012 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'target_path': 'None', 'cache_selected_only': 'False', 'warn_error': 'None', 'empty': 'None', 'debug': 'False', 'static_parser': 'True', 'log_cache_events': 'False', 'profiles_dir': 'D:\\lab2\\dbt_project', 'log_format': 'default', 'quiet': 'False', 'use_colors': 'True', 'partial_parse': 'True', 'version_check': 'True', 'log_path': 'D:\\lab2\\dbt_project\\logs', 'introspect': 'True', 'invocation_command': 'dbt debug', 'fail_fast': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'write_json': 'True'}
[0m11:36:00.044857 [info ] [MainThread]: dbt version: 1.10.13
[0m11:36:00.045365 [info ] [MainThread]: python version: 3.13.2
[0m11:36:00.045749 [info ] [MainThread]: python path: C:\Users\HP\AppData\Local\Programs\Python\Python313\python.exe
[0m11:36:00.046115 [info ] [MainThread]: os info: Windows-11-10.0.26200-SP0
[0m11:36:00.225374 [info ] [MainThread]: Using profiles dir at D:\lab2\dbt_project
[0m11:36:00.225810 [info ] [MainThread]: Using profiles.yml file at D:\lab2\dbt_project\profiles.yml
[0m11:36:00.226140 [info ] [MainThread]: Using dbt_project.yml file at D:\lab2\dbt_project\dbt_project.yml
[0m11:36:00.228579 [info ] [MainThread]: adapter type: duckdb
[0m11:36:00.228957 [info ] [MainThread]: adapter version: 1.10.0
[0m11:36:00.376806 [info ] [MainThread]: Configuration:
[0m11:36:00.377246 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m11:36:00.377555 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m11:36:00.377851 [info ] [MainThread]: Required dependencies:
[0m11:36:00.378158 [debug] [MainThread]: Executing "git --help"
[0m11:36:00.433075 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--no-lazy-fetch]\n           [--no-optional-locks] [--no-advice] [--bare] [--git-dir=<path>]\n           [--work-tree=<path>] [--namespace=<name>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone      Clone a repository into a new directory\n   init       Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add        Add file contents to the index\n   mv         Move or rename a file, a directory, or a symlink\n   restore    Restore working tree files\n   rm         Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect     Use binary search to find the commit that introduced a bug\n   diff       Show changes between commits, commit and working tree, etc\n   grep       Print lines matching a pattern\n   log        Show commit logs\n   show       Show various types of objects\n   status     Show the working tree status\n\ngrow, mark and tweak your common history\n   backfill   Download missing objects in a partial clone\n   branch     List, create, or delete branches\n   commit     Record changes to the repository\n   merge      Join two or more development histories together\n   rebase     Reapply commits on top of another base tip\n   reset      Reset current HEAD to the specified state\n   switch     Switch branches\n   tag        Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch      Download objects and refs from another repository\n   pull       Fetch from and integrate with another repository or a local branch\n   push       Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m11:36:00.433908 [debug] [MainThread]: STDERR: "b''"
[0m11:36:00.434610 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m11:36:00.435229 [info ] [MainThread]: Connection:
[0m11:36:00.435779 [info ] [MainThread]:   database: memory
[0m11:36:00.436230 [info ] [MainThread]:   schema: main
[0m11:36:00.436696 [info ] [MainThread]:   path: :memory:
[0m11:36:00.437250 [info ] [MainThread]:   config_options: None
[0m11:36:00.437870 [info ] [MainThread]:   extensions: None
[0m11:36:00.438463 [info ] [MainThread]:   settings: {}
[0m11:36:00.438946 [info ] [MainThread]:   external_root: .
[0m11:36:00.439404 [info ] [MainThread]:   use_credential_provider: None
[0m11:36:00.439856 [info ] [MainThread]:   attach: None
[0m11:36:00.440315 [info ] [MainThread]:   filesystems: None
[0m11:36:00.440759 [info ] [MainThread]:   remote: None
[0m11:36:00.441202 [info ] [MainThread]:   plugins: None
[0m11:36:00.441642 [info ] [MainThread]:   disable_transactions: False
[0m11:36:00.442375 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m11:36:00.722502 [debug] [MainThread]: Acquiring new duckdb connection 'debug'
[0m11:36:00.783627 [debug] [MainThread]: Using duckdb connection "debug"
[0m11:36:00.783972 [debug] [MainThread]: On debug: select 1 as id
[0m11:36:00.784232 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:36:00.800475 [debug] [MainThread]: SQL status: OK in 0.016 seconds
[0m11:36:00.801366 [debug] [MainThread]: On debug: Close
[0m11:36:00.801675 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m11:36:00.801998 [info ] [MainThread]: [32mAll checks passed![0m
[0m11:36:00.802817 [debug] [MainThread]: Command `dbt debug` succeeded at 11:36:00.802717 after 0.99 seconds
[0m11:36:00.803096 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m11:36:00.803397 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC473EDF30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC474895B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC47360E20>]}
[0m11:36:00.803747 [debug] [MainThread]: Flushing usage events
[0m11:36:01.958947 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:36:05.376870 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B98D175FD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B98AE70190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B98ACE7890>]}


============================== 11:36:05.384169 | d9c29e5b-3c65-4c17-9882-3a03c9b40e42 ==============================
[0m11:36:05.384169 [info ] [MainThread]: Running with dbt=1.10.13
[0m11:36:05.384852 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'write_json': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'empty': 'False', 'debug': 'False', 'static_parser': 'True', 'log_cache_events': 'False', 'profiles_dir': 'D:\\lab2\\dbt_project', 'log_format': 'default', 'quiet': 'False', 'use_colors': 'True', 'partial_parse': 'True', 'version_check': 'True', 'log_path': 'D:\\lab2\\dbt_project\\logs', 'introspect': 'True', 'invocation_command': 'dbt run', 'fail_fast': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'no_print': 'None', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'target_path': 'None', 'use_experimental_parser': 'False'}
[0m11:36:05.777069 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'd9c29e5b-3c65-4c17-9882-3a03c9b40e42', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B98D62B230>]}
[0m11:36:05.864281 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'd9c29e5b-3c65-4c17-9882-3a03c9b40e42', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B98D1E8380>]}
[0m11:36:05.867224 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m11:36:06.157722 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m11:36:06.366651 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 4 files changed.
[0m11:36:06.367294 [debug] [MainThread]: Partial parsing: updated file: job_intelligent://models\gold\agg_skills_demand.sql
[0m11:36:06.367760 [debug] [MainThread]: Partial parsing: updated file: job_intelligent://models\gold\agg_location_analysis.sql
[0m11:36:06.368195 [debug] [MainThread]: Partial parsing: updated file: job_intelligent://models\gold\fact_job_skills.sql
[0m11:36:06.368630 [debug] [MainThread]: Partial parsing: updated file: job_intelligent://models\gold\agg_job_offers_by_category_time.sql
[0m11:36:06.784976 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- seeds
[0m11:36:06.790525 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'd9c29e5b-3c65-4c17-9882-3a03c9b40e42', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B98E464A50>]}
[0m11:36:06.859045 [debug] [MainThread]: Wrote artifact WritableManifest to D:\lab2\dbt_project\target\manifest.json
[0m11:36:06.862093 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\lab2\dbt_project\target\semantic_manifest.json
[0m11:36:06.902429 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'd9c29e5b-3c65-4c17-9882-3a03c9b40e42', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B98E16B7A0>]}
[0m11:36:06.903055 [info ] [MainThread]: Found 13 models, 456 macros
[0m11:36:06.903555 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd9c29e5b-3c65-4c17-9882-3a03c9b40e42', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B98F561390>]}
[0m11:36:06.906242 [info ] [MainThread]: 
[0m11:36:06.906839 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m11:36:06.907307 [info ] [MainThread]: 
[0m11:36:06.908084 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m11:36:06.915419 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_memory'
[0m11:36:06.922062 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_memory'
[0m11:36:06.934724 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_memory'
[0m11:36:07.008185 [debug] [ThreadPool]: Using duckdb connection "list_memory"
[0m11:36:07.008566 [debug] [ThreadPool]: On list_memory: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_memory"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"memory"'
    
  
  
[0m11:36:07.008961 [debug] [ThreadPool]: Using duckdb connection "list_memory"
[0m11:36:07.009581 [debug] [ThreadPool]: On list_memory: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_memory"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"memory"'
    
  
  
[0m11:36:07.010171 [debug] [ThreadPool]: Using duckdb connection "list_memory"
[0m11:36:07.009323 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:36:07.011143 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:36:07.011600 [debug] [ThreadPool]: On list_memory: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_memory"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"memory"'
    
  
  
[0m11:36:07.012250 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:36:07.028098 [debug] [ThreadPool]: SQL status: OK in 0.019 seconds
[0m11:36:07.029483 [debug] [ThreadPool]: On list_memory: Close
[0m11:36:07.029838 [debug] [ThreadPool]: SQL status: OK in 0.019 seconds
[0m11:36:07.030392 [debug] [ThreadPool]: SQL status: OK in 0.018 seconds
[0m11:36:07.031465 [debug] [ThreadPool]: On list_memory: Close
[0m11:36:07.032777 [debug] [ThreadPool]: On list_memory: Close
[0m11:36:07.033472 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_memory, now create_memory_main_bronze)
[0m11:36:07.033888 [debug] [ThreadPool]: Creating schema "database: "memory"
schema: "main_bronze"
"
[0m11:36:07.039236 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_bronze"
[0m11:36:07.039539 [debug] [ThreadPool]: On create_memory_main_bronze: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_memory_main_bronze"} */

    
        select type from duckdb_databases()
        where lower(database_name)='memory'
        and type='sqlite'
    
  
[0m11:36:07.039816 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:36:07.040376 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_memory, now create_memory_main_gold)
[0m11:36:07.040841 [debug] [ThreadPool]: Creating schema "database: "memory"
schema: "main_gold"
"
[0m11:36:07.041176 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_memory, now create_memory_main_silver)
[0m11:36:07.044887 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_gold"
[0m11:36:07.041454 [debug] [ThreadPool]: SQL status: OK in 0.002 seconds
[0m11:36:07.045784 [debug] [ThreadPool]: Creating schema "database: "memory"
schema: "main_silver"
"
[0m11:36:07.046144 [debug] [ThreadPool]: On create_memory_main_gold: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_memory_main_gold"} */

    
        select type from duckdb_databases()
        where lower(database_name)='memory'
        and type='sqlite'
    
  
[0m11:36:07.047885 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_silver"
[0m11:36:07.049103 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_bronze"
[0m11:36:07.049427 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:36:07.049857 [debug] [ThreadPool]: On create_memory_main_silver: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_memory_main_silver"} */

    
        select type from duckdb_databases()
        where lower(database_name)='memory'
        and type='sqlite'
    
  
[0m11:36:07.050222 [debug] [ThreadPool]: On create_memory_main_bronze: BEGIN
[0m11:36:07.050790 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:36:07.051620 [debug] [ThreadPool]: SQL status: OK in 0.002 seconds
[0m11:36:07.052040 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m11:36:07.052324 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_bronze"
[0m11:36:07.052612 [debug] [ThreadPool]: On create_memory_main_bronze: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_memory_main_bronze"} */

    
    
        create schema if not exists "memory"."main_bronze"
    
[0m11:36:07.053884 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_gold"
[0m11:36:07.054214 [debug] [ThreadPool]: On create_memory_main_gold: BEGIN
[0m11:36:07.054671 [debug] [ThreadPool]: SQL status: OK in 0.004 seconds
[0m11:36:07.055032 [debug] [ThreadPool]: SQL status: OK in 0.002 seconds
[0m11:36:07.055896 [debug] [ThreadPool]: On create_memory_main_bronze: COMMIT
[0m11:36:07.056229 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_bronze"
[0m11:36:07.056534 [debug] [ThreadPool]: On create_memory_main_bronze: COMMIT
[0m11:36:07.056930 [debug] [ThreadPool]: SQL status: OK in 0.002 seconds
[0m11:36:07.057314 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_gold"
[0m11:36:07.057592 [debug] [ThreadPool]: On create_memory_main_gold: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_memory_main_gold"} */

    
    
        create schema if not exists "memory"."main_gold"
    
[0m11:36:07.058803 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_silver"
[0m11:36:07.059088 [debug] [ThreadPool]: On create_memory_main_silver: BEGIN
[0m11:36:07.059643 [debug] [ThreadPool]: SQL status: OK in 0.003 seconds
[0m11:36:07.060178 [debug] [ThreadPool]: SQL status: OK in 0.002 seconds
[0m11:36:07.060813 [debug] [ThreadPool]: On create_memory_main_bronze: Close
[0m11:36:07.061884 [debug] [ThreadPool]: On create_memory_main_gold: COMMIT
[0m11:36:07.062243 [debug] [ThreadPool]: SQL status: OK in 0.003 seconds
[0m11:36:07.063136 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_gold"
[0m11:36:07.063507 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_silver"
[0m11:36:07.063884 [debug] [ThreadPool]: On create_memory_main_gold: COMMIT
[0m11:36:07.064342 [debug] [ThreadPool]: On create_memory_main_silver: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_memory_main_silver"} */

    
    
        create schema if not exists "memory"."main_silver"
    
[0m11:36:07.065127 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m11:36:07.065444 [debug] [ThreadPool]: On create_memory_main_gold: Close
[0m11:36:07.066038 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m11:36:07.066908 [debug] [ThreadPool]: On create_memory_main_silver: COMMIT
[0m11:36:07.067231 [debug] [ThreadPool]: Using duckdb connection "create_memory_main_silver"
[0m11:36:07.067529 [debug] [ThreadPool]: On create_memory_main_silver: COMMIT
[0m11:36:07.068031 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m11:36:07.068319 [debug] [ThreadPool]: On create_memory_main_silver: Close
[0m11:36:07.070614 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_memory_main_bronze'
[0m11:36:07.075852 [debug] [ThreadPool]: Using duckdb connection "list_memory_main_bronze"
[0m11:36:07.076363 [debug] [ThreadPool]: On list_memory_main_bronze: BEGIN
[0m11:36:07.077027 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_memory_main_silver'
[0m11:36:07.077573 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:36:07.080045 [debug] [ThreadPool]: Using duckdb connection "list_memory_main_silver"
[0m11:36:07.080859 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_memory_main_gold'
[0m11:36:07.081267 [debug] [ThreadPool]: On list_memory_main_silver: BEGIN
[0m11:36:07.081790 [debug] [ThreadPool]: SQL status: OK in 0.004 seconds
[0m11:36:07.083496 [debug] [ThreadPool]: Using duckdb connection "list_memory_main_gold"
[0m11:36:07.084010 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:36:07.084458 [debug] [ThreadPool]: Using duckdb connection "list_memory_main_bronze"
[0m11:36:07.084908 [debug] [ThreadPool]: On list_memory_main_gold: BEGIN
[0m11:36:07.085602 [debug] [ThreadPool]: On list_memory_main_bronze: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_memory_main_bronze"} */
select
      'memory' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main_bronze'
    and lower(table_catalog) = 'memory'
  
[0m11:36:07.086094 [debug] [ThreadPool]: SQL status: OK in 0.002 seconds
[0m11:36:07.086468 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:36:07.087155 [debug] [ThreadPool]: Using duckdb connection "list_memory_main_silver"
[0m11:36:07.087808 [debug] [ThreadPool]: On list_memory_main_silver: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_memory_main_silver"} */
select
      'memory' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main_silver'
    and lower(table_catalog) = 'memory'
  
[0m11:36:07.088424 [debug] [ThreadPool]: SQL status: OK in 0.002 seconds
[0m11:36:07.088806 [debug] [ThreadPool]: Using duckdb connection "list_memory_main_gold"
[0m11:36:07.089242 [debug] [ThreadPool]: On list_memory_main_gold: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_memory_main_gold"} */
select
      'memory' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main_gold'
    and lower(table_catalog) = 'memory'
  
[0m11:36:07.104226 [debug] [ThreadPool]: SQL status: OK in 0.015 seconds
[0m11:36:07.105700 [debug] [ThreadPool]: On list_memory_main_gold: ROLLBACK
[0m11:36:07.106161 [debug] [ThreadPool]: SQL status: OK in 0.019 seconds
[0m11:36:07.106509 [debug] [ThreadPool]: SQL status: OK in 0.018 seconds
[0m11:36:07.107503 [debug] [ThreadPool]: On list_memory_main_bronze: ROLLBACK
[0m11:36:07.108529 [debug] [ThreadPool]: On list_memory_main_silver: ROLLBACK
[0m11:36:07.110868 [debug] [ThreadPool]: Failed to rollback 'list_memory_main_gold'
[0m11:36:07.111371 [debug] [ThreadPool]: On list_memory_main_gold: Close
[0m11:36:07.112500 [debug] [ThreadPool]: Failed to rollback 'list_memory_main_bronze'
[0m11:36:07.112787 [debug] [ThreadPool]: On list_memory_main_bronze: Close
[0m11:36:07.113732 [debug] [ThreadPool]: Failed to rollback 'list_memory_main_silver'
[0m11:36:07.114003 [debug] [ThreadPool]: On list_memory_main_silver: Close
[0m11:36:07.114833 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd9c29e5b-3c65-4c17-9882-3a03c9b40e42', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B98FCB6B60>]}
[0m11:36:07.115232 [debug] [MainThread]: Using duckdb connection "master"
[0m11:36:07.115497 [debug] [MainThread]: On master: BEGIN
[0m11:36:07.115857 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:36:07.116437 [debug] [MainThread]: SQL status: OK in 0.001 seconds
[0m11:36:07.116710 [debug] [MainThread]: On master: COMMIT
[0m11:36:07.116974 [debug] [MainThread]: Using duckdb connection "master"
[0m11:36:07.117225 [debug] [MainThread]: On master: COMMIT
[0m11:36:07.117746 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m11:36:07.118018 [debug] [MainThread]: On master: Close
[0m11:36:07.122961 [debug] [Thread-1 (]: Began running node model.job_intelligent.stg_jobs_raw
[0m11:36:07.123469 [info ] [Thread-1 (]: 1 of 13 START sql view model main_bronze.stg_jobs_raw .......................... [RUN]
[0m11:36:07.123977 [debug] [Thread-1 (]: Acquiring new duckdb connection 'model.job_intelligent.stg_jobs_raw'
[0m11:36:07.124306 [debug] [Thread-1 (]: Began compiling node model.job_intelligent.stg_jobs_raw
[0m11:36:07.131612 [debug] [Thread-1 (]: Writing injected SQL for node "model.job_intelligent.stg_jobs_raw"
[0m11:36:07.132441 [debug] [Thread-1 (]: Began executing node model.job_intelligent.stg_jobs_raw
[0m11:36:07.163744 [debug] [Thread-1 (]: Writing runtime sql for node "model.job_intelligent.stg_jobs_raw"
[0m11:36:07.164675 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.stg_jobs_raw"
[0m11:36:07.165060 [debug] [Thread-1 (]: On model.job_intelligent.stg_jobs_raw: BEGIN
[0m11:36:07.165408 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m11:36:07.166246 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m11:36:07.166634 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.stg_jobs_raw"
[0m11:36:07.167053 [debug] [Thread-1 (]: On model.job_intelligent.stg_jobs_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.stg_jobs_raw"} */

  
  create view "memory"."main_bronze"."stg_jobs_raw__dbt_tmp" as (
    -- models/bronze/stg_jobs_raw.sql
-- Bronze layer: Lecture directe des données brutes depuis final_data.csv
-- Renommage et typage minimal



-- Read directly from CSV file
SELECT
    -- Renommer les colonnes pour cohérence
    title as job_title,
    location,
    postedTime as posted_time,
    publishedAt as published_at,
    jobUrl as job_url,
    companyName as company_name,
    companyUrl as company_url,
    description as job_description,
    contractType as contract_type,
    workType as work_type,
    
    -- Métadonnées de traçabilité
    NOW() as ingestion_timestamp,
    '2026-01-08 10:36:05.279724+00:00' as dbt_run_id

FROM read_csv_auto('D:/lab2/final_data.csv')

WHERE 1=1
    -- Filtrer les lignes vides
    AND title IS NOT NULL
    AND description IS NOT NULL
  );

[0m11:36:07.224166 [debug] [Thread-1 (]: SQL status: OK in 0.057 seconds
[0m11:36:07.232939 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.stg_jobs_raw"
[0m11:36:07.233573 [debug] [Thread-1 (]: On model.job_intelligent.stg_jobs_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.stg_jobs_raw"} */
alter view "memory"."main_bronze"."stg_jobs_raw__dbt_tmp" rename to "stg_jobs_raw"
[0m11:36:07.234642 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m11:36:07.247457 [debug] [Thread-1 (]: On model.job_intelligent.stg_jobs_raw: COMMIT
[0m11:36:07.247970 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.stg_jobs_raw"
[0m11:36:07.248371 [debug] [Thread-1 (]: On model.job_intelligent.stg_jobs_raw: COMMIT
[0m11:36:07.249109 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m11:36:07.254295 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.stg_jobs_raw"
[0m11:36:07.254729 [debug] [Thread-1 (]: On model.job_intelligent.stg_jobs_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.stg_jobs_raw"} */

      drop view if exists "memory"."main_bronze"."stg_jobs_raw__dbt_backup" cascade
    
[0m11:36:07.255460 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m11:36:07.257801 [debug] [Thread-1 (]: On model.job_intelligent.stg_jobs_raw: Close
[0m11:36:07.260218 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd9c29e5b-3c65-4c17-9882-3a03c9b40e42', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B98F5E93D0>]}
[0m11:36:07.261205 [info ] [Thread-1 (]: 1 of 13 OK created sql view model main_bronze.stg_jobs_raw ..................... [[32mOK[0m in 0.13s]
[0m11:36:07.261981 [debug] [Thread-1 (]: Finished running node model.job_intelligent.stg_jobs_raw
[0m11:36:07.262923 [debug] [Thread-2 (]: Began running node model.job_intelligent.int_jobs_cleaned
[0m11:36:07.263467 [info ] [Thread-2 (]: 2 of 13 START sql table model main_silver.int_jobs_cleaned ..................... [RUN]
[0m11:36:07.264093 [debug] [Thread-2 (]: Acquiring new duckdb connection 'model.job_intelligent.int_jobs_cleaned'
[0m11:36:07.264506 [debug] [Thread-2 (]: Began compiling node model.job_intelligent.int_jobs_cleaned
[0m11:36:07.268361 [debug] [Thread-2 (]: Writing injected SQL for node "model.job_intelligent.int_jobs_cleaned"
[0m11:36:07.269199 [debug] [Thread-2 (]: Began executing node model.job_intelligent.int_jobs_cleaned
[0m11:36:07.288817 [debug] [Thread-2 (]: Writing runtime sql for node "model.job_intelligent.int_jobs_cleaned"
[0m11:36:07.289745 [debug] [Thread-2 (]: Using duckdb connection "model.job_intelligent.int_jobs_cleaned"
[0m11:36:07.290148 [debug] [Thread-2 (]: On model.job_intelligent.int_jobs_cleaned: BEGIN
[0m11:36:07.290521 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m11:36:07.291253 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m11:36:07.291630 [debug] [Thread-2 (]: Using duckdb connection "model.job_intelligent.int_jobs_cleaned"
[0m11:36:07.292064 [debug] [Thread-2 (]: On model.job_intelligent.int_jobs_cleaned: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.int_jobs_cleaned"} */

  
    
    

    create  table
      "memory"."main_silver"."int_jobs_cleaned__dbt_tmp"
  
    as (
      -- models/silver/int_jobs_cleaned.sql
-- Silver layer: Nettoyage et normalisation des données brutes



WITH raw_jobs AS (
    SELECT * FROM "memory"."main_bronze"."stg_jobs_raw"
),

cleaned_jobs AS (
    SELECT
        -- Texte: lowercase, trim, remove special characters
        LOWER(TRIM(job_title)) as job_title_cleaned,
        LOWER(TRIM(location)) as location_cleaned,
        LOWER(TRIM(company_name)) as company_name_cleaned,
        LOWER(TRIM(job_description)) as job_description_cleaned,
        LOWER(TRIM(contract_type)) as contract_type_cleaned,
        LOWER(TRIM(work_type)) as work_type_cleaned,
        
        -- URLs as-is
        job_url,
        company_url,
        
        -- Dates
        TRY_CAST(published_at AS DATE) as published_date,
        posted_time,
        
        -- Extract year-month for time-based analysis
        DATE_TRUNC('month', TRY_CAST(published_at AS DATE)) as published_year_month,
        EXTRACT(YEAR FROM TRY_CAST(published_at AS DATE)) as published_year,
        EXTRACT(MONTH FROM TRY_CAST(published_at AS DATE)) as published_month,
        
        -- Métadonnées
        ingestion_timestamp
    FROM raw_jobs
)

SELECT
    *,
    -- Deduplication flag
    ROW_NUMBER() OVER (
        PARTITION BY job_title_cleaned, company_name_cleaned, location_cleaned 
        ORDER BY published_date DESC
    ) as dedup_rank
FROM cleaned_jobs
    );
  
  
[0m11:36:07.433156 [debug] [Thread-2 (]: SQL status: OK in 0.141 seconds
[0m11:36:07.436014 [debug] [Thread-2 (]: Using duckdb connection "model.job_intelligent.int_jobs_cleaned"
[0m11:36:07.436369 [debug] [Thread-2 (]: On model.job_intelligent.int_jobs_cleaned: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.int_jobs_cleaned"} */
alter table "memory"."main_silver"."int_jobs_cleaned__dbt_tmp" rename to "int_jobs_cleaned"
[0m11:36:07.437067 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m11:36:07.442341 [debug] [Thread-2 (]: On model.job_intelligent.int_jobs_cleaned: COMMIT
[0m11:36:07.443089 [debug] [Thread-2 (]: Using duckdb connection "model.job_intelligent.int_jobs_cleaned"
[0m11:36:07.443570 [debug] [Thread-2 (]: On model.job_intelligent.int_jobs_cleaned: COMMIT
[0m11:36:07.447352 [debug] [Thread-2 (]: SQL status: OK in 0.003 seconds
[0m11:36:07.449501 [debug] [Thread-2 (]: Using duckdb connection "model.job_intelligent.int_jobs_cleaned"
[0m11:36:07.449907 [debug] [Thread-2 (]: On model.job_intelligent.int_jobs_cleaned: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.int_jobs_cleaned"} */

      drop table if exists "memory"."main_silver"."int_jobs_cleaned__dbt_backup" cascade
    
[0m11:36:07.450618 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m11:36:07.451923 [debug] [Thread-2 (]: On model.job_intelligent.int_jobs_cleaned: Close
[0m11:36:07.452481 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd9c29e5b-3c65-4c17-9882-3a03c9b40e42', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B98FB14100>]}
[0m11:36:07.453115 [info ] [Thread-2 (]: 2 of 13 OK created sql table model main_silver.int_jobs_cleaned ................ [[32mOK[0m in 0.19s]
[0m11:36:07.453795 [debug] [Thread-2 (]: Finished running node model.job_intelligent.int_jobs_cleaned
[0m11:36:07.454683 [debug] [Thread-3 (]: Began running node model.job_intelligent.int_job_title_normalization
[0m11:36:07.455223 [info ] [Thread-3 (]: 3 of 13 START sql table model main_silver.int_job_title_normalization .......... [RUN]
[0m11:36:07.455826 [debug] [Thread-3 (]: Acquiring new duckdb connection 'model.job_intelligent.int_job_title_normalization'
[0m11:36:07.456415 [debug] [Thread-3 (]: Began compiling node model.job_intelligent.int_job_title_normalization
[0m11:36:07.460841 [debug] [Thread-3 (]: Writing injected SQL for node "model.job_intelligent.int_job_title_normalization"
[0m11:36:07.461976 [debug] [Thread-3 (]: Began executing node model.job_intelligent.int_job_title_normalization
[0m11:36:07.465629 [debug] [Thread-3 (]: Writing runtime sql for node "model.job_intelligent.int_job_title_normalization"
[0m11:36:07.466633 [debug] [Thread-3 (]: Using duckdb connection "model.job_intelligent.int_job_title_normalization"
[0m11:36:07.467127 [debug] [Thread-3 (]: On model.job_intelligent.int_job_title_normalization: BEGIN
[0m11:36:07.467576 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m11:36:07.468471 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m11:36:07.468934 [debug] [Thread-3 (]: Using duckdb connection "model.job_intelligent.int_job_title_normalization"
[0m11:36:07.469540 [debug] [Thread-3 (]: On model.job_intelligent.int_job_title_normalization: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.int_job_title_normalization"} */

  
    
    

    create  table
      "memory"."main_silver"."int_job_title_normalization__dbt_tmp"
  
    as (
      -- models/silver/int_job_title_normalization.sql
-- Silver layer: Normaliser les intitulés de postes



WITH cleaned_jobs AS (
    SELECT * FROM "memory"."main_silver"."int_jobs_cleaned"
),

title_normalized AS (
    SELECT
        *,
        CASE
            WHEN job_title_cleaned LIKE '%data engineer%' THEN 'Data Engineer'
            WHEN job_title_cleaned LIKE '%data scientist%' THEN 'Data Scientist'
            WHEN job_title_cleaned LIKE '%data analyst%' THEN 'Data Analyst'
            WHEN job_title_cleaned LIKE '%analytics engineer%' THEN 'Analytics Engineer'
            WHEN job_title_cleaned LIKE '%ml engineer%' OR job_title_cleaned LIKE '%machine learning%' THEN 'ML Engineer'
            WHEN job_title_cleaned LIKE '%data architect%' THEN 'Data Architect'
            WHEN job_title_cleaned LIKE '%bi developer%' OR job_title_cleaned LIKE '%business intelligence%' THEN 'BI Developer'
            WHEN job_title_cleaned LIKE '%etl%' OR job_title_cleaned LIKE '%pipeline%' THEN 'ETL/Pipeline Engineer'
            WHEN job_title_cleaned LIKE '%data engineer%' THEN 'Data Engineer'
            ELSE 'Other Data Role'
        END as job_category,
        
        CASE
            WHEN contract_type_cleaned LIKE '%cdi%' OR contract_type_cleaned LIKE '%permanent%' THEN 'Permanent'
            WHEN contract_type_cleaned LIKE '%cdd%' OR contract_type_cleaned LIKE '%contract%' THEN 'Contract'
            WHEN contract_type_cleaned LIKE '%stage%' OR contract_type_cleaned LIKE '%internship%' THEN 'Internship'
            WHEN contract_type_cleaned LIKE '%freelance%' THEN 'Freelance'
            ELSE 'Not Specified'
        END as contract_type_normalized,
        
        CASE
            WHEN work_type_cleaned LIKE '%remote%' THEN 'Remote'
            WHEN work_type_cleaned LIKE '%hybrid%' THEN 'Hybrid'
            WHEN work_type_cleaned LIKE '%onsite%' OR work_type_cleaned LIKE '%on-site%' THEN 'On-site'
            ELSE 'Not Specified'
        END as work_type_normalized
    
    FROM cleaned_jobs
    WHERE dedup_rank = 1  -- Keep only first occurrence (most recent)
)

SELECT * FROM title_normalized
    );
  
  
[0m11:36:07.502116 [debug] [Thread-3 (]: SQL status: OK in 0.032 seconds
[0m11:36:07.505244 [debug] [Thread-3 (]: Using duckdb connection "model.job_intelligent.int_job_title_normalization"
[0m11:36:07.505684 [debug] [Thread-3 (]: On model.job_intelligent.int_job_title_normalization: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.int_job_title_normalization"} */
alter table "memory"."main_silver"."int_job_title_normalization__dbt_tmp" rename to "int_job_title_normalization"
[0m11:36:07.506535 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m11:36:07.508105 [debug] [Thread-3 (]: On model.job_intelligent.int_job_title_normalization: COMMIT
[0m11:36:07.508527 [debug] [Thread-3 (]: Using duckdb connection "model.job_intelligent.int_job_title_normalization"
[0m11:36:07.508923 [debug] [Thread-3 (]: On model.job_intelligent.int_job_title_normalization: COMMIT
[0m11:36:07.509651 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m11:36:07.512480 [debug] [Thread-3 (]: Using duckdb connection "model.job_intelligent.int_job_title_normalization"
[0m11:36:07.512931 [debug] [Thread-3 (]: On model.job_intelligent.int_job_title_normalization: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.int_job_title_normalization"} */

      drop table if exists "memory"."main_silver"."int_job_title_normalization__dbt_backup" cascade
    
[0m11:36:07.513700 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m11:36:07.515103 [debug] [Thread-3 (]: On model.job_intelligent.int_job_title_normalization: Close
[0m11:36:07.515663 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd9c29e5b-3c65-4c17-9882-3a03c9b40e42', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B98F5713B0>]}
[0m11:36:07.516331 [info ] [Thread-3 (]: 3 of 13 OK created sql table model main_silver.int_job_title_normalization ..... [[32mOK[0m in 0.06s]
[0m11:36:07.517025 [debug] [Thread-3 (]: Finished running node model.job_intelligent.int_job_title_normalization
[0m11:36:07.517878 [debug] [Thread-2 (]: Began running node model.job_intelligent.dim_time
[0m11:36:07.518440 [info ] [Thread-2 (]: 6 of 13 START sql table model main_gold.dim_time ............................... [RUN]
[0m11:36:07.519013 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly model.job_intelligent.int_jobs_cleaned, now model.job_intelligent.dim_time)
[0m11:36:07.519706 [debug] [Thread-4 (]: Began running node model.job_intelligent.dim_company
[0m11:36:07.520243 [info ] [Thread-4 (]: 4 of 13 START sql table model main_gold.dim_company ............................ [RUN]
[0m11:36:07.520913 [debug] [Thread-4 (]: Acquiring new duckdb connection 'model.job_intelligent.dim_company'
[0m11:36:07.521369 [debug] [Thread-4 (]: Began compiling node model.job_intelligent.dim_company
[0m11:36:07.525047 [debug] [Thread-4 (]: Writing injected SQL for node "model.job_intelligent.dim_company"
[0m11:36:07.519448 [debug] [Thread-2 (]: Began compiling node model.job_intelligent.dim_time
[0m11:36:07.526182 [debug] [Thread-3 (]: Began running node model.job_intelligent.int_skills_extraction
[0m11:36:07.526833 [debug] [Thread-1 (]: Began running node model.job_intelligent.dim_location
[0m11:36:07.532026 [debug] [Thread-2 (]: Writing injected SQL for node "model.job_intelligent.dim_time"
[0m11:36:07.533130 [info ] [Thread-3 (]: 7 of 13 START sql table model main_silver.int_skills_extraction ................ [RUN]
[0m11:36:07.533871 [info ] [Thread-1 (]: 5 of 13 START sql table model main_gold.dim_location ........................... [RUN]
[0m11:36:07.534428 [debug] [Thread-4 (]: Began executing node model.job_intelligent.dim_company
[0m11:36:07.535135 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly model.job_intelligent.int_job_title_normalization, now model.job_intelligent.int_skills_extraction)
[0m11:36:07.535719 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.job_intelligent.stg_jobs_raw, now model.job_intelligent.dim_location)
[0m11:36:07.539168 [debug] [Thread-4 (]: Writing runtime sql for node "model.job_intelligent.dim_company"
[0m11:36:07.539940 [debug] [Thread-3 (]: Began compiling node model.job_intelligent.int_skills_extraction
[0m11:36:07.540566 [debug] [Thread-1 (]: Began compiling node model.job_intelligent.dim_location
[0m11:36:07.541249 [debug] [Thread-2 (]: Began executing node model.job_intelligent.dim_time
[0m11:36:07.545700 [debug] [Thread-3 (]: Writing injected SQL for node "model.job_intelligent.int_skills_extraction"
[0m11:36:07.550367 [debug] [Thread-1 (]: Writing injected SQL for node "model.job_intelligent.dim_location"
[0m11:36:07.551956 [debug] [Thread-4 (]: Using duckdb connection "model.job_intelligent.dim_company"
[0m11:36:07.554933 [debug] [Thread-2 (]: Writing runtime sql for node "model.job_intelligent.dim_time"
[0m11:36:07.555712 [debug] [Thread-4 (]: On model.job_intelligent.dim_company: BEGIN
[0m11:36:07.556354 [debug] [Thread-3 (]: Began executing node model.job_intelligent.int_skills_extraction
[0m11:36:07.557145 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m11:36:07.557809 [debug] [Thread-1 (]: Began executing node model.job_intelligent.dim_location
[0m11:36:07.562903 [debug] [Thread-3 (]: Writing runtime sql for node "model.job_intelligent.int_skills_extraction"
[0m11:36:07.564206 [debug] [Thread-2 (]: Using duckdb connection "model.job_intelligent.dim_time"
[0m11:36:07.568303 [debug] [Thread-1 (]: Writing runtime sql for node "model.job_intelligent.dim_location"
[0m11:36:07.569167 [debug] [Thread-4 (]: SQL status: OK in 0.012 seconds
[0m11:36:07.569811 [debug] [Thread-2 (]: On model.job_intelligent.dim_time: BEGIN
[0m11:36:07.570742 [debug] [Thread-3 (]: Using duckdb connection "model.job_intelligent.int_skills_extraction"
[0m11:36:07.571380 [debug] [Thread-4 (]: Using duckdb connection "model.job_intelligent.dim_company"
[0m11:36:07.572070 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m11:36:07.572718 [debug] [Thread-3 (]: On model.job_intelligent.int_skills_extraction: BEGIN
[0m11:36:07.573489 [debug] [Thread-4 (]: On model.job_intelligent.dim_company: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_company"} */

  
    
    

    create  table
      "memory"."main_gold"."dim_company__dbt_tmp"
  
    as (
      -- models/gold/dim_company.sql
-- Gold layer: Dimension Company



WITH jobs AS (
    SELECT DISTINCT
        company_name_cleaned,
        company_url
    FROM "memory"."main_silver"."int_job_title_normalization"
    WHERE company_name_cleaned IS NOT NULL
),

ranked_companies AS (
    SELECT
        ROW_NUMBER() OVER (ORDER BY company_name_cleaned) as company_id,
        company_name_cleaned as company_name,
        company_url,
        NOW() as created_at
    FROM jobs
)

SELECT
    company_id,
    company_name,
    company_url,
    created_at
FROM ranked_companies
ORDER BY company_id
    );
  
  
[0m11:36:07.574571 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m11:36:07.575317 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.dim_location"
[0m11:36:07.576386 [debug] [Thread-2 (]: SQL status: OK in 0.004 seconds
[0m11:36:07.577765 [debug] [Thread-1 (]: On model.job_intelligent.dim_location: BEGIN
[0m11:36:07.578558 [debug] [Thread-3 (]: SQL status: OK in 0.004 seconds
[0m11:36:07.579322 [debug] [Thread-2 (]: Using duckdb connection "model.job_intelligent.dim_time"
[0m11:36:07.580164 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:36:07.580904 [debug] [Thread-3 (]: Using duckdb connection "model.job_intelligent.int_skills_extraction"
[0m11:36:07.581913 [debug] [Thread-2 (]: On model.job_intelligent.dim_time: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_time"} */

  
    
    

    create  table
      "memory"."main_gold"."dim_time__dbt_tmp"
  
    as (
      -- models/gold/dim_time.sql
-- Gold layer: Dimension Time



-- Generate a date dimension for time-based analysis
WITH date_spine AS (
    SELECT
        published_date,
        EXTRACT(YEAR FROM published_date) as year,
        EXTRACT(MONTH FROM published_date) as month,
        EXTRACT(QUARTER FROM published_date) as quarter,
        EXTRACT(WEEK FROM published_date) as week,
        EXTRACT(DAYOFWEEK FROM published_date) as day_of_week,
        DATE_TRUNC('month', published_date) as month_start,
        DATE_TRUNC('quarter', published_date) as quarter_start,
        DATE_TRUNC('year', published_date) as year_start,
        
        CASE 
            WHEN EXTRACT(MONTH FROM published_date) IN (1,2,3) THEN 'Q1'
            WHEN EXTRACT(MONTH FROM published_date) IN (4,5,6) THEN 'Q2'
            WHEN EXTRACT(MONTH FROM published_date) IN (7,8,9) THEN 'Q3'
            ELSE 'Q4'
        END as quarter_name,
        
        CASE EXTRACT(DAYOFWEEK FROM published_date)
            WHEN 0 THEN 'Sunday'
            WHEN 1 THEN 'Monday'
            WHEN 2 THEN 'Tuesday'
            WHEN 3 THEN 'Wednesday'
            WHEN 4 THEN 'Thursday'
            WHEN 5 THEN 'Friday'
            WHEN 6 THEN 'Saturday'
        END as day_name,
        
        CASE EXTRACT(MONTH FROM published_date)
            WHEN 1 THEN 'January'
            WHEN 2 THEN 'February'
            WHEN 3 THEN 'March'
            WHEN 4 THEN 'April'
            WHEN 5 THEN 'May'
            WHEN 6 THEN 'June'
            WHEN 7 THEN 'July'
            WHEN 8 THEN 'August'
            WHEN 9 THEN 'September'
            WHEN 10 THEN 'October'
            WHEN 11 THEN 'November'
            WHEN 12 THEN 'December'
        END as month_name
        
    FROM (
        SELECT DISTINCT published_date
        FROM "memory"."main_silver"."int_job_title_normalization"
        WHERE published_date IS NOT NULL
    )
)

SELECT
    published_date as date_id,
    year,
    month,
    quarter,
    week,
    day_of_week,
    day_name,
    month_name,
    quarter_name,
    month_start,
    quarter_start,
    year_start,
    NOW() as created_at
FROM date_spine
ORDER BY published_date
    );
  
  
[0m11:36:07.583303 [debug] [Thread-3 (]: On model.job_intelligent.int_skills_extraction: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.int_skills_extraction"} */

  
    
    

    create  table
      "memory"."main_silver"."int_skills_extraction__dbt_tmp"
  
    as (
      -- models/silver/int_skills_extraction.sql
-- Silver layer: Extraction des compétences depuis la description



WITH jobs_with_titles AS (
    SELECT * FROM "memory"."main_silver"."int_job_title_normalization"
),

skills_mapping AS (
    -- Définir un mapping de compétences communes Data/Tech
    SELECT
        'Python' as skill_name,
        'python|py |\.py' as skill_pattern
    UNION ALL SELECT 'SQL', 'sql|sql|sql server|postgres|oracle'
    UNION ALL SELECT 'Spark', 'spark|pyspark'
    UNION ALL SELECT 'Hadoop', 'hadoop|hdfs'
    UNION ALL SELECT 'Scala', 'scala'
    UNION ALL SELECT 'Java', '\bjava\b'
    UNION ALL SELECT 'R', '\br\b|r programming'
    UNION ALL SELECT 'Tableau', 'tableau'
    UNION ALL SELECT 'Power BI', 'power bi|powerbi'
    UNION ALL SELECT 'Looker', 'looker'
    UNION ALL SELECT 'AWS', 'aws|amazon web|s3 |ec2|redshift'
    UNION ALL SELECT 'Azure', 'azure|microsoft azure|synapse|cosmos'
    UNION ALL SELECT 'GCP', 'gcp|google cloud|bigquery'
    UNION ALL SELECT 'Airflow', 'airflow'
    UNION ALL SELECT 'DBT', '\bdbt\b|dbt'
    UNION ALL SELECT 'Kubernetes', 'kubernetes|k8s'
    UNION ALL SELECT 'Docker', 'docker'
    UNION ALL SELECT 'Git', 'git|github|gitlab'
    UNION ALL SELECT 'TensorFlow', 'tensorflow'
    UNION ALL SELECT 'PyTorch', 'pytorch'
    UNION ALL SELECT 'Scikit-learn', 'scikit|sklearn'
    UNION ALL SELECT 'Pandas', 'pandas'
    UNION ALL SELECT 'NumPy', 'numpy'
    UNION ALL SELECT 'Machine Learning', 'machine learning|deep learning|ml|artificial intelligence'
    UNION ALL SELECT 'Statistics', 'statistics|statistical|probability'
    UNION ALL SELECT 'Data Visualization', 'data visualization|visualization|charts|graphs'
),

jobs_exploded AS (
    SELECT
        j.*,
        s.skill_name,
        CASE 
            WHEN job_description_cleaned ILIKE '%' || s.skill_pattern || '%' THEN 1 
            ELSE 0 
        END as has_skill
    FROM jobs_with_titles j
    CROSS JOIN skills_mapping s
)

SELECT
    *
FROM jobs_exploded
WHERE has_skill = 1

ORDER BY job_title_cleaned, published_date DESC
    );
  
  
[0m11:36:07.584147 [debug] [Thread-1 (]: SQL status: OK in 0.004 seconds
[0m11:36:07.585882 [debug] [Thread-4 (]: SQL status: OK in 0.010 seconds
[0m11:36:07.586535 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.dim_location"
[0m11:36:07.591427 [debug] [Thread-4 (]: Using duckdb connection "model.job_intelligent.dim_company"
[0m11:36:07.592589 [debug] [Thread-1 (]: On model.job_intelligent.dim_location: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_location"} */

  
    
    

    create  table
      "memory"."main_gold"."dim_location__dbt_tmp"
  
    as (
      -- models/gold/dim_location.sql
-- Gold layer: Dimension Location



WITH jobs AS (
    SELECT DISTINCT
        location_cleaned as location_raw
    FROM "memory"."main_silver"."int_job_title_normalization"
    WHERE location_cleaned IS NOT NULL
),

location_parsed AS (
    SELECT
        location_raw,
        -- Extract city (before comma)
        CASE 
            WHEN POSITION(',' IN location_raw) > 0 
            THEN TRIM(SUBSTRING(location_raw, 1, POSITION(',' IN location_raw) - 1))
            ELSE location_raw
        END as city,
        
        -- Extract country (after comma)
        CASE 
            WHEN POSITION(',' IN location_raw) > 0 
            THEN TRIM(SUBSTRING(location_raw, POSITION(',' IN location_raw) + 1))
            ELSE 'Not Specified'
        END as country,
        
        -- Detect if remote
        CASE 
            WHEN location_raw LIKE '%remote%' 
            THEN 'Remote'
            ELSE 'On-site'
        END as work_location_type
    FROM jobs
),

ranked_locations AS (
    SELECT
        ROW_NUMBER() OVER (ORDER BY location_raw) as location_id,
        location_raw,
        city,
        country,
        work_location_type,
        NOW() as created_at
    FROM location_parsed
)

SELECT
    location_id,
    location_raw,
    city,
    country,
    work_location_type,
    created_at
FROM ranked_locations
ORDER BY location_id
    );
  
  
[0m11:36:07.594702 [debug] [Thread-4 (]: On model.job_intelligent.dim_company: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_company"} */
alter table "memory"."main_gold"."dim_company__dbt_tmp" rename to "dim_company"
[0m11:36:07.596193 [debug] [Thread-4 (]: SQL status: OK in 0.001 seconds
[0m11:36:07.598623 [debug] [Thread-4 (]: On model.job_intelligent.dim_company: COMMIT
[0m11:36:07.599532 [debug] [Thread-2 (]: SQL status: OK in 0.015 seconds
[0m11:36:07.600608 [debug] [Thread-4 (]: Using duckdb connection "model.job_intelligent.dim_company"
[0m11:36:07.604774 [debug] [Thread-2 (]: Using duckdb connection "model.job_intelligent.dim_time"
[0m11:36:07.605449 [debug] [Thread-4 (]: On model.job_intelligent.dim_company: COMMIT
[0m11:36:07.606006 [debug] [Thread-1 (]: SQL status: OK in 0.011 seconds
[0m11:36:07.606616 [debug] [Thread-2 (]: On model.job_intelligent.dim_time: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_time"} */
alter table "memory"."main_gold"."dim_time__dbt_tmp" rename to "dim_time"
[0m11:36:07.610380 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.dim_location"
[0m11:36:07.611433 [debug] [Thread-4 (]: SQL status: OK in 0.004 seconds
[0m11:36:07.612010 [debug] [Thread-1 (]: On model.job_intelligent.dim_location: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_location"} */
alter table "memory"."main_gold"."dim_location__dbt_tmp" rename to "dim_location"
[0m11:36:07.614866 [debug] [Thread-4 (]: Using duckdb connection "model.job_intelligent.dim_company"
[0m11:36:07.615469 [debug] [Thread-2 (]: SQL status: OK in 0.004 seconds
[0m11:36:07.616185 [debug] [Thread-4 (]: On model.job_intelligent.dim_company: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_company"} */

      drop table if exists "memory"."main_gold"."dim_company__dbt_backup" cascade
    
[0m11:36:07.616749 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m11:36:07.618395 [debug] [Thread-2 (]: On model.job_intelligent.dim_time: COMMIT
[0m11:36:07.620226 [debug] [Thread-1 (]: On model.job_intelligent.dim_location: COMMIT
[0m11:36:07.620782 [debug] [Thread-4 (]: SQL status: OK in 0.002 seconds
[0m11:36:07.621225 [debug] [Thread-2 (]: Using duckdb connection "model.job_intelligent.dim_time"
[0m11:36:07.621746 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.dim_location"
[0m11:36:07.623147 [debug] [Thread-4 (]: On model.job_intelligent.dim_company: Close
[0m11:36:07.623658 [debug] [Thread-2 (]: On model.job_intelligent.dim_time: COMMIT
[0m11:36:07.624166 [debug] [Thread-1 (]: On model.job_intelligent.dim_location: COMMIT
[0m11:36:07.624844 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd9c29e5b-3c65-4c17-9882-3a03c9b40e42', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B98FD5ACD0>]}
[0m11:36:07.626753 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m11:36:07.629170 [debug] [Thread-2 (]: Using duckdb connection "model.job_intelligent.dim_time"
[0m11:36:07.629581 [debug] [Thread-2 (]: On model.job_intelligent.dim_time: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_time"} */

      drop table if exists "memory"."main_gold"."dim_time__dbt_backup" cascade
    
[0m11:36:07.626185 [info ] [Thread-4 (]: 4 of 13 OK created sql table model main_gold.dim_company ....................... [[32mOK[0m in 0.10s]
[0m11:36:07.630572 [debug] [Thread-4 (]: Finished running node model.job_intelligent.dim_company
[0m11:36:07.631061 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m11:36:07.632420 [debug] [Thread-2 (]: On model.job_intelligent.dim_time: Close
[0m11:36:07.632878 [debug] [Thread-1 (]: SQL status: OK in 0.007 seconds
[0m11:36:07.635232 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.dim_location"
[0m11:36:07.635624 [debug] [Thread-1 (]: On model.job_intelligent.dim_location: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_location"} */

      drop table if exists "memory"."main_gold"."dim_location__dbt_backup" cascade
    
[0m11:36:07.636159 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd9c29e5b-3c65-4c17-9882-3a03c9b40e42', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B98FE99E80>]}
[0m11:36:07.636780 [info ] [Thread-2 (]: 6 of 13 OK created sql table model main_gold.dim_time .......................... [[32mOK[0m in 0.12s]
[0m11:36:07.637424 [debug] [Thread-2 (]: Finished running node model.job_intelligent.dim_time
[0m11:36:07.638320 [debug] [Thread-1 (]: SQL status: OK in 0.002 seconds
[0m11:36:07.639559 [debug] [Thread-1 (]: On model.job_intelligent.dim_location: Close
[0m11:36:07.640031 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd9c29e5b-3c65-4c17-9882-3a03c9b40e42', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B98FEAF9B0>]}
[0m11:36:07.640561 [info ] [Thread-1 (]: 5 of 13 OK created sql table model main_gold.dim_location ...................... [[32mOK[0m in 0.10s]
[0m11:36:07.641160 [debug] [Thread-1 (]: Finished running node model.job_intelligent.dim_location
[0m11:36:07.641852 [debug] [Thread-4 (]: Began running node model.job_intelligent.fact_job_offers
[0m11:36:07.642380 [info ] [Thread-4 (]: 8 of 13 START sql table model main_gold.fact_job_offers ........................ [RUN]
[0m11:36:07.642881 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly model.job_intelligent.dim_company, now model.job_intelligent.fact_job_offers)
[0m11:36:07.643320 [debug] [Thread-4 (]: Began compiling node model.job_intelligent.fact_job_offers
[0m11:36:07.647165 [debug] [Thread-4 (]: Writing injected SQL for node "model.job_intelligent.fact_job_offers"
[0m11:36:07.648052 [debug] [Thread-4 (]: Began executing node model.job_intelligent.fact_job_offers
[0m11:36:07.650612 [debug] [Thread-4 (]: Writing runtime sql for node "model.job_intelligent.fact_job_offers"
[0m11:36:07.651449 [debug] [Thread-4 (]: Using duckdb connection "model.job_intelligent.fact_job_offers"
[0m11:36:07.651812 [debug] [Thread-4 (]: On model.job_intelligent.fact_job_offers: BEGIN
[0m11:36:07.652132 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m11:36:07.652772 [debug] [Thread-4 (]: SQL status: OK in 0.001 seconds
[0m11:36:07.653095 [debug] [Thread-4 (]: Using duckdb connection "model.job_intelligent.fact_job_offers"
[0m11:36:07.653507 [debug] [Thread-4 (]: On model.job_intelligent.fact_job_offers: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.fact_job_offers"} */

  
    
    

    create  table
      "memory"."main_gold"."fact_job_offers__dbt_tmp"
  
    as (
      -- models/gold/fact_job_offers.sql
-- Gold layer: Fact Table - Job Offers (Schéma en Étoile)



WITH jobs AS (
    SELECT
        j.*
    FROM "memory"."main_silver"."int_job_title_normalization" j
),

companies AS (
    SELECT * FROM "memory"."main_gold"."dim_company"
),

locations AS (
    SELECT * FROM "memory"."main_gold"."dim_location"
),

times AS (
    SELECT * FROM "memory"."main_gold"."dim_time"
),

fact_table AS (
    SELECT
        -- Surrogate keys
        ROW_NUMBER() OVER (ORDER BY j.job_url, j.company_name_cleaned) as job_offer_id,
        
        -- Foreign keys
        c.company_id,
        l.location_id,
        t.date_id as published_date_id,
        
        -- Job dimensions
        j.job_title_cleaned as job_title,
        j.job_category,
        j.contract_type_normalized as contract_type,
        j.work_type_normalized as work_type,
        
        -- URLs
        j.job_url,
        j.company_url,
        
        -- Description
        j.job_description_cleaned as job_description,
        
        -- Time dimension
        j.published_date,
        j.posted_time,
        j.published_year_month,
        j.published_year,
        j.published_month,
        
        -- Metrics
        LENGTH(j.job_description_cleaned) as description_length,
        (LENGTH(j.job_description_cleaned) - LENGTH(REPLACE(j.job_description_cleaned, ' ', ''))) + 1 as word_count,
        
        -- Flags
        CASE WHEN j.work_type_normalized = 'Remote' THEN 1 ELSE 0 END as is_remote,
        CASE WHEN j.contract_type_normalized = 'Permanent' THEN 1 ELSE 0 END as is_permanent,
        
        -- Metadata
        NOW() as created_at,
        j.ingestion_timestamp
        
    FROM jobs j
    LEFT JOIN companies c ON j.company_name_cleaned = c.company_name
    LEFT JOIN locations l ON j.location_cleaned = l.location_raw
    LEFT JOIN times t ON j.published_date = t.date_id
)

SELECT
    job_offer_id,
    company_id,
    location_id,
    published_date_id,
    job_title,
    job_category,
    contract_type,
    work_type,
    job_url,
    company_url,
    job_description,
    published_date,
    posted_time,
    published_year_month,
    published_year,
    published_month,
    description_length,
    word_count,
    is_remote,
    is_permanent,
    created_at,
    ingestion_timestamp
FROM fact_table
ORDER BY published_date DESC, job_offer_id
    );
  
  
[0m11:36:07.769688 [debug] [Thread-4 (]: SQL status: OK in 0.116 seconds
[0m11:36:07.772418 [debug] [Thread-4 (]: Using duckdb connection "model.job_intelligent.fact_job_offers"
[0m11:36:07.772766 [debug] [Thread-4 (]: On model.job_intelligent.fact_job_offers: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.fact_job_offers"} */
alter table "memory"."main_gold"."fact_job_offers__dbt_tmp" rename to "fact_job_offers"
[0m11:36:07.773461 [debug] [Thread-4 (]: SQL status: OK in 0.000 seconds
[0m11:36:07.774734 [debug] [Thread-4 (]: On model.job_intelligent.fact_job_offers: COMMIT
[0m11:36:07.775068 [debug] [Thread-4 (]: Using duckdb connection "model.job_intelligent.fact_job_offers"
[0m11:36:07.775382 [debug] [Thread-4 (]: On model.job_intelligent.fact_job_offers: COMMIT
[0m11:36:07.776107 [debug] [Thread-4 (]: SQL status: OK in 0.000 seconds
[0m11:36:07.778779 [debug] [Thread-4 (]: Using duckdb connection "model.job_intelligent.fact_job_offers"
[0m11:36:07.779143 [debug] [Thread-4 (]: On model.job_intelligent.fact_job_offers: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.fact_job_offers"} */

      drop table if exists "memory"."main_gold"."fact_job_offers__dbt_backup" cascade
    
[0m11:36:07.779815 [debug] [Thread-4 (]: SQL status: OK in 0.000 seconds
[0m11:36:07.780948 [debug] [Thread-4 (]: On model.job_intelligent.fact_job_offers: Close
[0m11:36:07.781417 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd9c29e5b-3c65-4c17-9882-3a03c9b40e42', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B990F40890>]}
[0m11:36:07.781950 [info ] [Thread-4 (]: 8 of 13 OK created sql table model main_gold.fact_job_offers ................... [[32mOK[0m in 0.14s]
[0m11:36:07.782552 [debug] [Thread-4 (]: Finished running node model.job_intelligent.fact_job_offers
[0m11:36:07.783267 [debug] [Thread-2 (]: Began running node model.job_intelligent.agg_job_offers_by_category_time
[0m11:36:07.783734 [info ] [Thread-2 (]: 9 of 13 START sql table model main_gold.agg_job_offers_by_category_time ........ [RUN]
[0m11:36:07.784419 [debug] [Thread-1 (]: Began running node model.job_intelligent.agg_location_analysis
[0m11:36:07.784719 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly model.job_intelligent.dim_time, now model.job_intelligent.agg_job_offers_by_category_time)
[0m11:36:07.785526 [info ] [Thread-1 (]: 10 of 13 START sql table model main_gold.agg_location_analysis ................. [RUN]
[0m11:36:07.786298 [debug] [Thread-2 (]: Began compiling node model.job_intelligent.agg_job_offers_by_category_time
[0m11:36:07.786838 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.job_intelligent.dim_location, now model.job_intelligent.agg_location_analysis)
[0m11:36:07.793015 [debug] [Thread-2 (]: Writing injected SQL for node "model.job_intelligent.agg_job_offers_by_category_time"
[0m11:36:07.793812 [debug] [Thread-1 (]: Began compiling node model.job_intelligent.agg_location_analysis
[0m11:36:07.797116 [debug] [Thread-1 (]: Writing injected SQL for node "model.job_intelligent.agg_location_analysis"
[0m11:36:07.797984 [debug] [Thread-2 (]: Began executing node model.job_intelligent.agg_job_offers_by_category_time
[0m11:36:07.798389 [debug] [Thread-1 (]: Began executing node model.job_intelligent.agg_location_analysis
[0m11:36:07.800999 [debug] [Thread-2 (]: Writing runtime sql for node "model.job_intelligent.agg_job_offers_by_category_time"
[0m11:36:07.803670 [debug] [Thread-1 (]: Writing runtime sql for node "model.job_intelligent.agg_location_analysis"
[0m11:36:07.804749 [debug] [Thread-2 (]: Using duckdb connection "model.job_intelligent.agg_job_offers_by_category_time"
[0m11:36:07.805340 [debug] [Thread-2 (]: On model.job_intelligent.agg_job_offers_by_category_time: BEGIN
[0m11:36:07.805855 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.agg_location_analysis"
[0m11:36:07.806393 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m11:36:07.806827 [debug] [Thread-1 (]: On model.job_intelligent.agg_location_analysis: BEGIN
[0m11:36:07.807667 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:36:07.808395 [debug] [Thread-2 (]: SQL status: OK in 0.002 seconds
[0m11:36:07.808759 [debug] [Thread-2 (]: Using duckdb connection "model.job_intelligent.agg_job_offers_by_category_time"
[0m11:36:07.809208 [debug] [Thread-2 (]: On model.job_intelligent.agg_job_offers_by_category_time: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.agg_job_offers_by_category_time"} */

  
    
    

    create  table
      "memory"."main_gold"."agg_job_offers_by_category_time__dbt_tmp"
  
    as (
      -- models/gold/agg_job_offers_by_category_time.sql
-- Gold layer: Aggregate - Job Offers by Category and Time



SELECT
    f.published_year,
    f.published_month,
    f.published_year_month,
    f.job_category,
    f.contract_type,
    f.work_type,
    
    COUNT(DISTINCT f.job_offer_id) as count_job_offers,
    COUNT(DISTINCT f.company_id) as count_companies,
    
    AVG(f.description_length) as avg_description_length,
    AVG(f.word_count) as avg_word_count,
    
    SUM(CASE WHEN f.is_remote = 1 THEN 1 ELSE 0 END) as remote_jobs,
    SUM(CASE WHEN f.is_permanent = 1 THEN 1 ELSE 0 END) as permanent_jobs,
    
    NOW() as created_at
    
FROM "memory"."main_gold"."fact_job_offers" f
WHERE f.published_date IS NOT NULL
GROUP BY
    f.published_year,
    f.published_month,
    f.published_year_month,
    f.job_category,
    f.contract_type,
    f.work_type
ORDER BY f.published_year_month DESC, f.job_category
    );
  
  
[0m11:36:07.810194 [debug] [Thread-1 (]: SQL status: OK in 0.003 seconds
[0m11:36:07.810718 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.agg_location_analysis"
[0m11:36:07.811236 [debug] [Thread-1 (]: On model.job_intelligent.agg_location_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.agg_location_analysis"} */

  
    
    

    create  table
      "memory"."main_gold"."agg_location_analysis__dbt_tmp"
  
    as (
      -- models/gold/agg_location_analysis.sql
-- Gold layer: Aggregate - Location Analysis



SELECT
    dl.location_id,
    dl.location_raw,
    dl.city,
    dl.country,
    dl.work_location_type,
    
    COUNT(DISTINCT f.job_offer_id) as count_job_offers,
    COUNT(DISTINCT f.company_id) as count_companies,
    
    -- Distribution by job category
    COUNT(DISTINCT CASE WHEN f.job_category = 'Data Engineer' THEN f.job_offer_id END) as data_engineer_count,
    COUNT(DISTINCT CASE WHEN f.job_category = 'Data Scientist' THEN f.job_offer_id END) as data_scientist_count,
    COUNT(DISTINCT CASE WHEN f.job_category = 'Data Analyst' THEN f.job_offer_id END) as data_analyst_count,
    COUNT(DISTINCT CASE WHEN f.job_category = 'ML Engineer' THEN f.job_offer_id END) as ml_engineer_count,
    
    -- Remote percentage
    ROUND(
        100.0 * SUM(CASE WHEN f.is_remote = 1 THEN 1 ELSE 0 END) / COUNT(DISTINCT f.job_offer_id),
        2
    ) as pct_remote,
    
    NOW() as created_at
    
FROM "memory"."main_gold"."dim_location" dl
LEFT JOIN "memory"."main_gold"."fact_job_offers" f ON dl.location_id = f.location_id
GROUP BY
    dl.location_id,
    dl.location_raw,
    dl.city,
    dl.country,
    dl.work_location_type
ORDER BY count_job_offers DESC
    );
  
  
[0m11:36:07.826041 [debug] [Thread-2 (]: SQL status: OK in 0.016 seconds
[0m11:36:07.830113 [debug] [Thread-2 (]: Using duckdb connection "model.job_intelligent.agg_job_offers_by_category_time"
[0m11:36:07.830793 [debug] [Thread-2 (]: On model.job_intelligent.agg_job_offers_by_category_time: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.agg_job_offers_by_category_time"} */
alter table "memory"."main_gold"."agg_job_offers_by_category_time__dbt_tmp" rename to "agg_job_offers_by_category_time"
[0m11:36:07.832040 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m11:36:07.834048 [debug] [Thread-2 (]: On model.job_intelligent.agg_job_offers_by_category_time: COMMIT
[0m11:36:07.834487 [debug] [Thread-2 (]: Using duckdb connection "model.job_intelligent.agg_job_offers_by_category_time"
[0m11:36:07.834833 [debug] [Thread-2 (]: On model.job_intelligent.agg_job_offers_by_category_time: COMMIT
[0m11:36:07.835487 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m11:36:07.837872 [debug] [Thread-2 (]: Using duckdb connection "model.job_intelligent.agg_job_offers_by_category_time"
[0m11:36:07.838422 [debug] [Thread-2 (]: On model.job_intelligent.agg_job_offers_by_category_time: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.agg_job_offers_by_category_time"} */

      drop table if exists "memory"."main_gold"."agg_job_offers_by_category_time__dbt_backup" cascade
    
[0m11:36:07.839432 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m11:36:07.840699 [debug] [Thread-2 (]: On model.job_intelligent.agg_job_offers_by_category_time: Close
[0m11:36:07.841215 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd9c29e5b-3c65-4c17-9882-3a03c9b40e42', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B98FEAFB90>]}
[0m11:36:07.841982 [info ] [Thread-2 (]: 9 of 13 OK created sql table model main_gold.agg_job_offers_by_category_time ... [[32mOK[0m in 0.06s]
[0m11:36:07.842746 [debug] [Thread-2 (]: Finished running node model.job_intelligent.agg_job_offers_by_category_time
[0m11:36:07.843336 [debug] [Thread-1 (]: SQL status: OK in 0.032 seconds
[0m11:36:07.846243 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.agg_location_analysis"
[0m11:36:07.846612 [debug] [Thread-1 (]: On model.job_intelligent.agg_location_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.agg_location_analysis"} */
alter table "memory"."main_gold"."agg_location_analysis__dbt_tmp" rename to "agg_location_analysis"
[0m11:36:07.848315 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m11:36:07.849783 [debug] [Thread-1 (]: On model.job_intelligent.agg_location_analysis: COMMIT
[0m11:36:07.850155 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.agg_location_analysis"
[0m11:36:07.850507 [debug] [Thread-1 (]: On model.job_intelligent.agg_location_analysis: COMMIT
[0m11:36:07.851118 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m11:36:07.853024 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.agg_location_analysis"
[0m11:36:07.853389 [debug] [Thread-1 (]: On model.job_intelligent.agg_location_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.agg_location_analysis"} */

      drop table if exists "memory"."main_gold"."agg_location_analysis__dbt_backup" cascade
    
[0m11:36:07.853998 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m11:36:07.855174 [debug] [Thread-1 (]: On model.job_intelligent.agg_location_analysis: Close
[0m11:36:07.855651 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd9c29e5b-3c65-4c17-9882-3a03c9b40e42', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B98FEAFCB0>]}
[0m11:36:07.856218 [info ] [Thread-1 (]: 10 of 13 OK created sql table model main_gold.agg_location_analysis ............ [[32mOK[0m in 0.07s]
[0m11:36:07.856814 [debug] [Thread-1 (]: Finished running node model.job_intelligent.agg_location_analysis
[0m11:36:10.389505 [debug] [Thread-3 (]: SQL status: OK in 2.804 seconds
[0m11:36:10.395545 [debug] [Thread-3 (]: Using duckdb connection "model.job_intelligent.int_skills_extraction"
[0m11:36:10.396262 [debug] [Thread-3 (]: On model.job_intelligent.int_skills_extraction: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.int_skills_extraction"} */
alter table "memory"."main_silver"."int_skills_extraction__dbt_tmp" rename to "int_skills_extraction"
[0m11:36:10.397755 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m11:36:10.400054 [debug] [Thread-3 (]: On model.job_intelligent.int_skills_extraction: COMMIT
[0m11:36:10.400693 [debug] [Thread-3 (]: Using duckdb connection "model.job_intelligent.int_skills_extraction"
[0m11:36:10.401290 [debug] [Thread-3 (]: On model.job_intelligent.int_skills_extraction: COMMIT
[0m11:36:10.402850 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m11:36:10.406234 [debug] [Thread-3 (]: Using duckdb connection "model.job_intelligent.int_skills_extraction"
[0m11:36:10.406887 [debug] [Thread-3 (]: On model.job_intelligent.int_skills_extraction: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.int_skills_extraction"} */

      drop table if exists "memory"."main_silver"."int_skills_extraction__dbt_backup" cascade
    
[0m11:36:10.408221 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m11:36:10.411286 [debug] [Thread-3 (]: On model.job_intelligent.int_skills_extraction: Close
[0m11:36:10.412161 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd9c29e5b-3c65-4c17-9882-3a03c9b40e42', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B98FEAE6F0>]}
[0m11:36:10.413073 [info ] [Thread-3 (]: 7 of 13 OK created sql table model main_silver.int_skills_extraction ........... [[32mOK[0m in 2.88s]
[0m11:36:10.414151 [debug] [Thread-3 (]: Finished running node model.job_intelligent.int_skills_extraction
[0m11:36:10.415360 [debug] [Thread-4 (]: Began running node model.job_intelligent.dim_skills
[0m11:36:10.416192 [info ] [Thread-4 (]: 11 of 13 START sql table model main_gold.dim_skills ............................ [RUN]
[0m11:36:10.416954 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly model.job_intelligent.fact_job_offers, now model.job_intelligent.dim_skills)
[0m11:36:10.417579 [debug] [Thread-4 (]: Began compiling node model.job_intelligent.dim_skills
[0m11:36:10.423392 [debug] [Thread-4 (]: Writing injected SQL for node "model.job_intelligent.dim_skills"
[0m11:36:10.424915 [debug] [Thread-4 (]: Began executing node model.job_intelligent.dim_skills
[0m11:36:10.431575 [debug] [Thread-4 (]: Writing runtime sql for node "model.job_intelligent.dim_skills"
[0m11:36:10.432917 [debug] [Thread-4 (]: Using duckdb connection "model.job_intelligent.dim_skills"
[0m11:36:10.433493 [debug] [Thread-4 (]: On model.job_intelligent.dim_skills: BEGIN
[0m11:36:10.434067 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m11:36:10.435173 [debug] [Thread-4 (]: SQL status: OK in 0.001 seconds
[0m11:36:10.435758 [debug] [Thread-4 (]: Using duckdb connection "model.job_intelligent.dim_skills"
[0m11:36:10.436557 [debug] [Thread-4 (]: On model.job_intelligent.dim_skills: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_skills"} */

  
    
    

    create  table
      "memory"."main_gold"."dim_skills__dbt_tmp"
  
    as (
      -- models/gold/dim_skills.sql
-- Gold layer: Dimension Skills



WITH skills AS (
    SELECT DISTINCT
        skill_name
    FROM "memory"."main_silver"."int_skills_extraction"
    WHERE skill_name IS NOT NULL
),

skill_categorization AS (
    SELECT
        skill_name,
        CASE
            WHEN skill_name IN ('Python', 'Java', 'Scala', 'R') THEN 'Programming Language'
            WHEN skill_name IN ('SQL', 'NoSQL') THEN 'Database'
            WHEN skill_name IN ('Spark', 'Hadoop', 'Hive', 'Kafka') THEN 'Big Data Framework'
            WHEN skill_name IN ('TensorFlow', 'PyTorch', 'Scikit-learn') THEN 'ML/DL Library'
            WHEN skill_name IN ('AWS', 'Azure', 'GCP') THEN 'Cloud Platform'
            WHEN skill_name IN ('Tableau', 'Power BI', 'Looker') THEN 'BI Tool'
            WHEN skill_name IN ('Airflow', 'DBT', 'Kubernetes', 'Docker') THEN 'DataOps/DevOps'
            WHEN skill_name IN ('Pandas', 'NumPy', 'Matplotlib') THEN 'Data Analysis Library'
            WHEN skill_name IN ('Machine Learning', 'Statistics', 'Data Visualization') THEN 'Domain Knowledge'
            ELSE 'Other'
        END as skill_category
    FROM skills
),

ranked_skills AS (
    SELECT
        ROW_NUMBER() OVER (ORDER BY skill_name) as skill_id,
        skill_name,
        skill_category,
        NOW() as created_at
    FROM skill_categorization
)

SELECT
    skill_id,
    skill_name,
    skill_category,
    created_at
FROM ranked_skills
ORDER BY skill_id
    );
  
  
[0m11:36:10.444005 [debug] [Thread-4 (]: SQL status: OK in 0.007 seconds
[0m11:36:10.448363 [debug] [Thread-4 (]: Using duckdb connection "model.job_intelligent.dim_skills"
[0m11:36:10.448990 [debug] [Thread-4 (]: On model.job_intelligent.dim_skills: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_skills"} */
alter table "memory"."main_gold"."dim_skills__dbt_tmp" rename to "dim_skills"
[0m11:36:10.450159 [debug] [Thread-4 (]: SQL status: OK in 0.001 seconds
[0m11:36:10.452635 [debug] [Thread-4 (]: On model.job_intelligent.dim_skills: COMMIT
[0m11:36:10.453291 [debug] [Thread-4 (]: Using duckdb connection "model.job_intelligent.dim_skills"
[0m11:36:10.453882 [debug] [Thread-4 (]: On model.job_intelligent.dim_skills: COMMIT
[0m11:36:10.454935 [debug] [Thread-4 (]: SQL status: OK in 0.000 seconds
[0m11:36:10.459309 [debug] [Thread-4 (]: Using duckdb connection "model.job_intelligent.dim_skills"
[0m11:36:10.460066 [debug] [Thread-4 (]: On model.job_intelligent.dim_skills: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_skills"} */

      drop table if exists "memory"."main_gold"."dim_skills__dbt_backup" cascade
    
[0m11:36:10.461191 [debug] [Thread-4 (]: SQL status: OK in 0.001 seconds
[0m11:36:10.463337 [debug] [Thread-4 (]: On model.job_intelligent.dim_skills: Close
[0m11:36:10.464154 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd9c29e5b-3c65-4c17-9882-3a03c9b40e42', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B98FEAEDB0>]}
[0m11:36:10.465133 [info ] [Thread-4 (]: 11 of 13 OK created sql table model main_gold.dim_skills ....................... [[32mOK[0m in 0.05s]
[0m11:36:10.466302 [debug] [Thread-4 (]: Finished running node model.job_intelligent.dim_skills
[0m11:36:10.467374 [debug] [Thread-2 (]: Began running node model.job_intelligent.fact_job_skills
[0m11:36:10.468290 [info ] [Thread-2 (]: 12 of 13 START sql table model main_gold.fact_job_skills ....................... [RUN]
[0m11:36:10.469219 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly model.job_intelligent.agg_job_offers_by_category_time, now model.job_intelligent.fact_job_skills)
[0m11:36:10.469910 [debug] [Thread-2 (]: Began compiling node model.job_intelligent.fact_job_skills
[0m11:36:10.476271 [debug] [Thread-2 (]: Writing injected SQL for node "model.job_intelligent.fact_job_skills"
[0m11:36:10.477901 [debug] [Thread-2 (]: Began executing node model.job_intelligent.fact_job_skills
[0m11:36:10.483322 [debug] [Thread-2 (]: Writing runtime sql for node "model.job_intelligent.fact_job_skills"
[0m11:36:10.484860 [debug] [Thread-2 (]: Using duckdb connection "model.job_intelligent.fact_job_skills"
[0m11:36:10.485652 [debug] [Thread-2 (]: On model.job_intelligent.fact_job_skills: BEGIN
[0m11:36:10.486333 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m11:36:10.487659 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m11:36:10.488346 [debug] [Thread-2 (]: Using duckdb connection "model.job_intelligent.fact_job_skills"
[0m11:36:10.489144 [debug] [Thread-2 (]: On model.job_intelligent.fact_job_skills: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.fact_job_skills"} */

  
    
    

    create  table
      "memory"."main_gold"."fact_job_skills__dbt_tmp"
  
    as (
      -- models/gold/fact_job_skills.sql
-- Gold layer: Bridge Table - Job Skills



WITH skills_raw AS (
    SELECT DISTINCT
        job_url,
        company_name_cleaned,
        skill_name
    FROM "memory"."main_silver"."int_skills_extraction"
),

jobs AS (
    SELECT
        job_offer_id,
        job_url
    FROM "memory"."main_gold"."fact_job_offers"
),

skills_dim AS (
    SELECT
        skill_id,
        skill_name
    FROM "memory"."main_gold"."dim_skills"
),

fact_table AS (
    SELECT
        ROW_NUMBER() OVER (ORDER BY s.job_url, sd.skill_id) as job_skill_id,
        j.job_offer_id,
        sd.skill_id,
        s.skill_name,
        NOW() as created_at
    FROM skills_raw s
    LEFT JOIN jobs j ON s.job_url = j.job_url
    LEFT JOIN skills_dim sd ON s.skill_name = sd.skill_name
)

SELECT
    job_skill_id,
    job_offer_id,
    skill_id,
    skill_name,
    created_at
FROM fact_table
WHERE job_offer_id IS NOT NULL
ORDER BY job_offer_id, skill_id
    );
  
  
[0m11:36:10.515350 [debug] [Thread-2 (]: SQL status: OK in 0.025 seconds
[0m11:36:10.520743 [debug] [Thread-2 (]: Using duckdb connection "model.job_intelligent.fact_job_skills"
[0m11:36:10.521431 [debug] [Thread-2 (]: On model.job_intelligent.fact_job_skills: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.fact_job_skills"} */
alter table "memory"."main_gold"."fact_job_skills__dbt_tmp" rename to "fact_job_skills"
[0m11:36:10.522714 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m11:36:10.525550 [debug] [Thread-2 (]: On model.job_intelligent.fact_job_skills: COMMIT
[0m11:36:10.526382 [debug] [Thread-2 (]: Using duckdb connection "model.job_intelligent.fact_job_skills"
[0m11:36:10.527035 [debug] [Thread-2 (]: On model.job_intelligent.fact_job_skills: COMMIT
[0m11:36:10.528236 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m11:36:10.531758 [debug] [Thread-2 (]: Using duckdb connection "model.job_intelligent.fact_job_skills"
[0m11:36:10.532339 [debug] [Thread-2 (]: On model.job_intelligent.fact_job_skills: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.fact_job_skills"} */

      drop table if exists "memory"."main_gold"."fact_job_skills__dbt_backup" cascade
    
[0m11:36:10.533267 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m11:36:10.535151 [debug] [Thread-2 (]: On model.job_intelligent.fact_job_skills: Close
[0m11:36:10.535895 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd9c29e5b-3c65-4c17-9882-3a03c9b40e42', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B98FD85730>]}
[0m11:36:10.536782 [info ] [Thread-2 (]: 12 of 13 OK created sql table model main_gold.fact_job_skills .................. [[32mOK[0m in 0.07s]
[0m11:36:10.537726 [debug] [Thread-2 (]: Finished running node model.job_intelligent.fact_job_skills
[0m11:36:10.538914 [debug] [Thread-1 (]: Began running node model.job_intelligent.agg_skills_demand
[0m11:36:10.539763 [info ] [Thread-1 (]: 13 of 13 START sql table model main_gold.agg_skills_demand ..................... [RUN]
[0m11:36:10.540560 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.job_intelligent.agg_location_analysis, now model.job_intelligent.agg_skills_demand)
[0m11:36:10.541173 [debug] [Thread-1 (]: Began compiling node model.job_intelligent.agg_skills_demand
[0m11:36:10.547585 [debug] [Thread-1 (]: Writing injected SQL for node "model.job_intelligent.agg_skills_demand"
[0m11:36:10.549062 [debug] [Thread-1 (]: Began executing node model.job_intelligent.agg_skills_demand
[0m11:36:10.553197 [debug] [Thread-1 (]: Writing runtime sql for node "model.job_intelligent.agg_skills_demand"
[0m11:36:10.554947 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.agg_skills_demand"
[0m11:36:10.555610 [debug] [Thread-1 (]: On model.job_intelligent.agg_skills_demand: BEGIN
[0m11:36:10.556264 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:36:10.557304 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m11:36:10.558037 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.agg_skills_demand"
[0m11:36:10.558825 [debug] [Thread-1 (]: On model.job_intelligent.agg_skills_demand: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.agg_skills_demand"} */

  
    
    

    create  table
      "memory"."main_gold"."agg_skills_demand__dbt_tmp"
  
    as (
      -- models/gold/agg_skills_demand.sql
-- Gold layer: Aggregate - Skills in Demand



SELECT
    sd.skill_id,
    sd.skill_name,
    sd.skill_category,
    
    COUNT(DISTINCT fs.job_offer_id) as count_jobs_requiring_skill,
    COUNT(DISTINCT f.company_id) as count_companies_requiring_skill,
    
    -- Percentage of all jobs
    ROUND(
        100.0 * COUNT(DISTINCT fs.job_offer_id) / (
            SELECT COUNT(DISTINCT job_offer_id) FROM "memory"."main_gold"."fact_job_offers"
        ),
        2
    ) as pct_of_total_jobs,
    
    -- Average job details for jobs requiring this skill
    AVG(f.description_length) as avg_description_length,
    AVG(f.word_count) as avg_word_count,
    
    NOW() as created_at
    
FROM "memory"."main_gold"."dim_skills" sd
LEFT JOIN "memory"."main_gold"."fact_job_skills" fs ON sd.skill_id = fs.skill_id
LEFT JOIN "memory"."main_gold"."fact_job_offers" f ON fs.job_offer_id = f.job_offer_id
GROUP BY
    sd.skill_id,
    sd.skill_name,
    sd.skill_category
ORDER BY count_jobs_requiring_skill DESC
    );
  
  
[0m11:36:10.576620 [debug] [Thread-1 (]: SQL status: OK in 0.017 seconds
[0m11:36:10.580606 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.agg_skills_demand"
[0m11:36:10.581185 [debug] [Thread-1 (]: On model.job_intelligent.agg_skills_demand: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.agg_skills_demand"} */
alter table "memory"."main_gold"."agg_skills_demand__dbt_tmp" rename to "agg_skills_demand"
[0m11:36:10.582284 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m11:36:10.584381 [debug] [Thread-1 (]: On model.job_intelligent.agg_skills_demand: COMMIT
[0m11:36:10.584952 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.agg_skills_demand"
[0m11:36:10.585490 [debug] [Thread-1 (]: On model.job_intelligent.agg_skills_demand: COMMIT
[0m11:36:10.586436 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m11:36:10.589887 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.agg_skills_demand"
[0m11:36:10.590381 [debug] [Thread-1 (]: On model.job_intelligent.agg_skills_demand: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.agg_skills_demand"} */

      drop table if exists "memory"."main_gold"."agg_skills_demand__dbt_backup" cascade
    
[0m11:36:10.591385 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m11:36:10.593572 [debug] [Thread-1 (]: On model.job_intelligent.agg_skills_demand: Close
[0m11:36:10.594274 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd9c29e5b-3c65-4c17-9882-3a03c9b40e42', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B98FD84E30>]}
[0m11:36:10.595033 [info ] [Thread-1 (]: 13 of 13 OK created sql table model main_gold.agg_skills_demand ................ [[32mOK[0m in 0.05s]
[0m11:36:10.595856 [debug] [Thread-1 (]: Finished running node model.job_intelligent.agg_skills_demand
[0m11:36:10.598295 [debug] [MainThread]: Using duckdb connection "master"
[0m11:36:10.598666 [debug] [MainThread]: On master: BEGIN
[0m11:36:10.599096 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m11:36:10.600008 [debug] [MainThread]: SQL status: OK in 0.001 seconds
[0m11:36:10.600362 [debug] [MainThread]: On master: COMMIT
[0m11:36:10.600708 [debug] [MainThread]: Using duckdb connection "master"
[0m11:36:10.601125 [debug] [MainThread]: On master: COMMIT
[0m11:36:10.601879 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m11:36:10.602233 [debug] [MainThread]: On master: Close
[0m11:36:10.602694 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:36:10.603163 [debug] [MainThread]: Connection 'create_memory_main_gold' was properly closed.
[0m11:36:10.603567 [debug] [MainThread]: Connection 'create_memory_main_bronze' was properly closed.
[0m11:36:10.603963 [debug] [MainThread]: Connection 'create_memory_main_silver' was properly closed.
[0m11:36:10.604351 [debug] [MainThread]: Connection 'list_memory_main_bronze' was properly closed.
[0m11:36:10.604731 [debug] [MainThread]: Connection 'list_memory_main_silver' was properly closed.
[0m11:36:10.605108 [debug] [MainThread]: Connection 'list_memory_main_gold' was properly closed.
[0m11:36:10.605487 [debug] [MainThread]: Connection 'model.job_intelligent.agg_skills_demand' was properly closed.
[0m11:36:10.605885 [debug] [MainThread]: Connection 'model.job_intelligent.fact_job_skills' was properly closed.
[0m11:36:10.606266 [debug] [MainThread]: Connection 'model.job_intelligent.int_skills_extraction' was properly closed.
[0m11:36:10.606643 [debug] [MainThread]: Connection 'model.job_intelligent.dim_skills' was properly closed.
[0m11:36:10.607191 [info ] [MainThread]: 
[0m11:36:10.607721 [info ] [MainThread]: Finished running 12 table models, 1 view model in 0 hours 0 minutes and 3.70 seconds (3.70s).
[0m11:36:10.611496 [debug] [MainThread]: Command end result
[0m11:36:10.639280 [debug] [MainThread]: Wrote artifact WritableManifest to D:\lab2\dbt_project\target\manifest.json
[0m11:36:10.643161 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\lab2\dbt_project\target\semantic_manifest.json
[0m11:36:10.650462 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\lab2\dbt_project\target\run_results.json
[0m11:36:10.650956 [info ] [MainThread]: 
[0m11:36:10.651457 [info ] [MainThread]: [32mCompleted successfully[0m
[0m11:36:10.651912 [info ] [MainThread]: 
[0m11:36:10.652369 [info ] [MainThread]: Done. PASS=13 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=13
[0m11:36:10.653528 [debug] [MainThread]: Command `dbt run` succeeded at 11:36:10.653379 after 5.47 seconds
[0m11:36:10.654030 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B98E1B0890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B98F878830>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B990F414F0>]}
[0m11:36:10.654501 [debug] [MainThread]: Flushing usage events
[0m11:36:18.152940 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:36:21.404567 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019DB1889FD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019DAF594190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019DAF407890>]}


============================== 11:36:21.415168 | 713a3641-619d-4c7a-af60-a4072f5d7fc8 ==============================
[0m11:36:21.415168 [info ] [MainThread]: Running with dbt=1.10.13
[0m11:36:21.416563 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'target_path': 'None', 'cache_selected_only': 'False', 'warn_error': 'None', 'empty': 'None', 'debug': 'False', 'static_parser': 'True', 'log_cache_events': 'False', 'profiles_dir': 'D:\\lab2\\dbt_project', 'log_format': 'default', 'quiet': 'False', 'use_colors': 'True', 'partial_parse': 'True', 'version_check': 'True', 'log_path': 'D:\\lab2\\dbt_project\\logs', 'introspect': 'True', 'no_print': 'None', 'fail_fast': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'invocation_command': 'dbt test', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'write_json': 'True', 'use_experimental_parser': 'False'}
[0m11:36:21.782334 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '713a3641-619d-4c7a-af60-a4072f5d7fc8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019DB1D3F230>]}
[0m11:36:21.859902 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '713a3641-619d-4c7a-af60-a4072f5d7fc8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019DB18F8380>]}
[0m11:36:21.862986 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m11:36:22.178916 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m11:36:22.333580 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m11:36:22.333946 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m11:36:22.339753 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- seeds
[0m11:36:22.358297 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '713a3641-619d-4c7a-af60-a4072f5d7fc8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019DB2B94450>]}
[0m11:36:22.412062 [debug] [MainThread]: Wrote artifact WritableManifest to D:\lab2\dbt_project\target\manifest.json
[0m11:36:22.414187 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\lab2\dbt_project\target\semantic_manifest.json
[0m11:36:22.502193 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '713a3641-619d-4c7a-af60-a4072f5d7fc8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019DB41D8500>]}
[0m11:36:22.502815 [info ] [MainThread]: Found 13 models, 456 macros
[0m11:36:22.503443 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '713a3641-619d-4c7a-af60-a4072f5d7fc8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019DB41CDC50>]}
[0m11:36:22.505988 [warn ] [MainThread]: Nothing to do. Try checking your model configs and model specification args
[0m11:36:22.509040 [debug] [MainThread]: Command end result
[0m11:36:22.611533 [debug] [MainThread]: Wrote artifact WritableManifest to D:\lab2\dbt_project\target\manifest.json
[0m11:36:22.614335 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\lab2\dbt_project\target\semantic_manifest.json
[0m11:36:22.618397 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\lab2\dbt_project\target\run_results.json
[0m11:36:22.619537 [debug] [MainThread]: Command `dbt test` succeeded at 11:36:22.619335 after 1.37 seconds
[0m11:36:22.620144 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019DB41B7EE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019DB3FBFAD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019DB3D181D0>]}
[0m11:36:22.620720 [debug] [MainThread]: Flushing usage events
[0m11:36:25.306861 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:37:57.214697 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FA1CB31D30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FA1D609E50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FA1A6996D0>]}


============================== 11:37:57.226209 | 2c4d9713-87ae-49a0-bb9d-53945ab78495 ==============================
[0m11:37:57.226209 [info ] [MainThread]: Running with dbt=1.10.13
[0m11:37:57.227907 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'write_json': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'empty': 'None', 'debug': 'False', 'static_parser': 'True', 'log_cache_events': 'False', 'profiles_dir': 'D:\\lab2\\dbt_project', 'quiet': 'False', 'log_format': 'default', 'use_colors': 'True', 'partial_parse': 'True', 'version_check': 'True', 'log_path': 'D:\\lab2\\dbt_project\\logs', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'fail_fast': 'False', 'no_print': 'None', 'invocation_command': 'dbt run-operation export_tables --args {}', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'target_path': 'None', 'use_experimental_parser': 'False'}
[0m11:37:57.599931 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '2c4d9713-87ae-49a0-bb9d-53945ab78495', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FA1CFCB230>]}
[0m11:37:57.680868 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '2c4d9713-87ae-49a0-bb9d-53945ab78495', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FA1CB88380>]}
[0m11:37:57.684546 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m11:37:57.954417 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m11:37:58.069223 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m11:37:58.069685 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m11:37:58.077373 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- seeds
[0m11:37:58.101217 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '2c4d9713-87ae-49a0-bb9d-53945ab78495', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FA1DDE0450>]}
[0m11:37:58.191524 [debug] [MainThread]: Wrote artifact WritableManifest to D:\lab2\dbt_project\target\manifest.json
[0m11:37:58.194113 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\lab2\dbt_project\target\semantic_manifest.json
[0m11:37:58.230804 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '2c4d9713-87ae-49a0-bb9d-53945ab78495', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FA1DBBD5E0>]}
[0m11:37:58.231339 [info ] [MainThread]: Found 13 models, 456 macros
[0m11:37:58.232016 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2c4d9713-87ae-49a0-bb9d-53945ab78495', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FA1F30BE70>]}
[0m11:37:58.232638 [debug] [MainThread]: Acquiring new duckdb connection 'macro_export_tables'
[0m11:37:58.233017 [debug] [MainThread]: Using duckdb connection "macro_export_tables"
[0m11:37:58.233348 [debug] [MainThread]: On macro_export_tables: BEGIN
[0m11:37:58.233686 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:37:58.251615 [debug] [MainThread]: SQL status: OK in 0.018 seconds
[0m11:37:58.252059 [debug] [MainThread]: On macro_export_tables: COMMIT
[0m11:37:58.252556 [debug] [MainThread]: Using duckdb connection "macro_export_tables"
[0m11:37:58.252997 [debug] [MainThread]: On macro_export_tables: COMMIT
[0m11:37:58.253708 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m11:37:58.254161 [debug] [MainThread]: On macro_export_tables: Close
[0m11:37:58.254675 [error] [MainThread]: Encountered an error while running operation: Runtime Error
  dbt could not find a macro with the name "export_tables" in any package
[0m11:37:58.258301 [debug] [MainThread]: Traceback (most recent call last):
  File "C:\Users\HP\AppData\Local\Programs\Python\Python313\Lib\site-packages\dbt\task\run_operation.py", line 64, in run
    self._run_unsafe(package_name, macro_name)
    ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\HP\AppData\Local\Programs\Python\Python313\Lib\site-packages\dbt\task\run_operation.py", line 45, in _run_unsafe
    res = adapter.execute_macro(
        macro_name, project=package_name, kwargs=macro_kwargs, macro_resolver=self.manifest
    )
  File "C:\Users\HP\AppData\Local\Programs\Python\Python313\Lib\site-packages\dbt\adapters\base\impl.py", line 1294, in execute_macro
    raise DbtRuntimeError(
    ...<3 lines>...
    )
dbt_common.exceptions.base.DbtRuntimeError: Runtime Error
  dbt could not find a macro with the name "export_tables" in any package

[0m11:37:58.259156 [error] [MainThread]: Encountered an error:
Compilation Error
  dbt could not find a macro with the name 'export_tables' in any package. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m11:37:58.261146 [debug] [MainThread]: Command `dbt run-operation` failed at 11:37:58.260844 after 1.23 seconds
[0m11:37:58.261669 [debug] [MainThread]: Connection 'macro_export_tables' was properly closed.
[0m11:37:58.262077 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FA1DDE6680>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FA1DB13590>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FA1F269490>]}
[0m11:37:58.262489 [debug] [MainThread]: Flushing usage events
[0m11:37:59.514236 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:38:40.641151 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019E9B695FD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019E99394190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019E99207890>]}


============================== 11:38:40.646568 | 3804c86b-182b-403e-bec7-717be7502d92 ==============================
[0m11:38:40.646568 [info ] [MainThread]: Running with dbt=1.10.13
[0m11:38:40.647401 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'use_experimental_parser': 'False', 'cache_selected_only': 'False', 'warn_error': 'None', 'profiles_dir': 'D:\\lab2\\dbt_project', 'debug': 'False', 'static_parser': 'True', 'log_cache_events': 'False', 'empty': 'None', 'quiet': 'False', 'log_format': 'default', 'use_colors': 'True', 'partial_parse': 'True', 'version_check': 'True', 'log_path': 'D:\\lab2\\dbt_project\\logs', 'introspect': 'True', 'no_print': 'None', 'fail_fast': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'invocation_command': 'dbt debug', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'target_path': 'None', 'write_json': 'True'}
[0m11:38:40.675310 [info ] [MainThread]: dbt version: 1.10.13
[0m11:38:40.675790 [info ] [MainThread]: python version: 3.13.2
[0m11:38:40.676176 [info ] [MainThread]: python path: C:\Users\HP\AppData\Local\Programs\Python\Python313\python.exe
[0m11:38:40.676546 [info ] [MainThread]: os info: Windows-11-10.0.26200-SP0
[0m11:38:40.828634 [info ] [MainThread]: Using profiles dir at D:\lab2\dbt_project
[0m11:38:40.829093 [info ] [MainThread]: Using profiles.yml file at D:\lab2\dbt_project\profiles.yml
[0m11:38:40.829417 [info ] [MainThread]: Using dbt_project.yml file at D:\lab2\dbt_project\dbt_project.yml
[0m11:38:40.832793 [info ] [MainThread]: adapter type: duckdb
[0m11:38:40.833261 [info ] [MainThread]: adapter version: 1.10.0
[0m11:38:40.998555 [info ] [MainThread]: Configuration:
[0m11:38:40.999069 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m11:38:40.999492 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m11:38:40.999947 [info ] [MainThread]: Required dependencies:
[0m11:38:41.000340 [debug] [MainThread]: Executing "git --help"
[0m11:38:41.078776 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--no-lazy-fetch]\n           [--no-optional-locks] [--no-advice] [--bare] [--git-dir=<path>]\n           [--work-tree=<path>] [--namespace=<name>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone      Clone a repository into a new directory\n   init       Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add        Add file contents to the index\n   mv         Move or rename a file, a directory, or a symlink\n   restore    Restore working tree files\n   rm         Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect     Use binary search to find the commit that introduced a bug\n   diff       Show changes between commits, commit and working tree, etc\n   grep       Print lines matching a pattern\n   log        Show commit logs\n   show       Show various types of objects\n   status     Show the working tree status\n\ngrow, mark and tweak your common history\n   backfill   Download missing objects in a partial clone\n   branch     List, create, or delete branches\n   commit     Record changes to the repository\n   merge      Join two or more development histories together\n   rebase     Reapply commits on top of another base tip\n   reset      Reset current HEAD to the specified state\n   switch     Switch branches\n   tag        Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch      Download objects and refs from another repository\n   pull       Fetch from and integrate with another repository or a local branch\n   push       Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m11:38:41.079612 [debug] [MainThread]: STDERR: "b''"
[0m11:38:41.080255 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m11:38:41.081186 [info ] [MainThread]: Connection:
[0m11:38:41.081893 [info ] [MainThread]:   database: duckdb
[0m11:38:41.082650 [info ] [MainThread]:   schema: main
[0m11:38:41.083714 [info ] [MainThread]:   path: duckdb.db
[0m11:38:41.084324 [info ] [MainThread]:   config_options: None
[0m11:38:41.084882 [info ] [MainThread]:   extensions: None
[0m11:38:41.085413 [info ] [MainThread]:   settings: {}
[0m11:38:41.085942 [info ] [MainThread]:   external_root: .
[0m11:38:41.086480 [info ] [MainThread]:   use_credential_provider: None
[0m11:38:41.087012 [info ] [MainThread]:   attach: None
[0m11:38:41.087535 [info ] [MainThread]:   filesystems: None
[0m11:38:41.088074 [info ] [MainThread]:   remote: None
[0m11:38:41.088615 [info ] [MainThread]:   plugins: None
[0m11:38:41.089142 [info ] [MainThread]:   disable_transactions: False
[0m11:38:41.089993 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m11:38:41.403659 [debug] [MainThread]: Acquiring new duckdb connection 'debug'
[0m11:38:41.465370 [debug] [MainThread]: Using duckdb connection "debug"
[0m11:38:41.465734 [debug] [MainThread]: On debug: select 1 as id
[0m11:38:41.465991 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:38:41.486062 [debug] [MainThread]: SQL status: OK in 0.020 seconds
[0m11:38:41.486963 [debug] [MainThread]: On debug: Close
[0m11:38:41.487272 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m11:38:41.487592 [info ] [MainThread]: [32mAll checks passed![0m
[0m11:38:41.488400 [debug] [MainThread]: Command `dbt debug` succeeded at 11:38:41.488300 after 1.09 seconds
[0m11:38:41.488724 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m11:38:41.489028 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019E9DB79F30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019E9DC1D5B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019E9DAF0E20>]}
[0m11:38:41.489373 [debug] [MainThread]: Flushing usage events
[0m11:38:42.596525 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:38:46.240543 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000294D8825FD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000294D6524190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000294D6397890>]}


============================== 11:38:46.248900 | 302cd842-b611-46d5-9b41-edaf36bf90e4 ==============================
[0m11:38:46.248900 [info ] [MainThread]: Running with dbt=1.10.13
[0m11:38:46.249931 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'write_json': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'profiles_dir': 'D:\\lab2\\dbt_project', 'debug': 'False', 'static_parser': 'True', 'log_cache_events': 'False', 'empty': 'False', 'quiet': 'False', 'log_format': 'default', 'use_colors': 'True', 'partial_parse': 'True', 'version_check': 'True', 'log_path': 'D:\\lab2\\dbt_project\\logs', 'introspect': 'True', 'invocation_command': 'dbt run', 'fail_fast': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'target_path': 'None', 'use_experimental_parser': 'False'}
[0m11:38:46.592476 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '302cd842-b611-46d5-9b41-edaf36bf90e4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000294D8CDF230>]}
[0m11:38:46.670063 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '302cd842-b611-46d5-9b41-edaf36bf90e4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000294D8898380>]}
[0m11:38:46.673111 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m11:38:46.927884 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m11:38:46.983379 [info ] [MainThread]: Unable to do partial parsing because profile has changed
[0m11:38:46.983830 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '302cd842-b611-46d5-9b41-edaf36bf90e4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000294D9BB7350>]}
[0m11:38:48.057203 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- seeds
[0m11:38:48.061514 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '302cd842-b611-46d5-9b41-edaf36bf90e4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000294DAF246E0>]}
[0m11:38:48.117421 [debug] [MainThread]: Wrote artifact WritableManifest to D:\lab2\dbt_project\target\manifest.json
[0m11:38:48.119539 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\lab2\dbt_project\target\semantic_manifest.json
[0m11:38:48.156389 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '302cd842-b611-46d5-9b41-edaf36bf90e4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000294DAF67CB0>]}
[0m11:38:48.157029 [info ] [MainThread]: Found 13 models, 456 macros
[0m11:38:48.157499 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '302cd842-b611-46d5-9b41-edaf36bf90e4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000294DB11B5F0>]}
[0m11:38:48.159941 [info ] [MainThread]: 
[0m11:38:48.160415 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m11:38:48.160812 [info ] [MainThread]: 
[0m11:38:48.161417 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m11:38:48.168424 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_duckdb'
[0m11:38:48.189380 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_duckdb'
[0m11:38:48.200650 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_duckdb'
[0m11:38:48.282618 [debug] [ThreadPool]: Using duckdb connection "list_duckdb"
[0m11:38:48.283000 [debug] [ThreadPool]: On list_duckdb: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_duckdb"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"duckdb"'
    
  
  
[0m11:38:48.283293 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:38:48.284223 [debug] [ThreadPool]: Using duckdb connection "list_duckdb"
[0m11:38:48.284575 [debug] [ThreadPool]: On list_duckdb: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_duckdb"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"duckdb"'
    
  
  
[0m11:38:48.284849 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:38:48.284010 [debug] [ThreadPool]: Using duckdb connection "list_duckdb"
[0m11:38:48.285396 [debug] [ThreadPool]: On list_duckdb: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_duckdb"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"duckdb"'
    
  
  
[0m11:38:48.285670 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:38:48.341415 [debug] [ThreadPool]: SQL status: OK in 0.058 seconds
[0m11:38:48.342186 [debug] [ThreadPool]: SQL status: OK in 0.056 seconds
[0m11:38:48.342955 [debug] [ThreadPool]: SQL status: OK in 0.058 seconds
[0m11:38:48.346156 [debug] [ThreadPool]: On list_duckdb: Close
[0m11:38:48.348930 [debug] [ThreadPool]: On list_duckdb: Close
[0m11:38:48.351863 [debug] [ThreadPool]: On list_duckdb: Close
[0m11:38:48.354721 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_duckdb, now create_duckdb_main_gold)
[0m11:38:48.355883 [debug] [ThreadPool]: Creating schema "database: "duckdb"
schema: "main_gold"
"
[0m11:38:48.364477 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_duckdb, now create_duckdb_main_bronze)
[0m11:38:48.372151 [debug] [ThreadPool]: Using duckdb connection "create_duckdb_main_gold"
[0m11:38:48.373200 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_duckdb, now create_duckdb_main_silver)
[0m11:38:48.374368 [debug] [ThreadPool]: Creating schema "database: "duckdb"
schema: "main_bronze"
"
[0m11:38:48.375339 [debug] [ThreadPool]: On create_duckdb_main_gold: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_duckdb_main_gold"} */

    
        select type from duckdb_databases()
        where lower(database_name)='duckdb'
        and type='sqlite'
    
  
[0m11:38:48.376614 [debug] [ThreadPool]: Creating schema "database: "duckdb"
schema: "main_silver"
"
[0m11:38:48.381886 [debug] [ThreadPool]: Using duckdb connection "create_duckdb_main_bronze"
[0m11:38:48.382753 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:38:48.386659 [debug] [ThreadPool]: Using duckdb connection "create_duckdb_main_silver"
[0m11:38:48.387754 [debug] [ThreadPool]: On create_duckdb_main_bronze: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_duckdb_main_bronze"} */

    
        select type from duckdb_databases()
        where lower(database_name)='duckdb'
        and type='sqlite'
    
  
[0m11:38:48.389285 [debug] [ThreadPool]: On create_duckdb_main_silver: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_duckdb_main_silver"} */

    
        select type from duckdb_databases()
        where lower(database_name)='duckdb'
        and type='sqlite'
    
  
[0m11:38:48.390279 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:38:48.391301 [debug] [ThreadPool]: SQL status: OK in 0.008 seconds
[0m11:38:48.392130 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:38:48.396073 [debug] [ThreadPool]: Using duckdb connection "create_duckdb_main_gold"
[0m11:38:48.397332 [debug] [ThreadPool]: SQL status: OK in 0.007 seconds
[0m11:38:48.398326 [debug] [ThreadPool]: On create_duckdb_main_gold: BEGIN
[0m11:38:48.399320 [debug] [ThreadPool]: SQL status: OK in 0.007 seconds
[0m11:38:48.402486 [debug] [ThreadPool]: Using duckdb connection "create_duckdb_main_bronze"
[0m11:38:48.405915 [debug] [ThreadPool]: Using duckdb connection "create_duckdb_main_silver"
[0m11:38:48.406936 [debug] [ThreadPool]: SQL status: OK in 0.004 seconds
[0m11:38:48.407793 [debug] [ThreadPool]: On create_duckdb_main_bronze: BEGIN
[0m11:38:48.408742 [debug] [ThreadPool]: On create_duckdb_main_silver: BEGIN
[0m11:38:48.409677 [debug] [ThreadPool]: Using duckdb connection "create_duckdb_main_gold"
[0m11:38:48.411391 [debug] [ThreadPool]: On create_duckdb_main_gold: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_duckdb_main_gold"} */

    
    
        create schema if not exists "duckdb"."main_gold"
    
[0m11:38:48.412527 [debug] [ThreadPool]: SQL status: OK in 0.002 seconds
[0m11:38:48.413380 [debug] [ThreadPool]: SQL status: OK in 0.002 seconds
[0m11:38:48.414262 [debug] [ThreadPool]: Using duckdb connection "create_duckdb_main_bronze"
[0m11:38:48.415251 [debug] [ThreadPool]: SQL status: OK in 0.003 seconds
[0m11:38:48.416107 [debug] [ThreadPool]: Using duckdb connection "create_duckdb_main_silver"
[0m11:38:48.417134 [debug] [ThreadPool]: On create_duckdb_main_bronze: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_duckdb_main_bronze"} */

    
    
        create schema if not exists "duckdb"."main_bronze"
    
[0m11:38:48.419391 [debug] [ThreadPool]: On create_duckdb_main_gold: COMMIT
[0m11:38:48.420362 [debug] [ThreadPool]: On create_duckdb_main_silver: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_duckdb_main_silver"} */

    
    
        create schema if not exists "duckdb"."main_silver"
    
[0m11:38:48.421699 [debug] [ThreadPool]: Using duckdb connection "create_duckdb_main_gold"
[0m11:38:48.422933 [debug] [ThreadPool]: SQL status: OK in 0.002 seconds
[0m11:38:48.423864 [debug] [ThreadPool]: On create_duckdb_main_gold: COMMIT
[0m11:38:48.424870 [debug] [ThreadPool]: SQL status: OK in 0.002 seconds
[0m11:38:48.427405 [debug] [ThreadPool]: On create_duckdb_main_bronze: COMMIT
[0m11:38:48.430017 [debug] [ThreadPool]: On create_duckdb_main_silver: COMMIT
[0m11:38:48.431119 [debug] [ThreadPool]: Using duckdb connection "create_duckdb_main_bronze"
[0m11:38:48.432289 [debug] [ThreadPool]: Using duckdb connection "create_duckdb_main_silver"
[0m11:38:48.433342 [debug] [ThreadPool]: On create_duckdb_main_bronze: COMMIT
[0m11:38:48.434279 [debug] [ThreadPool]: On create_duckdb_main_silver: COMMIT
[0m11:38:48.435258 [debug] [ThreadPool]: SQL status: OK in 0.007 seconds
[0m11:38:48.436896 [debug] [ThreadPool]: On create_duckdb_main_gold: Close
[0m11:38:48.440565 [debug] [ThreadPool]: SQL status: OK in 0.004 seconds
[0m11:38:48.441417 [debug] [ThreadPool]: On create_duckdb_main_bronze: Close
[0m11:38:48.444250 [debug] [ThreadPool]: SQL status: OK in 0.008 seconds
[0m11:38:48.445092 [debug] [ThreadPool]: On create_duckdb_main_silver: Close
[0m11:38:48.450231 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_duckdb_main_silver'
[0m11:38:48.463038 [debug] [ThreadPool]: Using duckdb connection "list_duckdb_main_silver"
[0m11:38:48.464196 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_duckdb_main_bronze'
[0m11:38:48.465094 [debug] [ThreadPool]: On list_duckdb_main_silver: BEGIN
[0m11:38:48.470271 [debug] [ThreadPool]: Using duckdb connection "list_duckdb_main_bronze"
[0m11:38:48.471202 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:38:48.472326 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_duckdb_main_gold'
[0m11:38:48.473434 [debug] [ThreadPool]: On list_duckdb_main_bronze: BEGIN
[0m11:38:48.478003 [debug] [ThreadPool]: Using duckdb connection "list_duckdb_main_gold"
[0m11:38:48.479037 [debug] [ThreadPool]: SQL status: OK in 0.008 seconds
[0m11:38:48.479876 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:38:48.480798 [debug] [ThreadPool]: On list_duckdb_main_gold: BEGIN
[0m11:38:48.481747 [debug] [ThreadPool]: Using duckdb connection "list_duckdb_main_silver"
[0m11:38:48.483138 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:38:48.484119 [debug] [ThreadPool]: SQL status: OK in 0.004 seconds
[0m11:38:48.485125 [debug] [ThreadPool]: On list_duckdb_main_silver: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_duckdb_main_silver"} */
select
      'duckdb' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main_silver'
    and lower(table_catalog) = 'duckdb'
  
[0m11:38:48.486623 [debug] [ThreadPool]: Using duckdb connection "list_duckdb_main_bronze"
[0m11:38:48.487657 [debug] [ThreadPool]: SQL status: OK in 0.005 seconds
[0m11:38:48.489046 [debug] [ThreadPool]: On list_duckdb_main_bronze: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_duckdb_main_bronze"} */
select
      'duckdb' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main_bronze'
    and lower(table_catalog) = 'duckdb'
  
[0m11:38:48.490064 [debug] [ThreadPool]: Using duckdb connection "list_duckdb_main_gold"
[0m11:38:48.491522 [debug] [ThreadPool]: On list_duckdb_main_gold: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_duckdb_main_gold"} */
select
      'duckdb' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main_gold'
    and lower(table_catalog) = 'duckdb'
  
[0m11:38:48.528119 [debug] [ThreadPool]: SQL status: OK in 0.036 seconds
[0m11:38:48.529190 [debug] [ThreadPool]: SQL status: OK in 0.041 seconds
[0m11:38:48.530988 [debug] [ThreadPool]: On list_duckdb_main_gold: ROLLBACK
[0m11:38:48.532628 [debug] [ThreadPool]: On list_duckdb_main_silver: ROLLBACK
[0m11:38:48.533201 [debug] [ThreadPool]: SQL status: OK in 0.042 seconds
[0m11:38:48.534561 [debug] [ThreadPool]: On list_duckdb_main_bronze: ROLLBACK
[0m11:38:48.536874 [debug] [ThreadPool]: Failed to rollback 'list_duckdb_main_gold'
[0m11:38:48.537272 [debug] [ThreadPool]: On list_duckdb_main_gold: Close
[0m11:38:48.538849 [debug] [ThreadPool]: Failed to rollback 'list_duckdb_main_bronze'
[0m11:38:48.539192 [debug] [ThreadPool]: On list_duckdb_main_bronze: Close
[0m11:38:48.540565 [debug] [ThreadPool]: Failed to rollback 'list_duckdb_main_silver'
[0m11:38:48.540899 [debug] [ThreadPool]: On list_duckdb_main_silver: Close
[0m11:38:48.541928 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '302cd842-b611-46d5-9b41-edaf36bf90e4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000294DB1B9FD0>]}
[0m11:38:48.542418 [debug] [MainThread]: Using duckdb connection "master"
[0m11:38:48.542763 [debug] [MainThread]: On master: BEGIN
[0m11:38:48.543126 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:38:48.543785 [debug] [MainThread]: SQL status: OK in 0.001 seconds
[0m11:38:48.544116 [debug] [MainThread]: On master: COMMIT
[0m11:38:48.544444 [debug] [MainThread]: Using duckdb connection "master"
[0m11:38:48.544792 [debug] [MainThread]: On master: COMMIT
[0m11:38:48.545378 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m11:38:48.545712 [debug] [MainThread]: On master: Close
[0m11:38:48.552113 [debug] [Thread-1 (]: Began running node model.job_intelligent.stg_jobs_raw
[0m11:38:48.552769 [info ] [Thread-1 (]: 1 of 13 START sql view model main_bronze.stg_jobs_raw .......................... [RUN]
[0m11:38:48.553569 [debug] [Thread-1 (]: Acquiring new duckdb connection 'model.job_intelligent.stg_jobs_raw'
[0m11:38:48.554126 [debug] [Thread-1 (]: Began compiling node model.job_intelligent.stg_jobs_raw
[0m11:38:48.562940 [debug] [Thread-1 (]: Writing injected SQL for node "model.job_intelligent.stg_jobs_raw"
[0m11:38:48.563788 [debug] [Thread-1 (]: Began executing node model.job_intelligent.stg_jobs_raw
[0m11:38:48.591063 [debug] [Thread-1 (]: Writing runtime sql for node "model.job_intelligent.stg_jobs_raw"
[0m11:38:48.591927 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.stg_jobs_raw"
[0m11:38:48.592298 [debug] [Thread-1 (]: On model.job_intelligent.stg_jobs_raw: BEGIN
[0m11:38:48.592630 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m11:38:48.593479 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m11:38:48.593833 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.stg_jobs_raw"
[0m11:38:48.594211 [debug] [Thread-1 (]: On model.job_intelligent.stg_jobs_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.stg_jobs_raw"} */

  
  create view "duckdb"."main_bronze"."stg_jobs_raw__dbt_tmp" as (
    -- models/bronze/stg_jobs_raw.sql
-- Bronze layer: Lecture directe des données brutes depuis final_data.csv
-- Renommage et typage minimal



-- Read directly from CSV file
SELECT
    -- Renommer les colonnes pour cohérence
    title as job_title,
    location,
    postedTime as posted_time,
    publishedAt as published_at,
    jobUrl as job_url,
    companyName as company_name,
    companyUrl as company_url,
    description as job_description,
    contractType as contract_type,
    workType as work_type,
    
    -- Métadonnées de traçabilité
    NOW() as ingestion_timestamp,
    '2026-01-08 10:38:46.135216+00:00' as dbt_run_id

FROM read_csv_auto('D:/lab2/final_data.csv')

WHERE 1=1
    -- Filtrer les lignes vides
    AND title IS NOT NULL
    AND description IS NOT NULL
  );

[0m11:38:48.634197 [debug] [Thread-1 (]: SQL status: OK in 0.040 seconds
[0m11:38:48.639534 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.stg_jobs_raw"
[0m11:38:48.639887 [debug] [Thread-1 (]: On model.job_intelligent.stg_jobs_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.stg_jobs_raw"} */
alter view "duckdb"."main_bronze"."stg_jobs_raw__dbt_tmp" rename to "stg_jobs_raw"
[0m11:38:48.640658 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m11:38:48.649645 [debug] [Thread-1 (]: On model.job_intelligent.stg_jobs_raw: COMMIT
[0m11:38:48.650365 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.stg_jobs_raw"
[0m11:38:48.650767 [debug] [Thread-1 (]: On model.job_intelligent.stg_jobs_raw: COMMIT
[0m11:38:48.655909 [debug] [Thread-1 (]: SQL status: OK in 0.005 seconds
[0m11:38:48.662986 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.stg_jobs_raw"
[0m11:38:48.663397 [debug] [Thread-1 (]: On model.job_intelligent.stg_jobs_raw: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.stg_jobs_raw"} */

      drop view if exists "duckdb"."main_bronze"."stg_jobs_raw__dbt_backup" cascade
    
[0m11:38:48.664087 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m11:38:48.666738 [debug] [Thread-1 (]: On model.job_intelligent.stg_jobs_raw: Close
[0m11:38:48.668929 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '302cd842-b611-46d5-9b41-edaf36bf90e4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000294DB1DB490>]}
[0m11:38:48.669614 [info ] [Thread-1 (]: 1 of 13 OK created sql view model main_bronze.stg_jobs_raw ..................... [[32mOK[0m in 0.11s]
[0m11:38:48.670222 [debug] [Thread-1 (]: Finished running node model.job_intelligent.stg_jobs_raw
[0m11:38:48.671006 [debug] [Thread-2 (]: Began running node model.job_intelligent.int_jobs_cleaned
[0m11:38:48.671486 [info ] [Thread-2 (]: 2 of 13 START sql table model main_silver.int_jobs_cleaned ..................... [RUN]
[0m11:38:48.672049 [debug] [Thread-2 (]: Acquiring new duckdb connection 'model.job_intelligent.int_jobs_cleaned'
[0m11:38:48.672394 [debug] [Thread-2 (]: Began compiling node model.job_intelligent.int_jobs_cleaned
[0m11:38:48.675977 [debug] [Thread-2 (]: Writing injected SQL for node "model.job_intelligent.int_jobs_cleaned"
[0m11:38:48.677199 [debug] [Thread-2 (]: Began executing node model.job_intelligent.int_jobs_cleaned
[0m11:38:48.698196 [debug] [Thread-2 (]: Writing runtime sql for node "model.job_intelligent.int_jobs_cleaned"
[0m11:38:48.699233 [debug] [Thread-2 (]: Using duckdb connection "model.job_intelligent.int_jobs_cleaned"
[0m11:38:48.699670 [debug] [Thread-2 (]: On model.job_intelligent.int_jobs_cleaned: BEGIN
[0m11:38:48.700063 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m11:38:48.700826 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m11:38:48.701238 [debug] [Thread-2 (]: Using duckdb connection "model.job_intelligent.int_jobs_cleaned"
[0m11:38:48.701717 [debug] [Thread-2 (]: On model.job_intelligent.int_jobs_cleaned: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.int_jobs_cleaned"} */

  
    
    

    create  table
      "duckdb"."main_silver"."int_jobs_cleaned__dbt_tmp"
  
    as (
      -- models/silver/int_jobs_cleaned.sql
-- Silver layer: Nettoyage et normalisation des données brutes



WITH raw_jobs AS (
    SELECT * FROM "duckdb"."main_bronze"."stg_jobs_raw"
),

cleaned_jobs AS (
    SELECT
        -- Texte: lowercase, trim, remove special characters
        LOWER(TRIM(job_title)) as job_title_cleaned,
        LOWER(TRIM(location)) as location_cleaned,
        LOWER(TRIM(company_name)) as company_name_cleaned,
        LOWER(TRIM(job_description)) as job_description_cleaned,
        LOWER(TRIM(contract_type)) as contract_type_cleaned,
        LOWER(TRIM(work_type)) as work_type_cleaned,
        
        -- URLs as-is
        job_url,
        company_url,
        
        -- Dates
        TRY_CAST(published_at AS DATE) as published_date,
        posted_time,
        
        -- Extract year-month for time-based analysis
        DATE_TRUNC('month', TRY_CAST(published_at AS DATE)) as published_year_month,
        EXTRACT(YEAR FROM TRY_CAST(published_at AS DATE)) as published_year,
        EXTRACT(MONTH FROM TRY_CAST(published_at AS DATE)) as published_month,
        
        -- Métadonnées
        ingestion_timestamp
    FROM raw_jobs
)

SELECT
    *,
    -- Deduplication flag
    ROW_NUMBER() OVER (
        PARTITION BY job_title_cleaned, company_name_cleaned, location_cleaned 
        ORDER BY published_date DESC
    ) as dedup_rank
FROM cleaned_jobs
    );
  
  
[0m11:38:48.844882 [debug] [Thread-2 (]: SQL status: OK in 0.143 seconds
[0m11:38:48.847650 [debug] [Thread-2 (]: Using duckdb connection "model.job_intelligent.int_jobs_cleaned"
[0m11:38:48.848051 [debug] [Thread-2 (]: On model.job_intelligent.int_jobs_cleaned: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.int_jobs_cleaned"} */
alter table "duckdb"."main_silver"."int_jobs_cleaned__dbt_tmp" rename to "int_jobs_cleaned"
[0m11:38:48.848821 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m11:38:48.853882 [debug] [Thread-2 (]: On model.job_intelligent.int_jobs_cleaned: COMMIT
[0m11:38:48.854304 [debug] [Thread-2 (]: Using duckdb connection "model.job_intelligent.int_jobs_cleaned"
[0m11:38:48.854693 [debug] [Thread-2 (]: On model.job_intelligent.int_jobs_cleaned: COMMIT
[0m11:38:48.917972 [debug] [Thread-2 (]: SQL status: OK in 0.063 seconds
[0m11:38:48.923351 [debug] [Thread-2 (]: Using duckdb connection "model.job_intelligent.int_jobs_cleaned"
[0m11:38:48.924340 [debug] [Thread-2 (]: On model.job_intelligent.int_jobs_cleaned: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.int_jobs_cleaned"} */

      drop table if exists "duckdb"."main_silver"."int_jobs_cleaned__dbt_backup" cascade
    
[0m11:38:48.925967 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m11:38:48.929227 [debug] [Thread-2 (]: On model.job_intelligent.int_jobs_cleaned: Close
[0m11:38:48.930513 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '302cd842-b611-46d5-9b41-edaf36bf90e4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000294DB741810>]}
[0m11:38:48.932005 [info ] [Thread-2 (]: 2 of 13 OK created sql table model main_silver.int_jobs_cleaned ................ [[32mOK[0m in 0.26s]
[0m11:38:48.933593 [debug] [Thread-2 (]: Finished running node model.job_intelligent.int_jobs_cleaned
[0m11:38:48.935235 [debug] [Thread-3 (]: Began running node model.job_intelligent.int_job_title_normalization
[0m11:38:48.936708 [info ] [Thread-3 (]: 3 of 13 START sql table model main_silver.int_job_title_normalization .......... [RUN]
[0m11:38:48.938114 [debug] [Thread-3 (]: Acquiring new duckdb connection 'model.job_intelligent.int_job_title_normalization'
[0m11:38:48.939110 [debug] [Thread-3 (]: Began compiling node model.job_intelligent.int_job_title_normalization
[0m11:38:48.947709 [debug] [Thread-3 (]: Writing injected SQL for node "model.job_intelligent.int_job_title_normalization"
[0m11:38:48.949634 [debug] [Thread-3 (]: Began executing node model.job_intelligent.int_job_title_normalization
[0m11:38:48.956486 [debug] [Thread-3 (]: Writing runtime sql for node "model.job_intelligent.int_job_title_normalization"
[0m11:38:48.958413 [debug] [Thread-3 (]: Using duckdb connection "model.job_intelligent.int_job_title_normalization"
[0m11:38:48.959185 [debug] [Thread-3 (]: On model.job_intelligent.int_job_title_normalization: BEGIN
[0m11:38:48.959883 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m11:38:48.961310 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m11:38:48.962037 [debug] [Thread-3 (]: Using duckdb connection "model.job_intelligent.int_job_title_normalization"
[0m11:38:48.962979 [debug] [Thread-3 (]: On model.job_intelligent.int_job_title_normalization: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.int_job_title_normalization"} */

  
    
    

    create  table
      "duckdb"."main_silver"."int_job_title_normalization__dbt_tmp"
  
    as (
      -- models/silver/int_job_title_normalization.sql
-- Silver layer: Normaliser les intitulés de postes



WITH cleaned_jobs AS (
    SELECT * FROM "duckdb"."main_silver"."int_jobs_cleaned"
),

title_normalized AS (
    SELECT
        *,
        CASE
            WHEN job_title_cleaned LIKE '%data engineer%' THEN 'Data Engineer'
            WHEN job_title_cleaned LIKE '%data scientist%' THEN 'Data Scientist'
            WHEN job_title_cleaned LIKE '%data analyst%' THEN 'Data Analyst'
            WHEN job_title_cleaned LIKE '%analytics engineer%' THEN 'Analytics Engineer'
            WHEN job_title_cleaned LIKE '%ml engineer%' OR job_title_cleaned LIKE '%machine learning%' THEN 'ML Engineer'
            WHEN job_title_cleaned LIKE '%data architect%' THEN 'Data Architect'
            WHEN job_title_cleaned LIKE '%bi developer%' OR job_title_cleaned LIKE '%business intelligence%' THEN 'BI Developer'
            WHEN job_title_cleaned LIKE '%etl%' OR job_title_cleaned LIKE '%pipeline%' THEN 'ETL/Pipeline Engineer'
            WHEN job_title_cleaned LIKE '%data engineer%' THEN 'Data Engineer'
            ELSE 'Other Data Role'
        END as job_category,
        
        CASE
            WHEN contract_type_cleaned LIKE '%cdi%' OR contract_type_cleaned LIKE '%permanent%' THEN 'Permanent'
            WHEN contract_type_cleaned LIKE '%cdd%' OR contract_type_cleaned LIKE '%contract%' THEN 'Contract'
            WHEN contract_type_cleaned LIKE '%stage%' OR contract_type_cleaned LIKE '%internship%' THEN 'Internship'
            WHEN contract_type_cleaned LIKE '%freelance%' THEN 'Freelance'
            ELSE 'Not Specified'
        END as contract_type_normalized,
        
        CASE
            WHEN work_type_cleaned LIKE '%remote%' THEN 'Remote'
            WHEN work_type_cleaned LIKE '%hybrid%' THEN 'Hybrid'
            WHEN work_type_cleaned LIKE '%onsite%' OR work_type_cleaned LIKE '%on-site%' THEN 'On-site'
            ELSE 'Not Specified'
        END as work_type_normalized
    
    FROM cleaned_jobs
    WHERE dedup_rank = 1  -- Keep only first occurrence (most recent)
)

SELECT * FROM title_normalized
    );
  
  
[0m11:38:48.996013 [debug] [Thread-3 (]: SQL status: OK in 0.032 seconds
[0m11:38:48.999044 [debug] [Thread-3 (]: Using duckdb connection "model.job_intelligent.int_job_title_normalization"
[0m11:38:48.999481 [debug] [Thread-3 (]: On model.job_intelligent.int_job_title_normalization: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.int_job_title_normalization"} */
alter table "duckdb"."main_silver"."int_job_title_normalization__dbt_tmp" rename to "int_job_title_normalization"
[0m11:38:49.000292 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m11:38:49.002197 [debug] [Thread-3 (]: On model.job_intelligent.int_job_title_normalization: COMMIT
[0m11:38:49.002619 [debug] [Thread-3 (]: Using duckdb connection "model.job_intelligent.int_job_title_normalization"
[0m11:38:49.003015 [debug] [Thread-3 (]: On model.job_intelligent.int_job_title_normalization: COMMIT
[0m11:38:49.050883 [debug] [Thread-3 (]: SQL status: OK in 0.047 seconds
[0m11:38:49.056110 [debug] [Thread-3 (]: Using duckdb connection "model.job_intelligent.int_job_title_normalization"
[0m11:38:49.057108 [debug] [Thread-3 (]: On model.job_intelligent.int_job_title_normalization: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.int_job_title_normalization"} */

      drop table if exists "duckdb"."main_silver"."int_job_title_normalization__dbt_backup" cascade
    
[0m11:38:49.058599 [debug] [Thread-3 (]: SQL status: OK in 0.001 seconds
[0m11:38:49.061113 [debug] [Thread-3 (]: On model.job_intelligent.int_job_title_normalization: Close
[0m11:38:49.062086 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '302cd842-b611-46d5-9b41-edaf36bf90e4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000294DB576B50>]}
[0m11:38:49.063207 [info ] [Thread-3 (]: 3 of 13 OK created sql table model main_silver.int_job_title_normalization ..... [[32mOK[0m in 0.12s]
[0m11:38:49.064967 [debug] [Thread-3 (]: Finished running node model.job_intelligent.int_job_title_normalization
[0m11:38:49.067122 [debug] [Thread-2 (]: Began running node model.job_intelligent.dim_time
[0m11:38:49.068057 [info ] [Thread-2 (]: 6 of 13 START sql table model main_gold.dim_time ............................... [RUN]
[0m11:38:49.069171 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly model.job_intelligent.int_jobs_cleaned, now model.job_intelligent.dim_time)
[0m11:38:49.069796 [debug] [Thread-2 (]: Began compiling node model.job_intelligent.dim_time
[0m11:38:49.077150 [debug] [Thread-2 (]: Writing injected SQL for node "model.job_intelligent.dim_time"
[0m11:38:49.078500 [debug] [Thread-4 (]: Began running node model.job_intelligent.dim_company
[0m11:38:49.080071 [debug] [Thread-1 (]: Began running node model.job_intelligent.dim_location
[0m11:38:49.081361 [info ] [Thread-4 (]: 4 of 13 START sql table model main_gold.dim_company ............................ [RUN]
[0m11:38:49.082369 [debug] [Thread-3 (]: Began running node model.job_intelligent.int_skills_extraction
[0m11:38:49.083800 [info ] [Thread-1 (]: 5 of 13 START sql table model main_gold.dim_location ........................... [RUN]
[0m11:38:49.085328 [debug] [Thread-4 (]: Acquiring new duckdb connection 'model.job_intelligent.dim_company'
[0m11:38:49.086772 [info ] [Thread-3 (]: 7 of 13 START sql table model main_silver.int_skills_extraction ................ [RUN]
[0m11:38:49.088197 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.job_intelligent.stg_jobs_raw, now model.job_intelligent.dim_location)
[0m11:38:49.089301 [debug] [Thread-2 (]: Began executing node model.job_intelligent.dim_time
[0m11:38:49.090332 [debug] [Thread-4 (]: Began compiling node model.job_intelligent.dim_company
[0m11:38:49.091458 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly model.job_intelligent.int_job_title_normalization, now model.job_intelligent.int_skills_extraction)
[0m11:38:49.092634 [debug] [Thread-1 (]: Began compiling node model.job_intelligent.dim_location
[0m11:38:49.101239 [debug] [Thread-2 (]: Writing runtime sql for node "model.job_intelligent.dim_time"
[0m11:38:49.108583 [debug] [Thread-4 (]: Writing injected SQL for node "model.job_intelligent.dim_company"
[0m11:38:49.109822 [debug] [Thread-3 (]: Began compiling node model.job_intelligent.int_skills_extraction
[0m11:38:49.118728 [debug] [Thread-1 (]: Writing injected SQL for node "model.job_intelligent.dim_location"
[0m11:38:49.126640 [debug] [Thread-3 (]: Writing injected SQL for node "model.job_intelligent.int_skills_extraction"
[0m11:38:49.128388 [debug] [Thread-4 (]: Began executing node model.job_intelligent.dim_company
[0m11:38:49.136033 [debug] [Thread-4 (]: Writing runtime sql for node "model.job_intelligent.dim_company"
[0m11:38:49.137416 [debug] [Thread-2 (]: Using duckdb connection "model.job_intelligent.dim_time"
[0m11:38:49.138393 [debug] [Thread-2 (]: On model.job_intelligent.dim_time: BEGIN
[0m11:38:49.139386 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m11:38:49.141336 [debug] [Thread-1 (]: Began executing node model.job_intelligent.dim_location
[0m11:38:49.142349 [debug] [Thread-3 (]: Began executing node model.job_intelligent.int_skills_extraction
[0m11:38:49.143533 [debug] [Thread-4 (]: Using duckdb connection "model.job_intelligent.dim_company"
[0m11:38:49.144669 [debug] [Thread-2 (]: SQL status: OK in 0.005 seconds
[0m11:38:49.152094 [debug] [Thread-1 (]: Writing runtime sql for node "model.job_intelligent.dim_location"
[0m11:38:49.159231 [debug] [Thread-3 (]: Writing runtime sql for node "model.job_intelligent.int_skills_extraction"
[0m11:38:49.160311 [debug] [Thread-4 (]: On model.job_intelligent.dim_company: BEGIN
[0m11:38:49.161434 [debug] [Thread-2 (]: Using duckdb connection "model.job_intelligent.dim_time"
[0m11:38:49.163121 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m11:38:49.164602 [debug] [Thread-2 (]: On model.job_intelligent.dim_time: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_time"} */

  
    
    

    create  table
      "duckdb"."main_gold"."dim_time__dbt_tmp"
  
    as (
      -- models/gold/dim_time.sql
-- Gold layer: Dimension Time



-- Generate a date dimension for time-based analysis
WITH date_spine AS (
    SELECT
        published_date,
        EXTRACT(YEAR FROM published_date) as year,
        EXTRACT(MONTH FROM published_date) as month,
        EXTRACT(QUARTER FROM published_date) as quarter,
        EXTRACT(WEEK FROM published_date) as week,
        EXTRACT(DAYOFWEEK FROM published_date) as day_of_week,
        DATE_TRUNC('month', published_date) as month_start,
        DATE_TRUNC('quarter', published_date) as quarter_start,
        DATE_TRUNC('year', published_date) as year_start,
        
        CASE 
            WHEN EXTRACT(MONTH FROM published_date) IN (1,2,3) THEN 'Q1'
            WHEN EXTRACT(MONTH FROM published_date) IN (4,5,6) THEN 'Q2'
            WHEN EXTRACT(MONTH FROM published_date) IN (7,8,9) THEN 'Q3'
            ELSE 'Q4'
        END as quarter_name,
        
        CASE EXTRACT(DAYOFWEEK FROM published_date)
            WHEN 0 THEN 'Sunday'
            WHEN 1 THEN 'Monday'
            WHEN 2 THEN 'Tuesday'
            WHEN 3 THEN 'Wednesday'
            WHEN 4 THEN 'Thursday'
            WHEN 5 THEN 'Friday'
            WHEN 6 THEN 'Saturday'
        END as day_name,
        
        CASE EXTRACT(MONTH FROM published_date)
            WHEN 1 THEN 'January'
            WHEN 2 THEN 'February'
            WHEN 3 THEN 'March'
            WHEN 4 THEN 'April'
            WHEN 5 THEN 'May'
            WHEN 6 THEN 'June'
            WHEN 7 THEN 'July'
            WHEN 8 THEN 'August'
            WHEN 9 THEN 'September'
            WHEN 10 THEN 'October'
            WHEN 11 THEN 'November'
            WHEN 12 THEN 'December'
        END as month_name
        
    FROM (
        SELECT DISTINCT published_date
        FROM "duckdb"."main_silver"."int_job_title_normalization"
        WHERE published_date IS NOT NULL
    )
)

SELECT
    published_date as date_id,
    year,
    month,
    quarter,
    week,
    day_of_week,
    day_name,
    month_name,
    quarter_name,
    month_start,
    quarter_start,
    year_start,
    NOW() as created_at
FROM date_spine
ORDER BY published_date
    );
  
  
[0m11:38:49.167083 [debug] [Thread-3 (]: Using duckdb connection "model.job_intelligent.int_skills_extraction"
[0m11:38:49.168506 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.dim_location"
[0m11:38:49.170417 [debug] [Thread-4 (]: SQL status: OK in 0.007 seconds
[0m11:38:49.171511 [debug] [Thread-3 (]: On model.job_intelligent.int_skills_extraction: BEGIN
[0m11:38:49.172581 [debug] [Thread-1 (]: On model.job_intelligent.dim_location: BEGIN
[0m11:38:49.173660 [debug] [Thread-4 (]: Using duckdb connection "model.job_intelligent.dim_company"
[0m11:38:49.174794 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m11:38:49.176049 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:38:49.177312 [debug] [Thread-4 (]: On model.job_intelligent.dim_company: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_company"} */

  
    
    

    create  table
      "duckdb"."main_gold"."dim_company__dbt_tmp"
  
    as (
      -- models/gold/dim_company.sql
-- Gold layer: Dimension Company



WITH jobs AS (
    SELECT DISTINCT
        company_name_cleaned,
        company_url
    FROM "duckdb"."main_silver"."int_job_title_normalization"
    WHERE company_name_cleaned IS NOT NULL
),

ranked_companies AS (
    SELECT
        ROW_NUMBER() OVER (ORDER BY company_name_cleaned) as company_id,
        company_name_cleaned as company_name,
        company_url,
        NOW() as created_at
    FROM jobs
)

SELECT
    company_id,
    company_name,
    company_url,
    created_at
FROM ranked_companies
ORDER BY company_id
    );
  
  
[0m11:38:49.180765 [debug] [Thread-1 (]: SQL status: OK in 0.004 seconds
[0m11:38:49.182278 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.dim_location"
[0m11:38:49.183883 [debug] [Thread-1 (]: On model.job_intelligent.dim_location: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_location"} */

  
    
    

    create  table
      "duckdb"."main_gold"."dim_location__dbt_tmp"
  
    as (
      -- models/gold/dim_location.sql
-- Gold layer: Dimension Location



WITH jobs AS (
    SELECT DISTINCT
        location_cleaned as location_raw
    FROM "duckdb"."main_silver"."int_job_title_normalization"
    WHERE location_cleaned IS NOT NULL
),

location_parsed AS (
    SELECT
        location_raw,
        -- Extract city (before comma)
        CASE 
            WHEN POSITION(',' IN location_raw) > 0 
            THEN TRIM(SUBSTRING(location_raw, 1, POSITION(',' IN location_raw) - 1))
            ELSE location_raw
        END as city,
        
        -- Extract country (after comma)
        CASE 
            WHEN POSITION(',' IN location_raw) > 0 
            THEN TRIM(SUBSTRING(location_raw, POSITION(',' IN location_raw) + 1))
            ELSE 'Not Specified'
        END as country,
        
        -- Detect if remote
        CASE 
            WHEN location_raw LIKE '%remote%' 
            THEN 'Remote'
            ELSE 'On-site'
        END as work_location_type
    FROM jobs
),

ranked_locations AS (
    SELECT
        ROW_NUMBER() OVER (ORDER BY location_raw) as location_id,
        location_raw,
        city,
        country,
        work_location_type,
        NOW() as created_at
    FROM location_parsed
)

SELECT
    location_id,
    location_raw,
    city,
    country,
    work_location_type,
    created_at
FROM ranked_locations
ORDER BY location_id
    );
  
  
[0m11:38:49.185854 [debug] [Thread-3 (]: SQL status: OK in 0.011 seconds
[0m11:38:49.187060 [debug] [Thread-3 (]: Using duckdb connection "model.job_intelligent.int_skills_extraction"
[0m11:38:49.188356 [debug] [Thread-3 (]: On model.job_intelligent.int_skills_extraction: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.int_skills_extraction"} */

  
    
    

    create  table
      "duckdb"."main_silver"."int_skills_extraction__dbt_tmp"
  
    as (
      -- models/silver/int_skills_extraction.sql
-- Silver layer: Extraction des compétences depuis la description



WITH jobs_with_titles AS (
    SELECT * FROM "duckdb"."main_silver"."int_job_title_normalization"
),

skills_mapping AS (
    -- Définir un mapping de compétences communes Data/Tech
    SELECT
        'Python' as skill_name,
        'python|py |\.py' as skill_pattern
    UNION ALL SELECT 'SQL', 'sql|sql|sql server|postgres|oracle'
    UNION ALL SELECT 'Spark', 'spark|pyspark'
    UNION ALL SELECT 'Hadoop', 'hadoop|hdfs'
    UNION ALL SELECT 'Scala', 'scala'
    UNION ALL SELECT 'Java', '\bjava\b'
    UNION ALL SELECT 'R', '\br\b|r programming'
    UNION ALL SELECT 'Tableau', 'tableau'
    UNION ALL SELECT 'Power BI', 'power bi|powerbi'
    UNION ALL SELECT 'Looker', 'looker'
    UNION ALL SELECT 'AWS', 'aws|amazon web|s3 |ec2|redshift'
    UNION ALL SELECT 'Azure', 'azure|microsoft azure|synapse|cosmos'
    UNION ALL SELECT 'GCP', 'gcp|google cloud|bigquery'
    UNION ALL SELECT 'Airflow', 'airflow'
    UNION ALL SELECT 'DBT', '\bdbt\b|dbt'
    UNION ALL SELECT 'Kubernetes', 'kubernetes|k8s'
    UNION ALL SELECT 'Docker', 'docker'
    UNION ALL SELECT 'Git', 'git|github|gitlab'
    UNION ALL SELECT 'TensorFlow', 'tensorflow'
    UNION ALL SELECT 'PyTorch', 'pytorch'
    UNION ALL SELECT 'Scikit-learn', 'scikit|sklearn'
    UNION ALL SELECT 'Pandas', 'pandas'
    UNION ALL SELECT 'NumPy', 'numpy'
    UNION ALL SELECT 'Machine Learning', 'machine learning|deep learning|ml|artificial intelligence'
    UNION ALL SELECT 'Statistics', 'statistics|statistical|probability'
    UNION ALL SELECT 'Data Visualization', 'data visualization|visualization|charts|graphs'
),

jobs_exploded AS (
    SELECT
        j.*,
        s.skill_name,
        CASE 
            WHEN job_description_cleaned ILIKE '%' || s.skill_pattern || '%' THEN 1 
            ELSE 0 
        END as has_skill
    FROM jobs_with_titles j
    CROSS JOIN skills_mapping s
)

SELECT
    *
FROM jobs_exploded
WHERE has_skill = 1

ORDER BY job_title_cleaned, published_date DESC
    );
  
  
[0m11:38:49.191286 [debug] [Thread-2 (]: SQL status: OK in 0.022 seconds
[0m11:38:49.200562 [debug] [Thread-2 (]: Using duckdb connection "model.job_intelligent.dim_time"
[0m11:38:49.201119 [debug] [Thread-2 (]: On model.job_intelligent.dim_time: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_time"} */
alter table "duckdb"."main_gold"."dim_time__dbt_tmp" rename to "dim_time"
[0m11:38:49.201637 [debug] [Thread-4 (]: SQL status: OK in 0.022 seconds
[0m11:38:49.204518 [debug] [Thread-4 (]: Using duckdb connection "model.job_intelligent.dim_company"
[0m11:38:49.204944 [debug] [Thread-4 (]: On model.job_intelligent.dim_company: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_company"} */
alter table "duckdb"."main_gold"."dim_company__dbt_tmp" rename to "dim_company"
[0m11:38:49.205851 [debug] [Thread-1 (]: SQL status: OK in 0.021 seconds
[0m11:38:49.207353 [debug] [Thread-2 (]: SQL status: OK in 0.006 seconds
[0m11:38:49.210035 [debug] [Thread-2 (]: On model.job_intelligent.dim_time: COMMIT
[0m11:38:49.210805 [debug] [Thread-4 (]: SQL status: OK in 0.005 seconds
[0m11:38:49.212988 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.dim_location"
[0m11:38:49.213472 [debug] [Thread-2 (]: Using duckdb connection "model.job_intelligent.dim_time"
[0m11:38:49.215590 [debug] [Thread-4 (]: On model.job_intelligent.dim_company: COMMIT
[0m11:38:49.216165 [debug] [Thread-1 (]: On model.job_intelligent.dim_location: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_location"} */
alter table "duckdb"."main_gold"."dim_location__dbt_tmp" rename to "dim_location"
[0m11:38:49.216710 [debug] [Thread-2 (]: On model.job_intelligent.dim_time: COMMIT
[0m11:38:49.217279 [debug] [Thread-4 (]: Using duckdb connection "model.job_intelligent.dim_company"
[0m11:38:49.218248 [debug] [Thread-4 (]: On model.job_intelligent.dim_company: COMMIT
[0m11:38:49.218923 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m11:38:49.220564 [debug] [Thread-1 (]: On model.job_intelligent.dim_location: COMMIT
[0m11:38:49.220993 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.dim_location"
[0m11:38:49.221408 [debug] [Thread-1 (]: On model.job_intelligent.dim_location: COMMIT
[0m11:38:49.222066 [debug] [Thread-2 (]: SQL status: OK in 0.004 seconds
[0m11:38:49.224611 [debug] [Thread-2 (]: Using duckdb connection "model.job_intelligent.dim_time"
[0m11:38:49.225069 [debug] [Thread-2 (]: On model.job_intelligent.dim_time: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_time"} */

      drop table if exists "duckdb"."main_gold"."dim_time__dbt_backup" cascade
    
[0m11:38:49.225668 [debug] [Thread-4 (]: SQL status: OK in 0.007 seconds
[0m11:38:49.228271 [debug] [Thread-4 (]: Using duckdb connection "model.job_intelligent.dim_company"
[0m11:38:49.228781 [debug] [Thread-4 (]: On model.job_intelligent.dim_company: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_company"} */

      drop table if exists "duckdb"."main_gold"."dim_company__dbt_backup" cascade
    
[0m11:38:49.229378 [debug] [Thread-2 (]: SQL status: OK in 0.004 seconds
[0m11:38:49.230838 [debug] [Thread-2 (]: On model.job_intelligent.dim_time: Close
[0m11:38:49.231409 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '302cd842-b611-46d5-9b41-edaf36bf90e4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000294DB676CF0>]}
[0m11:38:49.232055 [info ] [Thread-2 (]: 6 of 13 OK created sql table model main_gold.dim_time .......................... [[32mOK[0m in 0.16s]
[0m11:38:49.232742 [debug] [Thread-2 (]: Finished running node model.job_intelligent.dim_time
[0m11:38:49.233184 [debug] [Thread-1 (]: SQL status: OK in 0.011 seconds
[0m11:38:49.235733 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.dim_location"
[0m11:38:49.236147 [debug] [Thread-1 (]: On model.job_intelligent.dim_location: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_location"} */

      drop table if exists "duckdb"."main_gold"."dim_location__dbt_backup" cascade
    
[0m11:38:49.236819 [debug] [Thread-4 (]: SQL status: OK in 0.008 seconds
[0m11:38:49.238197 [debug] [Thread-4 (]: On model.job_intelligent.dim_company: Close
[0m11:38:49.238727 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '302cd842-b611-46d5-9b41-edaf36bf90e4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000294DB6A4BF0>]}
[0m11:38:49.239335 [info ] [Thread-4 (]: 4 of 13 OK created sql table model main_gold.dim_company ....................... [[32mOK[0m in 0.15s]
[0m11:38:49.239997 [debug] [Thread-4 (]: Finished running node model.job_intelligent.dim_company
[0m11:38:49.240575 [debug] [Thread-1 (]: SQL status: OK in 0.004 seconds
[0m11:38:49.241872 [debug] [Thread-1 (]: On model.job_intelligent.dim_location: Close
[0m11:38:49.242373 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '302cd842-b611-46d5-9b41-edaf36bf90e4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000294DB6A5430>]}
[0m11:38:49.243086 [info ] [Thread-1 (]: 5 of 13 OK created sql table model main_gold.dim_location ...................... [[32mOK[0m in 0.15s]
[0m11:38:49.243832 [debug] [Thread-1 (]: Finished running node model.job_intelligent.dim_location
[0m11:38:49.244710 [debug] [Thread-2 (]: Began running node model.job_intelligent.fact_job_offers
[0m11:38:49.245272 [info ] [Thread-2 (]: 8 of 13 START sql table model main_gold.fact_job_offers ........................ [RUN]
[0m11:38:49.245781 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly model.job_intelligent.dim_time, now model.job_intelligent.fact_job_offers)
[0m11:38:49.246182 [debug] [Thread-2 (]: Began compiling node model.job_intelligent.fact_job_offers
[0m11:38:49.250219 [debug] [Thread-2 (]: Writing injected SQL for node "model.job_intelligent.fact_job_offers"
[0m11:38:49.251084 [debug] [Thread-2 (]: Began executing node model.job_intelligent.fact_job_offers
[0m11:38:49.253994 [debug] [Thread-2 (]: Writing runtime sql for node "model.job_intelligent.fact_job_offers"
[0m11:38:49.254929 [debug] [Thread-2 (]: Using duckdb connection "model.job_intelligent.fact_job_offers"
[0m11:38:49.255347 [debug] [Thread-2 (]: On model.job_intelligent.fact_job_offers: BEGIN
[0m11:38:49.255722 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m11:38:49.256435 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m11:38:49.256817 [debug] [Thread-2 (]: Using duckdb connection "model.job_intelligent.fact_job_offers"
[0m11:38:49.257355 [debug] [Thread-2 (]: On model.job_intelligent.fact_job_offers: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.fact_job_offers"} */

  
    
    

    create  table
      "duckdb"."main_gold"."fact_job_offers__dbt_tmp"
  
    as (
      -- models/gold/fact_job_offers.sql
-- Gold layer: Fact Table - Job Offers (Schéma en Étoile)



WITH jobs AS (
    SELECT
        j.*
    FROM "duckdb"."main_silver"."int_job_title_normalization" j
),

companies AS (
    SELECT * FROM "duckdb"."main_gold"."dim_company"
),

locations AS (
    SELECT * FROM "duckdb"."main_gold"."dim_location"
),

times AS (
    SELECT * FROM "duckdb"."main_gold"."dim_time"
),

fact_table AS (
    SELECT
        -- Surrogate keys
        ROW_NUMBER() OVER (ORDER BY j.job_url, j.company_name_cleaned) as job_offer_id,
        
        -- Foreign keys
        c.company_id,
        l.location_id,
        t.date_id as published_date_id,
        
        -- Job dimensions
        j.job_title_cleaned as job_title,
        j.job_category,
        j.contract_type_normalized as contract_type,
        j.work_type_normalized as work_type,
        
        -- URLs
        j.job_url,
        j.company_url,
        
        -- Description
        j.job_description_cleaned as job_description,
        
        -- Time dimension
        j.published_date,
        j.posted_time,
        j.published_year_month,
        j.published_year,
        j.published_month,
        
        -- Metrics
        LENGTH(j.job_description_cleaned) as description_length,
        (LENGTH(j.job_description_cleaned) - LENGTH(REPLACE(j.job_description_cleaned, ' ', ''))) + 1 as word_count,
        
        -- Flags
        CASE WHEN j.work_type_normalized = 'Remote' THEN 1 ELSE 0 END as is_remote,
        CASE WHEN j.contract_type_normalized = 'Permanent' THEN 1 ELSE 0 END as is_permanent,
        
        -- Metadata
        NOW() as created_at,
        j.ingestion_timestamp
        
    FROM jobs j
    LEFT JOIN companies c ON j.company_name_cleaned = c.company_name
    LEFT JOIN locations l ON j.location_cleaned = l.location_raw
    LEFT JOIN times t ON j.published_date = t.date_id
)

SELECT
    job_offer_id,
    company_id,
    location_id,
    published_date_id,
    job_title,
    job_category,
    contract_type,
    work_type,
    job_url,
    company_url,
    job_description,
    published_date,
    posted_time,
    published_year_month,
    published_year,
    published_month,
    description_length,
    word_count,
    is_remote,
    is_permanent,
    created_at,
    ingestion_timestamp
FROM fact_table
ORDER BY published_date DESC, job_offer_id
    );
  
  
[0m11:38:49.379703 [debug] [Thread-2 (]: SQL status: OK in 0.122 seconds
[0m11:38:49.382530 [debug] [Thread-2 (]: Using duckdb connection "model.job_intelligent.fact_job_offers"
[0m11:38:49.382877 [debug] [Thread-2 (]: On model.job_intelligent.fact_job_offers: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.fact_job_offers"} */
alter table "duckdb"."main_gold"."fact_job_offers__dbt_tmp" rename to "fact_job_offers"
[0m11:38:49.383558 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m11:38:49.384836 [debug] [Thread-2 (]: On model.job_intelligent.fact_job_offers: COMMIT
[0m11:38:49.385171 [debug] [Thread-2 (]: Using duckdb connection "model.job_intelligent.fact_job_offers"
[0m11:38:49.385486 [debug] [Thread-2 (]: On model.job_intelligent.fact_job_offers: COMMIT
[0m11:38:49.427329 [debug] [Thread-2 (]: SQL status: OK in 0.041 seconds
[0m11:38:49.430521 [debug] [Thread-2 (]: Using duckdb connection "model.job_intelligent.fact_job_offers"
[0m11:38:49.431046 [debug] [Thread-2 (]: On model.job_intelligent.fact_job_offers: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.fact_job_offers"} */

      drop table if exists "duckdb"."main_gold"."fact_job_offers__dbt_backup" cascade
    
[0m11:38:49.431955 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m11:38:49.433658 [debug] [Thread-2 (]: On model.job_intelligent.fact_job_offers: Close
[0m11:38:49.434299 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '302cd842-b611-46d5-9b41-edaf36bf90e4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000294DB5EF8F0>]}
[0m11:38:49.435062 [info ] [Thread-2 (]: 8 of 13 OK created sql table model main_gold.fact_job_offers ................... [[32mOK[0m in 0.19s]
[0m11:38:49.435868 [debug] [Thread-2 (]: Finished running node model.job_intelligent.fact_job_offers
[0m11:38:49.436606 [debug] [Thread-4 (]: Began running node model.job_intelligent.agg_job_offers_by_category_time
[0m11:38:49.437578 [debug] [Thread-1 (]: Began running node model.job_intelligent.agg_location_analysis
[0m11:38:49.437144 [info ] [Thread-4 (]: 9 of 13 START sql table model main_gold.agg_job_offers_by_category_time ........ [RUN]
[0m11:38:49.438272 [info ] [Thread-1 (]: 10 of 13 START sql table model main_gold.agg_location_analysis ................. [RUN]
[0m11:38:49.438853 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly model.job_intelligent.dim_company, now model.job_intelligent.agg_job_offers_by_category_time)
[0m11:38:49.439320 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.job_intelligent.dim_location, now model.job_intelligent.agg_location_analysis)
[0m11:38:49.439775 [debug] [Thread-4 (]: Began compiling node model.job_intelligent.agg_job_offers_by_category_time
[0m11:38:49.440261 [debug] [Thread-1 (]: Began compiling node model.job_intelligent.agg_location_analysis
[0m11:38:49.444273 [debug] [Thread-4 (]: Writing injected SQL for node "model.job_intelligent.agg_job_offers_by_category_time"
[0m11:38:49.447515 [debug] [Thread-1 (]: Writing injected SQL for node "model.job_intelligent.agg_location_analysis"
[0m11:38:49.448764 [debug] [Thread-4 (]: Began executing node model.job_intelligent.agg_job_offers_by_category_time
[0m11:38:49.451279 [debug] [Thread-4 (]: Writing runtime sql for node "model.job_intelligent.agg_job_offers_by_category_time"
[0m11:38:49.451974 [debug] [Thread-1 (]: Began executing node model.job_intelligent.agg_location_analysis
[0m11:38:49.454997 [debug] [Thread-1 (]: Writing runtime sql for node "model.job_intelligent.agg_location_analysis"
[0m11:38:49.455808 [debug] [Thread-4 (]: Using duckdb connection "model.job_intelligent.agg_job_offers_by_category_time"
[0m11:38:49.456227 [debug] [Thread-4 (]: On model.job_intelligent.agg_job_offers_by_category_time: BEGIN
[0m11:38:49.456570 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m11:38:49.457374 [debug] [Thread-4 (]: SQL status: OK in 0.001 seconds
[0m11:38:49.457826 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.agg_location_analysis"
[0m11:38:49.458232 [debug] [Thread-4 (]: Using duckdb connection "model.job_intelligent.agg_job_offers_by_category_time"
[0m11:38:49.458705 [debug] [Thread-1 (]: On model.job_intelligent.agg_location_analysis: BEGIN
[0m11:38:49.459393 [debug] [Thread-4 (]: On model.job_intelligent.agg_job_offers_by_category_time: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.agg_job_offers_by_category_time"} */

  
    
    

    create  table
      "duckdb"."main_gold"."agg_job_offers_by_category_time__dbt_tmp"
  
    as (
      -- models/gold/agg_job_offers_by_category_time.sql
-- Gold layer: Aggregate - Job Offers by Category and Time



SELECT
    f.published_year,
    f.published_month,
    f.published_year_month,
    f.job_category,
    f.contract_type,
    f.work_type,
    
    COUNT(DISTINCT f.job_offer_id) as count_job_offers,
    COUNT(DISTINCT f.company_id) as count_companies,
    
    AVG(f.description_length) as avg_description_length,
    AVG(f.word_count) as avg_word_count,
    
    SUM(CASE WHEN f.is_remote = 1 THEN 1 ELSE 0 END) as remote_jobs,
    SUM(CASE WHEN f.is_permanent = 1 THEN 1 ELSE 0 END) as permanent_jobs,
    
    NOW() as created_at
    
FROM "duckdb"."main_gold"."fact_job_offers" f
WHERE f.published_date IS NOT NULL
GROUP BY
    f.published_year,
    f.published_month,
    f.published_year_month,
    f.job_category,
    f.contract_type,
    f.work_type
ORDER BY f.published_year_month DESC, f.job_category
    );
  
  
[0m11:38:49.460079 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:38:49.461441 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m11:38:49.461852 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.agg_location_analysis"
[0m11:38:49.462329 [debug] [Thread-1 (]: On model.job_intelligent.agg_location_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.agg_location_analysis"} */

  
    
    

    create  table
      "duckdb"."main_gold"."agg_location_analysis__dbt_tmp"
  
    as (
      -- models/gold/agg_location_analysis.sql
-- Gold layer: Aggregate - Location Analysis



SELECT
    dl.location_id,
    dl.location_raw,
    dl.city,
    dl.country,
    dl.work_location_type,
    
    COUNT(DISTINCT f.job_offer_id) as count_job_offers,
    COUNT(DISTINCT f.company_id) as count_companies,
    
    -- Distribution by job category
    COUNT(DISTINCT CASE WHEN f.job_category = 'Data Engineer' THEN f.job_offer_id END) as data_engineer_count,
    COUNT(DISTINCT CASE WHEN f.job_category = 'Data Scientist' THEN f.job_offer_id END) as data_scientist_count,
    COUNT(DISTINCT CASE WHEN f.job_category = 'Data Analyst' THEN f.job_offer_id END) as data_analyst_count,
    COUNT(DISTINCT CASE WHEN f.job_category = 'ML Engineer' THEN f.job_offer_id END) as ml_engineer_count,
    
    -- Remote percentage
    ROUND(
        100.0 * SUM(CASE WHEN f.is_remote = 1 THEN 1 ELSE 0 END) / COUNT(DISTINCT f.job_offer_id),
        2
    ) as pct_remote,
    
    NOW() as created_at
    
FROM "duckdb"."main_gold"."dim_location" dl
LEFT JOIN "duckdb"."main_gold"."fact_job_offers" f ON dl.location_id = f.location_id
GROUP BY
    dl.location_id,
    dl.location_raw,
    dl.city,
    dl.country,
    dl.work_location_type
ORDER BY count_job_offers DESC
    );
  
  
[0m11:38:49.473154 [debug] [Thread-4 (]: SQL status: OK in 0.012 seconds
[0m11:38:49.477558 [debug] [Thread-4 (]: Using duckdb connection "model.job_intelligent.agg_job_offers_by_category_time"
[0m11:38:49.478263 [debug] [Thread-4 (]: On model.job_intelligent.agg_job_offers_by_category_time: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.agg_job_offers_by_category_time"} */
alter table "duckdb"."main_gold"."agg_job_offers_by_category_time__dbt_tmp" rename to "agg_job_offers_by_category_time"
[0m11:38:49.480082 [debug] [Thread-4 (]: SQL status: OK in 0.001 seconds
[0m11:38:49.482139 [debug] [Thread-4 (]: On model.job_intelligent.agg_job_offers_by_category_time: COMMIT
[0m11:38:49.482639 [debug] [Thread-4 (]: Using duckdb connection "model.job_intelligent.agg_job_offers_by_category_time"
[0m11:38:49.483052 [debug] [Thread-4 (]: On model.job_intelligent.agg_job_offers_by_category_time: COMMIT
[0m11:38:49.487612 [debug] [Thread-4 (]: SQL status: OK in 0.004 seconds
[0m11:38:49.489979 [debug] [Thread-4 (]: Using duckdb connection "model.job_intelligent.agg_job_offers_by_category_time"
[0m11:38:49.490368 [debug] [Thread-4 (]: On model.job_intelligent.agg_job_offers_by_category_time: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.agg_job_offers_by_category_time"} */

      drop table if exists "duckdb"."main_gold"."agg_job_offers_by_category_time__dbt_backup" cascade
    
[0m11:38:49.490901 [debug] [Thread-1 (]: SQL status: OK in 0.028 seconds
[0m11:38:49.493561 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.agg_location_analysis"
[0m11:38:49.494026 [debug] [Thread-1 (]: On model.job_intelligent.agg_location_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.agg_location_analysis"} */
alter table "duckdb"."main_gold"."agg_location_analysis__dbt_tmp" rename to "agg_location_analysis"
[0m11:38:49.494684 [debug] [Thread-4 (]: SQL status: OK in 0.004 seconds
[0m11:38:49.495969 [debug] [Thread-4 (]: On model.job_intelligent.agg_job_offers_by_category_time: Close
[0m11:38:49.496467 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '302cd842-b611-46d5-9b41-edaf36bf90e4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000294DB5EC1D0>]}
[0m11:38:49.497054 [info ] [Thread-4 (]: 9 of 13 OK created sql table model main_gold.agg_job_offers_by_category_time ... [[32mOK[0m in 0.06s]
[0m11:38:49.497700 [debug] [Thread-4 (]: Finished running node model.job_intelligent.agg_job_offers_by_category_time
[0m11:38:49.498284 [debug] [Thread-1 (]: SQL status: OK in 0.004 seconds
[0m11:38:49.499661 [debug] [Thread-1 (]: On model.job_intelligent.agg_location_analysis: COMMIT
[0m11:38:49.500025 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.agg_location_analysis"
[0m11:38:49.500365 [debug] [Thread-1 (]: On model.job_intelligent.agg_location_analysis: COMMIT
[0m11:38:49.503063 [debug] [Thread-1 (]: SQL status: OK in 0.002 seconds
[0m11:38:49.505063 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.agg_location_analysis"
[0m11:38:49.505427 [debug] [Thread-1 (]: On model.job_intelligent.agg_location_analysis: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.agg_location_analysis"} */

      drop table if exists "duckdb"."main_gold"."agg_location_analysis__dbt_backup" cascade
    
[0m11:38:49.506071 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m11:38:49.507263 [debug] [Thread-1 (]: On model.job_intelligent.agg_location_analysis: Close
[0m11:38:49.507756 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '302cd842-b611-46d5-9b41-edaf36bf90e4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000294DB6A58B0>]}
[0m11:38:49.508314 [info ] [Thread-1 (]: 10 of 13 OK created sql table model main_gold.agg_location_analysis ............ [[32mOK[0m in 0.07s]
[0m11:38:49.509169 [debug] [Thread-1 (]: Finished running node model.job_intelligent.agg_location_analysis
[0m11:38:51.083508 [debug] [Thread-3 (]: SQL status: OK in 1.894 seconds
[0m11:38:51.086429 [debug] [Thread-3 (]: Using duckdb connection "model.job_intelligent.int_skills_extraction"
[0m11:38:51.086976 [debug] [Thread-3 (]: On model.job_intelligent.int_skills_extraction: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.int_skills_extraction"} */
alter table "duckdb"."main_silver"."int_skills_extraction__dbt_tmp" rename to "int_skills_extraction"
[0m11:38:51.087999 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m11:38:51.089461 [debug] [Thread-3 (]: On model.job_intelligent.int_skills_extraction: COMMIT
[0m11:38:51.089849 [debug] [Thread-3 (]: Using duckdb connection "model.job_intelligent.int_skills_extraction"
[0m11:38:51.090206 [debug] [Thread-3 (]: On model.job_intelligent.int_skills_extraction: COMMIT
[0m11:38:51.137538 [debug] [Thread-3 (]: SQL status: OK in 0.047 seconds
[0m11:38:51.142156 [debug] [Thread-3 (]: Using duckdb connection "model.job_intelligent.int_skills_extraction"
[0m11:38:51.142579 [debug] [Thread-3 (]: On model.job_intelligent.int_skills_extraction: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.int_skills_extraction"} */

      drop table if exists "duckdb"."main_silver"."int_skills_extraction__dbt_backup" cascade
    
[0m11:38:51.143363 [debug] [Thread-3 (]: SQL status: OK in 0.000 seconds
[0m11:38:51.144822 [debug] [Thread-3 (]: On model.job_intelligent.int_skills_extraction: Close
[0m11:38:51.145404 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '302cd842-b611-46d5-9b41-edaf36bf90e4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000294DC77C4D0>]}
[0m11:38:51.146056 [info ] [Thread-3 (]: 7 of 13 OK created sql table model main_silver.int_skills_extraction ........... [[32mOK[0m in 2.05s]
[0m11:38:51.146760 [debug] [Thread-3 (]: Finished running node model.job_intelligent.int_skills_extraction
[0m11:38:51.147651 [debug] [Thread-2 (]: Began running node model.job_intelligent.dim_skills
[0m11:38:51.148170 [info ] [Thread-2 (]: 11 of 13 START sql table model main_gold.dim_skills ............................ [RUN]
[0m11:38:51.148686 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly model.job_intelligent.fact_job_offers, now model.job_intelligent.dim_skills)
[0m11:38:51.149096 [debug] [Thread-2 (]: Began compiling node model.job_intelligent.dim_skills
[0m11:38:51.152711 [debug] [Thread-2 (]: Writing injected SQL for node "model.job_intelligent.dim_skills"
[0m11:38:51.153616 [debug] [Thread-2 (]: Began executing node model.job_intelligent.dim_skills
[0m11:38:51.157218 [debug] [Thread-2 (]: Writing runtime sql for node "model.job_intelligent.dim_skills"
[0m11:38:51.158060 [debug] [Thread-2 (]: Using duckdb connection "model.job_intelligent.dim_skills"
[0m11:38:51.158493 [debug] [Thread-2 (]: On model.job_intelligent.dim_skills: BEGIN
[0m11:38:51.158882 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m11:38:51.159639 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m11:38:51.160040 [debug] [Thread-2 (]: Using duckdb connection "model.job_intelligent.dim_skills"
[0m11:38:51.160505 [debug] [Thread-2 (]: On model.job_intelligent.dim_skills: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_skills"} */

  
    
    

    create  table
      "duckdb"."main_gold"."dim_skills__dbt_tmp"
  
    as (
      -- models/gold/dim_skills.sql
-- Gold layer: Dimension Skills



WITH skills AS (
    SELECT DISTINCT
        skill_name
    FROM "duckdb"."main_silver"."int_skills_extraction"
    WHERE skill_name IS NOT NULL
),

skill_categorization AS (
    SELECT
        skill_name,
        CASE
            WHEN skill_name IN ('Python', 'Java', 'Scala', 'R') THEN 'Programming Language'
            WHEN skill_name IN ('SQL', 'NoSQL') THEN 'Database'
            WHEN skill_name IN ('Spark', 'Hadoop', 'Hive', 'Kafka') THEN 'Big Data Framework'
            WHEN skill_name IN ('TensorFlow', 'PyTorch', 'Scikit-learn') THEN 'ML/DL Library'
            WHEN skill_name IN ('AWS', 'Azure', 'GCP') THEN 'Cloud Platform'
            WHEN skill_name IN ('Tableau', 'Power BI', 'Looker') THEN 'BI Tool'
            WHEN skill_name IN ('Airflow', 'DBT', 'Kubernetes', 'Docker') THEN 'DataOps/DevOps'
            WHEN skill_name IN ('Pandas', 'NumPy', 'Matplotlib') THEN 'Data Analysis Library'
            WHEN skill_name IN ('Machine Learning', 'Statistics', 'Data Visualization') THEN 'Domain Knowledge'
            ELSE 'Other'
        END as skill_category
    FROM skills
),

ranked_skills AS (
    SELECT
        ROW_NUMBER() OVER (ORDER BY skill_name) as skill_id,
        skill_name,
        skill_category,
        NOW() as created_at
    FROM skill_categorization
)

SELECT
    skill_id,
    skill_name,
    skill_category,
    created_at
FROM ranked_skills
ORDER BY skill_id
    );
  
  
[0m11:38:51.165903 [debug] [Thread-2 (]: SQL status: OK in 0.005 seconds
[0m11:38:51.169159 [debug] [Thread-2 (]: Using duckdb connection "model.job_intelligent.dim_skills"
[0m11:38:51.169621 [debug] [Thread-2 (]: On model.job_intelligent.dim_skills: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_skills"} */
alter table "duckdb"."main_gold"."dim_skills__dbt_tmp" rename to "dim_skills"
[0m11:38:51.170508 [debug] [Thread-2 (]: SQL status: OK in 0.000 seconds
[0m11:38:51.172141 [debug] [Thread-2 (]: On model.job_intelligent.dim_skills: COMMIT
[0m11:38:51.172593 [debug] [Thread-2 (]: Using duckdb connection "model.job_intelligent.dim_skills"
[0m11:38:51.173016 [debug] [Thread-2 (]: On model.job_intelligent.dim_skills: COMMIT
[0m11:38:51.527622 [debug] [Thread-2 (]: SQL status: OK in 0.354 seconds
[0m11:38:51.533233 [debug] [Thread-2 (]: Using duckdb connection "model.job_intelligent.dim_skills"
[0m11:38:51.534001 [debug] [Thread-2 (]: On model.job_intelligent.dim_skills: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_skills"} */

      drop table if exists "duckdb"."main_gold"."dim_skills__dbt_backup" cascade
    
[0m11:38:51.535240 [debug] [Thread-2 (]: SQL status: OK in 0.001 seconds
[0m11:38:51.537556 [debug] [Thread-2 (]: On model.job_intelligent.dim_skills: Close
[0m11:38:51.538417 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '302cd842-b611-46d5-9b41-edaf36bf90e4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000294DC77C6B0>]}
[0m11:38:51.539349 [info ] [Thread-2 (]: 11 of 13 OK created sql table model main_gold.dim_skills ....................... [[32mOK[0m in 0.39s]
[0m11:38:51.540367 [debug] [Thread-2 (]: Finished running node model.job_intelligent.dim_skills
[0m11:38:51.541522 [debug] [Thread-4 (]: Began running node model.job_intelligent.fact_job_skills
[0m11:38:51.542278 [info ] [Thread-4 (]: 12 of 13 START sql table model main_gold.fact_job_skills ....................... [RUN]
[0m11:38:51.542918 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly model.job_intelligent.agg_job_offers_by_category_time, now model.job_intelligent.fact_job_skills)
[0m11:38:51.543437 [debug] [Thread-4 (]: Began compiling node model.job_intelligent.fact_job_skills
[0m11:38:51.548099 [debug] [Thread-4 (]: Writing injected SQL for node "model.job_intelligent.fact_job_skills"
[0m11:38:51.549044 [debug] [Thread-4 (]: Began executing node model.job_intelligent.fact_job_skills
[0m11:38:51.552784 [debug] [Thread-4 (]: Writing runtime sql for node "model.job_intelligent.fact_job_skills"
[0m11:38:51.553627 [debug] [Thread-4 (]: Using duckdb connection "model.job_intelligent.fact_job_skills"
[0m11:38:51.554055 [debug] [Thread-4 (]: On model.job_intelligent.fact_job_skills: BEGIN
[0m11:38:51.554441 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m11:38:51.555182 [debug] [Thread-4 (]: SQL status: OK in 0.001 seconds
[0m11:38:51.555580 [debug] [Thread-4 (]: Using duckdb connection "model.job_intelligent.fact_job_skills"
[0m11:38:51.556032 [debug] [Thread-4 (]: On model.job_intelligent.fact_job_skills: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.fact_job_skills"} */

  
    
    

    create  table
      "duckdb"."main_gold"."fact_job_skills__dbt_tmp"
  
    as (
      -- models/gold/fact_job_skills.sql
-- Gold layer: Bridge Table - Job Skills



WITH skills_raw AS (
    SELECT DISTINCT
        job_url,
        company_name_cleaned,
        skill_name
    FROM "duckdb"."main_silver"."int_skills_extraction"
),

jobs AS (
    SELECT
        job_offer_id,
        job_url
    FROM "duckdb"."main_gold"."fact_job_offers"
),

skills_dim AS (
    SELECT
        skill_id,
        skill_name
    FROM "duckdb"."main_gold"."dim_skills"
),

fact_table AS (
    SELECT
        ROW_NUMBER() OVER (ORDER BY s.job_url, sd.skill_id) as job_skill_id,
        j.job_offer_id,
        sd.skill_id,
        s.skill_name,
        NOW() as created_at
    FROM skills_raw s
    LEFT JOIN jobs j ON s.job_url = j.job_url
    LEFT JOIN skills_dim sd ON s.skill_name = sd.skill_name
)

SELECT
    job_skill_id,
    job_offer_id,
    skill_id,
    skill_name,
    created_at
FROM fact_table
WHERE job_offer_id IS NOT NULL
ORDER BY job_offer_id, skill_id
    );
  
  
[0m11:38:51.572556 [debug] [Thread-4 (]: SQL status: OK in 0.016 seconds
[0m11:38:51.576555 [debug] [Thread-4 (]: Using duckdb connection "model.job_intelligent.fact_job_skills"
[0m11:38:51.577120 [debug] [Thread-4 (]: On model.job_intelligent.fact_job_skills: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.fact_job_skills"} */
alter table "duckdb"."main_gold"."fact_job_skills__dbt_tmp" rename to "fact_job_skills"
[0m11:38:51.578125 [debug] [Thread-4 (]: SQL status: OK in 0.000 seconds
[0m11:38:51.580069 [debug] [Thread-4 (]: On model.job_intelligent.fact_job_skills: COMMIT
[0m11:38:51.580571 [debug] [Thread-4 (]: Using duckdb connection "model.job_intelligent.fact_job_skills"
[0m11:38:51.581040 [debug] [Thread-4 (]: On model.job_intelligent.fact_job_skills: COMMIT
[0m11:38:51.585706 [debug] [Thread-4 (]: SQL status: OK in 0.004 seconds
[0m11:38:51.588671 [debug] [Thread-4 (]: Using duckdb connection "model.job_intelligent.fact_job_skills"
[0m11:38:51.589190 [debug] [Thread-4 (]: On model.job_intelligent.fact_job_skills: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.fact_job_skills"} */

      drop table if exists "duckdb"."main_gold"."fact_job_skills__dbt_backup" cascade
    
[0m11:38:51.590075 [debug] [Thread-4 (]: SQL status: OK in 0.000 seconds
[0m11:38:51.592060 [debug] [Thread-4 (]: On model.job_intelligent.fact_job_skills: Close
[0m11:38:51.592931 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '302cd842-b611-46d5-9b41-edaf36bf90e4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000294DC77D130>]}
[0m11:38:51.593942 [info ] [Thread-4 (]: 12 of 13 OK created sql table model main_gold.fact_job_skills .................. [[32mOK[0m in 0.05s]
[0m11:38:51.595029 [debug] [Thread-4 (]: Finished running node model.job_intelligent.fact_job_skills
[0m11:38:51.596248 [debug] [Thread-1 (]: Began running node model.job_intelligent.agg_skills_demand
[0m11:38:51.597009 [info ] [Thread-1 (]: 13 of 13 START sql table model main_gold.agg_skills_demand ..................... [RUN]
[0m11:38:51.597748 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.job_intelligent.agg_location_analysis, now model.job_intelligent.agg_skills_demand)
[0m11:38:51.598319 [debug] [Thread-1 (]: Began compiling node model.job_intelligent.agg_skills_demand
[0m11:38:51.604092 [debug] [Thread-1 (]: Writing injected SQL for node "model.job_intelligent.agg_skills_demand"
[0m11:38:51.605288 [debug] [Thread-1 (]: Began executing node model.job_intelligent.agg_skills_demand
[0m11:38:51.610341 [debug] [Thread-1 (]: Writing runtime sql for node "model.job_intelligent.agg_skills_demand"
[0m11:38:51.611600 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.agg_skills_demand"
[0m11:38:51.612174 [debug] [Thread-1 (]: On model.job_intelligent.agg_skills_demand: BEGIN
[0m11:38:51.612708 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:38:51.613664 [debug] [Thread-1 (]: SQL status: OK in 0.001 seconds
[0m11:38:51.614201 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.agg_skills_demand"
[0m11:38:51.614820 [debug] [Thread-1 (]: On model.job_intelligent.agg_skills_demand: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.agg_skills_demand"} */

  
    
    

    create  table
      "duckdb"."main_gold"."agg_skills_demand__dbt_tmp"
  
    as (
      -- models/gold/agg_skills_demand.sql
-- Gold layer: Aggregate - Skills in Demand



SELECT
    sd.skill_id,
    sd.skill_name,
    sd.skill_category,
    
    COUNT(DISTINCT fs.job_offer_id) as count_jobs_requiring_skill,
    COUNT(DISTINCT f.company_id) as count_companies_requiring_skill,
    
    -- Percentage of all jobs
    ROUND(
        100.0 * COUNT(DISTINCT fs.job_offer_id) / (
            SELECT COUNT(DISTINCT job_offer_id) FROM "duckdb"."main_gold"."fact_job_offers"
        ),
        2
    ) as pct_of_total_jobs,
    
    -- Average job details for jobs requiring this skill
    AVG(f.description_length) as avg_description_length,
    AVG(f.word_count) as avg_word_count,
    
    NOW() as created_at
    
FROM "duckdb"."main_gold"."dim_skills" sd
LEFT JOIN "duckdb"."main_gold"."fact_job_skills" fs ON sd.skill_id = fs.skill_id
LEFT JOIN "duckdb"."main_gold"."fact_job_offers" f ON fs.job_offer_id = f.job_offer_id
GROUP BY
    sd.skill_id,
    sd.skill_name,
    sd.skill_category
ORDER BY count_jobs_requiring_skill DESC
    );
  
  
[0m11:38:51.629067 [debug] [Thread-1 (]: SQL status: OK in 0.014 seconds
[0m11:38:51.632124 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.agg_skills_demand"
[0m11:38:51.632572 [debug] [Thread-1 (]: On model.job_intelligent.agg_skills_demand: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.agg_skills_demand"} */
alter table "duckdb"."main_gold"."agg_skills_demand__dbt_tmp" rename to "agg_skills_demand"
[0m11:38:51.636271 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m11:38:51.637875 [debug] [Thread-1 (]: On model.job_intelligent.agg_skills_demand: COMMIT
[0m11:38:51.638304 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.agg_skills_demand"
[0m11:38:51.638717 [debug] [Thread-1 (]: On model.job_intelligent.agg_skills_demand: COMMIT
[0m11:38:51.641963 [debug] [Thread-1 (]: SQL status: OK in 0.003 seconds
[0m11:38:51.644443 [debug] [Thread-1 (]: Using duckdb connection "model.job_intelligent.agg_skills_demand"
[0m11:38:51.644876 [debug] [Thread-1 (]: On model.job_intelligent.agg_skills_demand: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.agg_skills_demand"} */

      drop table if exists "duckdb"."main_gold"."agg_skills_demand__dbt_backup" cascade
    
[0m11:38:51.645647 [debug] [Thread-1 (]: SQL status: OK in 0.000 seconds
[0m11:38:51.647063 [debug] [Thread-1 (]: On model.job_intelligent.agg_skills_demand: Close
[0m11:38:51.647633 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '302cd842-b611-46d5-9b41-edaf36bf90e4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000294DB5ED550>]}
[0m11:38:51.648296 [info ] [Thread-1 (]: 13 of 13 OK created sql table model main_gold.agg_skills_demand ................ [[32mOK[0m in 0.05s]
[0m11:38:51.649005 [debug] [Thread-1 (]: Finished running node model.job_intelligent.agg_skills_demand
[0m11:38:51.650793 [debug] [MainThread]: Using duckdb connection "master"
[0m11:38:51.651196 [debug] [MainThread]: On master: BEGIN
[0m11:38:51.651551 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m11:38:51.652315 [debug] [MainThread]: SQL status: OK in 0.001 seconds
[0m11:38:51.652704 [debug] [MainThread]: On master: COMMIT
[0m11:38:51.653080 [debug] [MainThread]: Using duckdb connection "master"
[0m11:38:51.653466 [debug] [MainThread]: On master: COMMIT
[0m11:38:51.654114 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m11:38:51.654494 [debug] [MainThread]: On master: Close
[0m11:38:51.655004 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:38:51.655357 [debug] [MainThread]: Connection 'create_duckdb_main_silver' was properly closed.
[0m11:38:51.655716 [debug] [MainThread]: Connection 'create_duckdb_main_bronze' was properly closed.
[0m11:38:51.656083 [debug] [MainThread]: Connection 'create_duckdb_main_gold' was properly closed.
[0m11:38:51.656448 [debug] [MainThread]: Connection 'list_duckdb_main_silver' was properly closed.
[0m11:38:51.656823 [debug] [MainThread]: Connection 'list_duckdb_main_bronze' was properly closed.
[0m11:38:51.657233 [debug] [MainThread]: Connection 'list_duckdb_main_gold' was properly closed.
[0m11:38:51.657638 [debug] [MainThread]: Connection 'model.job_intelligent.agg_skills_demand' was properly closed.
[0m11:38:51.658053 [debug] [MainThread]: Connection 'model.job_intelligent.dim_skills' was properly closed.
[0m11:38:51.658416 [debug] [MainThread]: Connection 'model.job_intelligent.int_skills_extraction' was properly closed.
[0m11:38:51.658840 [debug] [MainThread]: Connection 'model.job_intelligent.fact_job_skills' was properly closed.
[0m11:38:51.659424 [info ] [MainThread]: 
[0m11:38:51.659970 [info ] [MainThread]: Finished running 12 table models, 1 view model in 0 hours 0 minutes and 3.50 seconds (3.50s).
[0m11:38:51.663687 [debug] [MainThread]: Command end result
[0m11:38:51.692952 [debug] [MainThread]: Wrote artifact WritableManifest to D:\lab2\dbt_project\target\manifest.json
[0m11:38:51.695368 [debug] [MainThread]: Wrote artifact SemanticManifest to D:\lab2\dbt_project\target\semantic_manifest.json
[0m11:38:51.702039 [debug] [MainThread]: Wrote artifact RunExecutionResult to D:\lab2\dbt_project\target\run_results.json
[0m11:38:51.702411 [info ] [MainThread]: 
[0m11:38:51.702814 [info ] [MainThread]: [32mCompleted successfully[0m
[0m11:38:51.703180 [info ] [MainThread]: 
[0m11:38:51.703541 [info ] [MainThread]: Done. PASS=13 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=13
[0m11:38:51.704464 [debug] [MainThread]: Command `dbt run` succeeded at 11:38:51.704347 after 5.64 seconds
[0m11:38:51.704838 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000294D87567B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000294D8CA34D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000294DB6A5C70>]}
[0m11:38:51.705220 [debug] [MainThread]: Flushing usage events
[0m11:38:53.068878 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:02:28.363267 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012A599720D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012A5AC74CA0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012A5AC74E50>]}


============================== 15:02:28.379528 | b1328de2-8858-489f-a00c-a24bbb996e12 ==============================
[0m15:02:28.379528 [info ] [MainThread]: Running with dbt=1.10.18
[0m15:02:28.382451 [debug] [MainThread]: running dbt with arguments {'log_path': 'C:\\Users\\Ayoub Gorry\\Desktop\\powerbi\\jobs-power-bi\\dbt_project\\logs', 'log_format': 'default', 'partial_parse': 'True', 'log_cache_events': 'False', 'empty': 'None', 'version_check': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Ayoub Gorry\\Desktop\\powerbi\\jobs-power-bi\\dbt_project', 'no_print': 'None', 'send_anonymous_usage_stats': 'True', 'target_path': 'None', 'use_experimental_parser': 'False', 'introspect': 'True', 'invocation_command': 'dbt debug', 'printer_width': '80', 'debug': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'use_colors': 'True', 'quiet': 'False', 'static_parser': 'True', 'fail_fast': 'False', 'warn_error': 'None', 'write_json': 'True', 'indirect_selection': 'eager'}
[0m15:02:28.520759 [info ] [MainThread]: dbt version: 1.10.18
[0m15:02:28.523821 [info ] [MainThread]: python version: 3.9.13
[0m15:02:28.529357 [info ] [MainThread]: python path: C:\Users\Ayoub Gorry\AppData\Local\Programs\Python\Python39\python.exe
[0m15:02:28.529357 [info ] [MainThread]: os info: Windows-10-10.0.26100-SP0
[0m15:02:30.078524 [info ] [MainThread]: Using profiles dir at C:\Users\Ayoub Gorry\Desktop\powerbi\jobs-power-bi\dbt_project
[0m15:02:30.086972 [info ] [MainThread]: Using profiles.yml file at C:\Users\Ayoub Gorry\Desktop\powerbi\jobs-power-bi\dbt_project\profiles.yml
[0m15:02:30.098683 [info ] [MainThread]: Using dbt_project.yml file at C:\Users\Ayoub Gorry\Desktop\powerbi\jobs-power-bi\dbt_project\dbt_project.yml
[0m15:02:30.195205 [info ] [MainThread]: adapter type: duckdb
[0m15:02:30.199880 [info ] [MainThread]: adapter version: 1.10.0
[0m15:02:31.190392 [info ] [MainThread]: Configuration:
[0m15:02:31.216614 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m15:02:31.249367 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m15:02:31.257069 [info ] [MainThread]: Required dependencies:
[0m15:02:31.272249 [debug] [MainThread]: Executing "git --help"
[0m15:02:31.559462 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--no-lazy-fetch]\n           [--no-optional-locks] [--no-advice] [--bare] [--git-dir=<path>]\n           [--work-tree=<path>] [--namespace=<name>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone      Clone a repository into a new directory\n   init       Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add        Add file contents to the index\n   mv         Move or rename a file, a directory, or a symlink\n   restore    Restore working tree files\n   rm         Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect     Use binary search to find the commit that introduced a bug\n   diff       Show changes between commits, commit and working tree, etc\n   grep       Print lines matching a pattern\n   log        Show commit logs\n   show       Show various types of objects\n   status     Show the working tree status\n\ngrow, mark and tweak your common history\n   backfill   Download missing objects in a partial clone\n   branch     List, create, or delete branches\n   commit     Record changes to the repository\n   merge      Join two or more development histories together\n   rebase     Reapply commits on top of another base tip\n   reset      Reset current HEAD to the specified state\n   switch     Switch branches\n   tag        Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch      Download objects and refs from another repository\n   pull       Fetch from and integrate with another repository or a local branch\n   push       Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m15:02:31.563519 [debug] [MainThread]: STDERR: "b''"
[0m15:02:31.567050 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m15:02:31.570301 [info ] [MainThread]: Connection:
[0m15:02:31.574070 [info ] [MainThread]:   database: duckdb
[0m15:02:31.581194 [info ] [MainThread]:   schema: main
[0m15:02:31.590416 [info ] [MainThread]:   path: duckdb.db
[0m15:02:31.603660 [info ] [MainThread]:   config_options: None
[0m15:02:31.604998 [info ] [MainThread]:   extensions: None
[0m15:02:31.613017 [info ] [MainThread]:   settings: {}
[0m15:02:31.617057 [info ] [MainThread]:   external_root: .
[0m15:02:31.621269 [info ] [MainThread]:   use_credential_provider: None
[0m15:02:31.623292 [info ] [MainThread]:   attach: None
[0m15:02:31.632624 [info ] [MainThread]:   filesystems: None
[0m15:02:31.636405 [info ] [MainThread]:   remote: None
[0m15:02:31.641581 [info ] [MainThread]:   plugins: None
[0m15:02:31.649406 [info ] [MainThread]:   disable_transactions: False
[0m15:02:31.699042 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m15:02:33.210318 [debug] [MainThread]: Acquiring new duckdb connection 'debug'
[0m15:02:43.268456 [debug] [MainThread]: Using duckdb connection "debug"
[0m15:02:43.271596 [debug] [MainThread]: On debug: select 1 as id
[0m15:02:43.273124 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:02:43.385299 [debug] [MainThread]: SQL status: OK in 0.114 seconds
[0m15:02:43.387386 [debug] [MainThread]: On debug: Close
[0m15:02:43.389422 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m15:02:43.391183 [info ] [MainThread]: [32mAll checks passed![0m
[0m15:02:43.393768 [debug] [MainThread]: Command `dbt debug` succeeded at 15:02:43.393768 after 15.68 seconds
[0m15:02:43.394856 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m15:02:43.394856 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012A599720D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012A5BDCDF40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012A59D0FFD0>]}
[0m15:02:43.396871 [debug] [MainThread]: Flushing usage events
[0m15:02:44.263466 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:05:04.871843 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026503BC11F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026504E74820>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026504E74A60>]}


============================== 15:05:04.896269 | 4b030452-3182-4753-9f9f-2ed0fe0e6bde ==============================
[0m15:05:04.896269 [info ] [MainThread]: Running with dbt=1.10.18
[0m15:05:04.900973 [debug] [MainThread]: running dbt with arguments {'log_path': 'C:\\Users\\Ayoub Gorry\\Desktop\\powerbi\\jobs-power-bi\\dbt_project\\logs', 'log_format': 'default', 'partial_parse': 'True', 'log_cache_events': 'False', 'empty': 'None', 'version_check': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Ayoub Gorry\\Desktop\\powerbi\\jobs-power-bi\\dbt_project', 'use_experimental_parser': 'False', 'printer_width': '80', 'target_path': 'None', 'send_anonymous_usage_stats': 'True', 'introspect': 'True', 'invocation_command': 'dbt debug', 'debug': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'static_parser': 'True', 'use_colors': 'True', 'quiet': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'write_json': 'True', 'indirect_selection': 'eager'}
[0m15:05:05.004228 [info ] [MainThread]: dbt version: 1.10.18
[0m15:05:05.009221 [info ] [MainThread]: python version: 3.9.13
[0m15:05:05.010547 [info ] [MainThread]: python path: C:\Users\Ayoub Gorry\AppData\Local\Programs\Python\Python39\python.exe
[0m15:05:05.010547 [info ] [MainThread]: os info: Windows-10-10.0.26100-SP0
[0m15:05:05.579301 [info ] [MainThread]: Using profiles dir at C:\Users\Ayoub Gorry\Desktop\powerbi\jobs-power-bi\dbt_project
[0m15:05:05.583720 [info ] [MainThread]: Using profiles.yml file at C:\Users\Ayoub Gorry\Desktop\powerbi\jobs-power-bi\dbt_project\profiles.yml
[0m15:05:05.583720 [info ] [MainThread]: Using dbt_project.yml file at C:\Users\Ayoub Gorry\Desktop\powerbi\jobs-power-bi\dbt_project\dbt_project.yml
[0m15:05:05.599282 [info ] [MainThread]: adapter type: duckdb
[0m15:05:05.603639 [info ] [MainThread]: adapter version: 1.10.0
[0m15:05:06.253967 [info ] [MainThread]: Configuration:
[0m15:05:06.258503 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m15:05:06.263105 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m15:05:06.263105 [info ] [MainThread]: Required dependencies:
[0m15:05:06.269297 [debug] [MainThread]: Executing "git --help"
[0m15:05:06.430826 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--no-lazy-fetch]\n           [--no-optional-locks] [--no-advice] [--bare] [--git-dir=<path>]\n           [--work-tree=<path>] [--namespace=<name>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone      Clone a repository into a new directory\n   init       Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add        Add file contents to the index\n   mv         Move or rename a file, a directory, or a symlink\n   restore    Restore working tree files\n   rm         Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect     Use binary search to find the commit that introduced a bug\n   diff       Show changes between commits, commit and working tree, etc\n   grep       Print lines matching a pattern\n   log        Show commit logs\n   show       Show various types of objects\n   status     Show the working tree status\n\ngrow, mark and tweak your common history\n   backfill   Download missing objects in a partial clone\n   branch     List, create, or delete branches\n   commit     Record changes to the repository\n   merge      Join two or more development histories together\n   rebase     Reapply commits on top of another base tip\n   reset      Reset current HEAD to the specified state\n   switch     Switch branches\n   tag        Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch      Download objects and refs from another repository\n   pull       Fetch from and integrate with another repository or a local branch\n   push       Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m15:05:06.437701 [debug] [MainThread]: STDERR: "b''"
[0m15:05:06.439326 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m15:05:06.441806 [info ] [MainThread]: Connection:
[0m15:05:06.441806 [info ] [MainThread]:   database: duckdb
[0m15:05:06.450458 [info ] [MainThread]:   schema: main
[0m15:05:06.450458 [info ] [MainThread]:   path: duckdb.db
[0m15:05:06.450458 [info ] [MainThread]:   config_options: None
[0m15:05:06.459538 [info ] [MainThread]:   extensions: None
[0m15:05:06.463305 [info ] [MainThread]:   settings: {}
[0m15:05:06.463305 [info ] [MainThread]:   external_root: .
[0m15:05:06.470419 [info ] [MainThread]:   use_credential_provider: None
[0m15:05:06.471478 [info ] [MainThread]:   attach: None
[0m15:05:06.474042 [info ] [MainThread]:   filesystems: None
[0m15:05:06.478433 [info ] [MainThread]:   remote: None
[0m15:05:06.481271 [info ] [MainThread]:   plugins: None
[0m15:05:06.482125 [info ] [MainThread]:   disable_transactions: False
[0m15:05:06.482125 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m15:05:07.888744 [debug] [MainThread]: Acquiring new duckdb connection 'debug'
[0m15:05:08.309295 [debug] [MainThread]: Using duckdb connection "debug"
[0m15:05:08.309295 [debug] [MainThread]: On debug: select 1 as id
[0m15:05:08.314625 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:05:08.392742 [debug] [MainThread]: SQL status: OK in 0.076 seconds
[0m15:05:08.399261 [debug] [MainThread]: On debug: Close
[0m15:05:08.402626 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m15:05:08.406791 [info ] [MainThread]: [32mAll checks passed![0m
[0m15:05:08.411825 [debug] [MainThread]: Command `dbt debug` succeeded at 15:05:08.411227 after 3.95 seconds
[0m15:05:08.415908 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m15:05:08.419520 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026503BC11F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000265060B48B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026504E74AF0>]}
[0m15:05:08.420965 [debug] [MainThread]: Flushing usage events
[0m15:05:09.230676 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:05:21.592579 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016BD4BA2160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016BD4F70D30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016BD5E38520>]}


============================== 15:05:21.611134 | edd5c6bb-f3cd-483a-b6c7-dc20500065ba ==============================
[0m15:05:21.611134 [info ] [MainThread]: Running with dbt=1.10.18
[0m15:05:21.613831 [debug] [MainThread]: running dbt with arguments {'log_path': 'C:\\Users\\Ayoub Gorry\\Desktop\\powerbi\\jobs-power-bi\\dbt_project\\logs', 'log_format': 'default', 'partial_parse': 'True', 'log_cache_events': 'False', 'empty': 'False', 'version_check': 'True', 'cache_selected_only': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'use_experimental_parser': 'False', 'send_anonymous_usage_stats': 'True', 'debug': 'False', 'profiles_dir': 'C:\\Users\\Ayoub Gorry\\Desktop\\powerbi\\jobs-power-bi\\dbt_project', 'introspect': 'True', 'invocation_command': 'dbt run', 'no_print': 'None', 'target_path': 'None', 'printer_width': '80', 'quiet': 'False', 'use_colors': 'True', 'static_parser': 'True', 'warn_error': 'None', 'fail_fast': 'False', 'write_json': 'True', 'indirect_selection': 'eager'}
[0m15:05:23.669275 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'edd5c6bb-f3cd-483a-b6c7-dc20500065ba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016BD70661C0>]}
[0m15:05:24.169626 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'edd5c6bb-f3cd-483a-b6c7-dc20500065ba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016BD5E14E20>]}
[0m15:05:24.185212 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m15:05:25.775062 [debug] [MainThread]: checksum: 9646db7d1a5f5b9389d9c545d3ebf898b26bc1f63f6b5366f34a6af01d52cb91, vars: {}, profile: , target: , version: 1.10.18
[0m15:05:26.324348 [info ] [MainThread]: Unable to do partial parsing because of a version mismatch
[0m15:05:26.328520 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'edd5c6bb-f3cd-483a-b6c7-dc20500065ba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016BD55C4FD0>]}
[0m15:05:36.880778 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- seeds
[0m15:05:36.910010 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'edd5c6bb-f3cd-483a-b6c7-dc20500065ba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016BD7615040>]}
[0m15:05:37.259395 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Ayoub Gorry\Desktop\powerbi\jobs-power-bi\dbt_project\target\manifest.json
[0m15:05:37.321138 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Ayoub Gorry\Desktop\powerbi\jobs-power-bi\dbt_project\target\semantic_manifest.json
[0m15:05:37.420002 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'edd5c6bb-f3cd-483a-b6c7-dc20500065ba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016BD770F0A0>]}
[0m15:05:37.423349 [info ] [MainThread]: Found 13 models, 456 macros
[0m15:05:37.428510 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'edd5c6bb-f3cd-483a-b6c7-dc20500065ba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016BD764A9A0>]}
[0m15:05:37.436642 [info ] [MainThread]: 
[0m15:05:37.439233 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:05:37.442982 [info ] [MainThread]: 
[0m15:05:37.447364 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m15:05:37.487182 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_duckdb'
[0m15:05:37.490748 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_duckdb'
[0m15:05:37.511573 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_duckdb'
[0m15:05:38.053332 [debug] [ThreadPool]: Using duckdb connection "list_duckdb"
[0m15:05:38.055214 [debug] [ThreadPool]: Using duckdb connection "list_duckdb"
[0m15:05:38.058803 [debug] [ThreadPool]: Using duckdb connection "list_duckdb"
[0m15:05:38.060743 [debug] [ThreadPool]: On list_duckdb: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_duckdb"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"duckdb"'
    
  
  
[0m15:05:38.064355 [debug] [ThreadPool]: On list_duckdb: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_duckdb"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"duckdb"'
    
  
  
[0m15:05:38.069592 [debug] [ThreadPool]: On list_duckdb: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_duckdb"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"duckdb"'
    
  
  
[0m15:05:38.069592 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:05:38.076059 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:05:38.079065 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:05:38.205325 [debug] [ThreadPool]: SQL status: OK in 0.132 seconds
[0m15:05:38.209265 [debug] [ThreadPool]: SQL status: OK in 0.137 seconds
[0m15:05:38.211804 [debug] [ThreadPool]: SQL status: OK in 0.133 seconds
[0m15:05:38.219249 [debug] [ThreadPool]: On list_duckdb: Close
[0m15:05:38.220710 [debug] [ThreadPool]: On list_duckdb: Close
[0m15:05:38.236946 [debug] [ThreadPool]: On list_duckdb: Close
[0m15:05:38.265331 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_duckdb, now create_duckdb_main_silver)
[0m15:05:38.271513 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_duckdb, now create_duckdb_main_bronze)
[0m15:05:38.277840 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_duckdb, now create_duckdb_main_gold)
[0m15:05:38.284845 [debug] [ThreadPool]: Creating schema "database: "duckdb"
schema: "main_silver"
"
[0m15:05:38.290897 [debug] [ThreadPool]: Creating schema "database: "duckdb"
schema: "main_bronze"
"
[0m15:05:38.299258 [debug] [ThreadPool]: Creating schema "database: "duckdb"
schema: "main_gold"
"
[0m15:05:38.342022 [debug] [ThreadPool]: Using duckdb connection "create_duckdb_main_silver"
[0m15:05:38.358550 [debug] [ThreadPool]: Using duckdb connection "create_duckdb_main_bronze"
[0m15:05:38.372157 [debug] [ThreadPool]: Using duckdb connection "create_duckdb_main_gold"
[0m15:05:38.372157 [debug] [ThreadPool]: On create_duckdb_main_silver: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_duckdb_main_silver"} */

    
        select type from duckdb_databases()
        where lower(database_name)='duckdb'
        and type='sqlite'
    
  
[0m15:05:38.379532 [debug] [ThreadPool]: On create_duckdb_main_bronze: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_duckdb_main_bronze"} */

    
        select type from duckdb_databases()
        where lower(database_name)='duckdb'
        and type='sqlite'
    
  
[0m15:05:38.381891 [debug] [ThreadPool]: On create_duckdb_main_gold: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_duckdb_main_gold"} */

    
        select type from duckdb_databases()
        where lower(database_name)='duckdb'
        and type='sqlite'
    
  
[0m15:05:38.381891 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:05:38.389190 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:05:38.391214 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:05:38.399144 [debug] [ThreadPool]: SQL status: OK in 0.012 seconds
[0m15:05:38.399795 [debug] [ThreadPool]: SQL status: OK in 0.012 seconds
[0m15:05:38.399795 [debug] [ThreadPool]: Using duckdb connection "create_duckdb_main_silver"
[0m15:05:38.409345 [debug] [ThreadPool]: SQL status: OK in 0.017 seconds
[0m15:05:38.413250 [debug] [ThreadPool]: Using duckdb connection "create_duckdb_main_bronze"
[0m15:05:38.419368 [debug] [ThreadPool]: On create_duckdb_main_silver: BEGIN
[0m15:05:38.419368 [debug] [ThreadPool]: Using duckdb connection "create_duckdb_main_gold"
[0m15:05:38.419368 [debug] [ThreadPool]: On create_duckdb_main_bronze: BEGIN
[0m15:05:38.432041 [debug] [ThreadPool]: On create_duckdb_main_gold: BEGIN
[0m15:05:38.432041 [debug] [ThreadPool]: SQL status: OK in 0.004 seconds
[0m15:05:38.437861 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m15:05:38.441514 [debug] [ThreadPool]: Using duckdb connection "create_duckdb_main_silver"
[0m15:05:38.444554 [debug] [ThreadPool]: SQL status: OK in 0.004 seconds
[0m15:05:38.445329 [debug] [ThreadPool]: Using duckdb connection "create_duckdb_main_bronze"
[0m15:05:38.449330 [debug] [ThreadPool]: On create_duckdb_main_silver: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_duckdb_main_silver"} */

    
    
        create schema if not exists "duckdb"."main_silver"
    
[0m15:05:38.454218 [debug] [ThreadPool]: Using duckdb connection "create_duckdb_main_gold"
[0m15:05:38.454218 [debug] [ThreadPool]: On create_duckdb_main_bronze: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_duckdb_main_bronze"} */

    
    
        create schema if not exists "duckdb"."main_bronze"
    
[0m15:05:38.462307 [debug] [ThreadPool]: On create_duckdb_main_gold: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_duckdb_main_gold"} */

    
    
        create schema if not exists "duckdb"."main_gold"
    
[0m15:05:38.463021 [debug] [ThreadPool]: SQL status: OK in 0.006 seconds
[0m15:05:38.472714 [debug] [ThreadPool]: On create_duckdb_main_silver: COMMIT
[0m15:05:38.472714 [debug] [ThreadPool]: SQL status: OK in 0.008 seconds
[0m15:05:38.475228 [debug] [ThreadPool]: SQL status: OK in 0.008 seconds
[0m15:05:38.477244 [debug] [ThreadPool]: Using duckdb connection "create_duckdb_main_silver"
[0m15:05:38.481904 [debug] [ThreadPool]: On create_duckdb_main_bronze: COMMIT
[0m15:05:38.484819 [debug] [ThreadPool]: On create_duckdb_main_gold: COMMIT
[0m15:05:38.489110 [debug] [ThreadPool]: On create_duckdb_main_silver: COMMIT
[0m15:05:38.490502 [debug] [ThreadPool]: Using duckdb connection "create_duckdb_main_bronze"
[0m15:05:38.492626 [debug] [ThreadPool]: Using duckdb connection "create_duckdb_main_gold"
[0m15:05:38.499258 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m15:05:38.501255 [debug] [ThreadPool]: On create_duckdb_main_bronze: COMMIT
[0m15:05:38.505582 [debug] [ThreadPool]: On create_duckdb_main_gold: COMMIT
[0m15:05:38.509189 [debug] [ThreadPool]: On create_duckdb_main_silver: Close
[0m15:05:38.512787 [debug] [ThreadPool]: SQL status: OK in 0.002 seconds
[0m15:05:38.518817 [debug] [ThreadPool]: SQL status: OK in 0.002 seconds
[0m15:05:38.521571 [debug] [ThreadPool]: On create_duckdb_main_bronze: Close
[0m15:05:38.526511 [debug] [ThreadPool]: On create_duckdb_main_gold: Close
[0m15:05:38.543501 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_duckdb_main_gold'
[0m15:05:38.544112 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_duckdb_main_silver'
[0m15:05:38.551439 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_duckdb_main_bronze'
[0m15:05:38.588551 [debug] [ThreadPool]: Using duckdb connection "list_duckdb_main_gold"
[0m15:05:38.606610 [debug] [ThreadPool]: Using duckdb connection "list_duckdb_main_silver"
[0m15:05:38.619116 [debug] [ThreadPool]: Using duckdb connection "list_duckdb_main_bronze"
[0m15:05:38.619116 [debug] [ThreadPool]: On list_duckdb_main_gold: BEGIN
[0m15:05:38.619116 [debug] [ThreadPool]: On list_duckdb_main_silver: BEGIN
[0m15:05:38.630634 [debug] [ThreadPool]: On list_duckdb_main_bronze: BEGIN
[0m15:05:38.633447 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:05:38.636743 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:05:38.639314 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:05:38.645305 [debug] [ThreadPool]: SQL status: OK in 0.012 seconds
[0m15:05:38.651670 [debug] [ThreadPool]: SQL status: OK in 0.015 seconds
[0m15:05:38.654941 [debug] [ThreadPool]: SQL status: OK in 0.015 seconds
[0m15:05:38.656382 [debug] [ThreadPool]: Using duckdb connection "list_duckdb_main_gold"
[0m15:05:38.659483 [debug] [ThreadPool]: Using duckdb connection "list_duckdb_main_silver"
[0m15:05:38.659483 [debug] [ThreadPool]: Using duckdb connection "list_duckdb_main_bronze"
[0m15:05:38.664913 [debug] [ThreadPool]: On list_duckdb_main_gold: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_duckdb_main_gold"} */
select
      'duckdb' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main_gold'
    and lower(table_catalog) = 'duckdb'
  
[0m15:05:38.664913 [debug] [ThreadPool]: On list_duckdb_main_silver: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_duckdb_main_silver"} */
select
      'duckdb' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main_silver'
    and lower(table_catalog) = 'duckdb'
  
[0m15:05:38.670707 [debug] [ThreadPool]: On list_duckdb_main_bronze: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_duckdb_main_bronze"} */
select
      'duckdb' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main_bronze'
    and lower(table_catalog) = 'duckdb'
  
[0m15:05:38.762612 [debug] [ThreadPool]: SQL status: OK in 0.086 seconds
[0m15:05:38.767155 [debug] [ThreadPool]: SQL status: OK in 0.091 seconds
[0m15:05:38.770484 [debug] [ThreadPool]: SQL status: OK in 0.094 seconds
[0m15:05:38.774587 [debug] [ThreadPool]: On list_duckdb_main_bronze: ROLLBACK
[0m15:05:38.780265 [debug] [ThreadPool]: On list_duckdb_main_gold: ROLLBACK
[0m15:05:38.786677 [debug] [ThreadPool]: On list_duckdb_main_silver: ROLLBACK
[0m15:05:38.791239 [debug] [ThreadPool]: Failed to rollback 'list_duckdb_main_bronze'
[0m15:05:38.795020 [debug] [ThreadPool]: Failed to rollback 'list_duckdb_main_silver'
[0m15:05:38.797042 [debug] [ThreadPool]: On list_duckdb_main_bronze: Close
[0m15:05:38.799094 [debug] [ThreadPool]: Failed to rollback 'list_duckdb_main_gold'
[0m15:05:38.801161 [debug] [ThreadPool]: On list_duckdb_main_silver: Close
[0m15:05:38.805981 [debug] [ThreadPool]: On list_duckdb_main_gold: Close
[0m15:05:38.823061 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'edd5c6bb-f3cd-483a-b6c7-dc20500065ba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016BD770DA00>]}
[0m15:05:38.829328 [debug] [MainThread]: Using duckdb connection "master"
[0m15:05:38.830641 [debug] [MainThread]: On master: BEGIN
[0m15:05:38.835598 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:05:38.839380 [debug] [MainThread]: SQL status: OK in 0.004 seconds
[0m15:05:38.839380 [debug] [MainThread]: On master: COMMIT
[0m15:05:38.844009 [debug] [MainThread]: Using duckdb connection "master"
[0m15:05:38.844009 [debug] [MainThread]: On master: COMMIT
[0m15:05:38.848580 [debug] [MainThread]: SQL status: OK in 0.001 seconds
[0m15:05:38.850765 [debug] [MainThread]: On master: Close
[0m15:05:38.951344 [debug] [Thread-1  ]: Began running node model.job_intelligent.stg_jobs_raw
[0m15:05:38.956876 [info ] [Thread-1  ]: 1 of 13 START sql view model main_bronze.stg_jobs_raw .......................... [RUN]
[0m15:05:38.960827 [debug] [Thread-1  ]: Acquiring new duckdb connection 'model.job_intelligent.stg_jobs_raw'
[0m15:05:38.962888 [debug] [Thread-1  ]: Began compiling node model.job_intelligent.stg_jobs_raw
[0m15:05:39.000872 [debug] [Thread-1  ]: Writing injected SQL for node "model.job_intelligent.stg_jobs_raw"
[0m15:05:39.009104 [debug] [Thread-1  ]: Began executing node model.job_intelligent.stg_jobs_raw
[0m15:05:39.181390 [debug] [Thread-1  ]: Writing runtime sql for node "model.job_intelligent.stg_jobs_raw"
[0m15:05:39.192928 [debug] [Thread-1  ]: Using duckdb connection "model.job_intelligent.stg_jobs_raw"
[0m15:05:39.195991 [debug] [Thread-1  ]: On model.job_intelligent.stg_jobs_raw: BEGIN
[0m15:05:39.199239 [debug] [Thread-1  ]: Opening a new connection, currently in state init
[0m15:05:39.200745 [debug] [Thread-1  ]: SQL status: OK in 0.003 seconds
[0m15:05:39.206178 [debug] [Thread-1  ]: Using duckdb connection "model.job_intelligent.stg_jobs_raw"
[0m15:05:39.209304 [debug] [Thread-1  ]: On model.job_intelligent.stg_jobs_raw: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.stg_jobs_raw"} */

  
  create view "duckdb"."main_bronze"."stg_jobs_raw__dbt_tmp" as (
    -- models/bronze/stg_jobs_raw.sql
-- Bronze layer: Lecture directe des données brutes depuis final_data.csv
-- Renommage et typage minimal



-- Read directly from CSV file
SELECT
    -- Renommer les colonnes pour cohérence
    title as job_title,
    location,
    postedTime as posted_time,
    publishedAt as published_at,
    jobUrl as job_url,
    companyName as company_name,
    companyUrl as company_url,
    description as job_description,
    contractType as contract_type,
    workType as work_type,
    
    -- Métadonnées de traçabilité
    NOW() as ingestion_timestamp,
    '2026-01-08 14:05:21.509578+00:00' as dbt_run_id

FROM read_csv_auto('D:/lab2/final_data.csv')

WHERE 1=1
    -- Filtrer les lignes vides
    AND title IS NOT NULL
    AND description IS NOT NULL
  );

[0m15:05:39.240876 [debug] [Thread-1  ]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.stg_jobs_raw"} */

  
  create view "duckdb"."main_bronze"."stg_jobs_raw__dbt_tmp" as (
    -- models/bronze/stg_jobs_raw.sql
-- Bronze layer: Lecture directe des données brutes depuis final_data.csv
-- Renommage et typage minimal



-- Read directly from CSV file
SELECT
    -- Renommer les colonnes pour cohérence
    title as job_title,
    location,
    postedTime as posted_time,
    publishedAt as published_at,
    jobUrl as job_url,
    companyName as company_name,
    companyUrl as company_url,
    description as job_description,
    contractType as contract_type,
    workType as work_type,
    
    -- Métadonnées de traçabilité
    NOW() as ingestion_timestamp,
    '2026-01-08 14:05:21.509578+00:00' as dbt_run_id

FROM read_csv_auto('D:/lab2/final_data.csv')

WHERE 1=1
    -- Filtrer les lignes vides
    AND title IS NOT NULL
    AND description IS NOT NULL
  );

[0m15:05:39.240876 [debug] [Thread-1  ]: DuckDB adapter: Rolling back transaction.
[0m15:05:39.249299 [debug] [Thread-1  ]: On model.job_intelligent.stg_jobs_raw: ROLLBACK
[0m15:05:39.452230 [debug] [Thread-1  ]: Failed to rollback 'model.job_intelligent.stg_jobs_raw'
[0m15:05:39.452230 [debug] [Thread-1  ]: On model.job_intelligent.stg_jobs_raw: Close
[0m15:05:39.463440 [debug] [Thread-1  ]: Runtime Error in model stg_jobs_raw (models\bronze\stg_jobs_raw.sql)
  IO Error: No files found that match the pattern "D:/lab2/final_data.csv"
  
  LINE 29: FROM read_csv_auto('D:/lab2/final_data.csv')
                ^
[0m15:05:39.474256 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'edd5c6bb-f3cd-483a-b6c7-dc20500065ba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016BD4F91B20>]}
[0m15:05:39.481347 [error] [Thread-1  ]: 1 of 13 ERROR creating sql view model main_bronze.stg_jobs_raw ................. [[31mERROR[0m in 0.50s]
[0m15:05:39.485513 [debug] [Thread-1  ]: Finished running node model.job_intelligent.stg_jobs_raw
[0m15:05:39.491179 [debug] [Thread-7  ]: Marking all children of 'model.job_intelligent.stg_jobs_raw' to be skipped because of status 'error'.  Reason: Runtime Error in model stg_jobs_raw (models\bronze\stg_jobs_raw.sql)
  IO Error: No files found that match the pattern "D:/lab2/final_data.csv"
  
  LINE 29: FROM read_csv_auto('D:/lab2/final_data.csv')
                ^.
[0m15:05:39.495965 [debug] [Thread-3  ]: Began running node model.job_intelligent.int_jobs_cleaned
[0m15:05:39.500851 [info ] [Thread-3  ]: 2 of 13 SKIP relation main_silver.int_jobs_cleaned ............................. [[33mSKIP[0m]
[0m15:05:39.506629 [debug] [Thread-3  ]: Finished running node model.job_intelligent.int_jobs_cleaned
[0m15:05:39.509229 [debug] [Thread-2  ]: Began running node model.job_intelligent.int_job_title_normalization
[0m15:05:39.511620 [info ] [Thread-2  ]: 3 of 13 SKIP relation main_silver.int_job_title_normalization .................. [[33mSKIP[0m]
[0m15:05:39.518814 [debug] [Thread-2  ]: Finished running node model.job_intelligent.int_job_title_normalization
[0m15:05:39.520887 [debug] [Thread-4  ]: Began running node model.job_intelligent.dim_company
[0m15:05:39.525294 [debug] [Thread-3  ]: Began running node model.job_intelligent.dim_location
[0m15:05:39.527307 [debug] [Thread-1  ]: Began running node model.job_intelligent.dim_time
[0m15:05:39.531270 [debug] [Thread-2  ]: Began running node model.job_intelligent.int_skills_extraction
[0m15:05:39.535126 [info ] [Thread-4  ]: 4 of 13 SKIP relation main_gold.dim_company .................................... [[33mSKIP[0m]
[0m15:05:39.539223 [info ] [Thread-3  ]: 5 of 13 SKIP relation main_gold.dim_location ................................... [[33mSKIP[0m]
[0m15:05:39.543118 [info ] [Thread-1  ]: 6 of 13 SKIP relation main_gold.dim_time ....................................... [[33mSKIP[0m]
[0m15:05:39.543118 [info ] [Thread-2  ]: 7 of 13 SKIP relation main_silver.int_skills_extraction ........................ [[33mSKIP[0m]
[0m15:05:39.551230 [debug] [Thread-4  ]: Finished running node model.job_intelligent.dim_company
[0m15:05:39.556334 [debug] [Thread-3  ]: Finished running node model.job_intelligent.dim_location
[0m15:05:39.559569 [debug] [Thread-1  ]: Finished running node model.job_intelligent.dim_time
[0m15:05:39.564305 [debug] [Thread-2  ]: Finished running node model.job_intelligent.int_skills_extraction
[0m15:05:39.570927 [debug] [Thread-4  ]: Began running node model.job_intelligent.fact_job_offers
[0m15:05:39.579472 [debug] [Thread-1  ]: Began running node model.job_intelligent.dim_skills
[0m15:05:39.583308 [info ] [Thread-4  ]: 8 of 13 SKIP relation main_gold.fact_job_offers ................................ [[33mSKIP[0m]
[0m15:05:39.589393 [info ] [Thread-1  ]: 9 of 13 SKIP relation main_gold.dim_skills ..................................... [[33mSKIP[0m]
[0m15:05:39.593665 [debug] [Thread-4  ]: Finished running node model.job_intelligent.fact_job_offers
[0m15:05:39.599212 [debug] [Thread-1  ]: Finished running node model.job_intelligent.dim_skills
[0m15:05:39.603030 [debug] [Thread-2  ]: Began running node model.job_intelligent.agg_job_offers_by_category_time
[0m15:05:39.605049 [debug] [Thread-3  ]: Began running node model.job_intelligent.agg_location_analysis
[0m15:05:39.611262 [debug] [Thread-4  ]: Began running node model.job_intelligent.fact_job_skills
[0m15:05:39.611262 [info ] [Thread-2  ]: 10 of 13 SKIP relation main_gold.agg_job_offers_by_category_time ............... [[33mSKIP[0m]
[0m15:05:39.619540 [info ] [Thread-3  ]: 11 of 13 SKIP relation main_gold.agg_location_analysis ......................... [[33mSKIP[0m]
[0m15:05:39.621609 [info ] [Thread-4  ]: 12 of 13 SKIP relation main_gold.fact_job_skills ............................... [[33mSKIP[0m]
[0m15:05:39.629174 [debug] [Thread-2  ]: Finished running node model.job_intelligent.agg_job_offers_by_category_time
[0m15:05:39.632185 [debug] [Thread-3  ]: Finished running node model.job_intelligent.agg_location_analysis
[0m15:05:39.632185 [debug] [Thread-4  ]: Finished running node model.job_intelligent.fact_job_skills
[0m15:05:39.642817 [debug] [Thread-1  ]: Began running node model.job_intelligent.agg_skills_demand
[0m15:05:39.649162 [info ] [Thread-1  ]: 13 of 13 SKIP relation main_gold.agg_skills_demand ............................. [[33mSKIP[0m]
[0m15:05:39.651260 [debug] [Thread-1  ]: Finished running node model.job_intelligent.agg_skills_demand
[0m15:05:39.660011 [debug] [MainThread]: Using duckdb connection "master"
[0m15:05:39.663901 [debug] [MainThread]: On master: BEGIN
[0m15:05:39.667138 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m15:05:39.670723 [debug] [MainThread]: SQL status: OK in 0.005 seconds
[0m15:05:39.670723 [debug] [MainThread]: On master: COMMIT
[0m15:05:39.679349 [debug] [MainThread]: Using duckdb connection "master"
[0m15:05:39.682092 [debug] [MainThread]: On master: COMMIT
[0m15:05:39.682092 [debug] [MainThread]: SQL status: OK in 0.001 seconds
[0m15:05:39.689102 [debug] [MainThread]: On master: Close
[0m15:05:39.690648 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:05:39.694518 [debug] [MainThread]: Connection 'create_duckdb_main_silver' was properly closed.
[0m15:05:39.699293 [debug] [MainThread]: Connection 'create_duckdb_main_gold' was properly closed.
[0m15:05:39.699293 [debug] [MainThread]: Connection 'create_duckdb_main_bronze' was properly closed.
[0m15:05:39.699293 [debug] [MainThread]: Connection 'list_duckdb_main_gold' was properly closed.
[0m15:05:39.699293 [debug] [MainThread]: Connection 'list_duckdb_main_silver' was properly closed.
[0m15:05:39.710343 [debug] [MainThread]: Connection 'list_duckdb_main_bronze' was properly closed.
[0m15:05:39.711119 [debug] [MainThread]: Connection 'model.job_intelligent.stg_jobs_raw' was properly closed.
[0m15:05:39.711119 [info ] [MainThread]: 
[0m15:05:39.719313 [info ] [MainThread]: Finished running 12 table models, 1 view model in 0 hours 0 minutes and 2.27 seconds (2.27s).
[0m15:05:39.726150 [debug] [MainThread]: Command end result
[0m15:05:39.849817 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Ayoub Gorry\Desktop\powerbi\jobs-power-bi\dbt_project\target\manifest.json
[0m15:05:39.860098 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Ayoub Gorry\Desktop\powerbi\jobs-power-bi\dbt_project\target\semantic_manifest.json
[0m15:05:39.910836 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\Ayoub Gorry\Desktop\powerbi\jobs-power-bi\dbt_project\target\run_results.json
[0m15:05:39.915490 [info ] [MainThread]: 
[0m15:05:39.919146 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m15:05:39.919146 [info ] [MainThread]: 
[0m15:05:39.919146 [error] [MainThread]: [31mFailure in model stg_jobs_raw (models\bronze\stg_jobs_raw.sql)[0m
[0m15:05:39.927314 [error] [MainThread]:   Runtime Error in model stg_jobs_raw (models\bronze\stg_jobs_raw.sql)
  IO Error: No files found that match the pattern "D:/lab2/final_data.csv"
  
  LINE 29: FROM read_csv_auto('D:/lab2/final_data.csv')
                ^
[0m15:05:39.931350 [info ] [MainThread]: 
[0m15:05:39.934349 [info ] [MainThread]:   compiled code at target\compiled\job_intelligent\models\bronze\stg_jobs_raw.sql
[0m15:05:39.936889 [info ] [MainThread]: 
[0m15:05:39.939279 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=12 NO-OP=0 TOTAL=13
[0m15:05:39.943137 [debug] [MainThread]: Command `dbt run` failed at 15:05:39.943137 after 18.79 seconds
[0m15:05:39.950642 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016BD4BA2160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016BD770F0A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016BD4F70FD0>]}
[0m15:05:39.952858 [debug] [MainThread]: Flushing usage events
[0m15:05:40.584003 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:07:28.204779 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DB3F9E1190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DB3FE33070>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DB40CD72E0>]}


============================== 15:07:28.224676 | 4f7f1c6e-3048-4d4a-a974-337f2b05aafb ==============================
[0m15:07:28.224676 [info ] [MainThread]: Running with dbt=1.10.18
[0m15:07:28.230350 [debug] [MainThread]: running dbt with arguments {'log_path': 'C:\\Users\\Ayoub Gorry\\Desktop\\powerbi\\jobs-power-bi\\dbt_project\\logs', 'log_format': 'default', 'partial_parse': 'True', 'log_cache_events': 'False', 'empty': 'None', 'version_check': 'True', 'cache_selected_only': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'no_print': 'None', 'send_anonymous_usage_stats': 'True', 'use_experimental_parser': 'False', 'target_path': 'None', 'introspect': 'True', 'invocation_command': 'dbt debug', 'profiles_dir': 'C:\\Users\\Ayoub Gorry\\Desktop\\powerbi\\jobs-power-bi\\dbt_project', 'debug': 'False', 'printer_width': '80', 'quiet': 'False', 'static_parser': 'True', 'use_colors': 'True', 'fail_fast': 'False', 'warn_error': 'None', 'write_json': 'True', 'indirect_selection': 'eager'}
[0m15:07:28.336252 [info ] [MainThread]: dbt version: 1.10.18
[0m15:07:28.340447 [info ] [MainThread]: python version: 3.9.13
[0m15:07:28.343545 [info ] [MainThread]: python path: C:\Users\Ayoub Gorry\AppData\Local\Programs\Python\Python39\python.exe
[0m15:07:28.345643 [info ] [MainThread]: os info: Windows-10-10.0.26100-SP0
[0m15:07:28.994293 [info ] [MainThread]: Using profiles dir at C:\Users\Ayoub Gorry\Desktop\powerbi\jobs-power-bi\dbt_project
[0m15:07:28.996317 [info ] [MainThread]: Using profiles.yml file at C:\Users\Ayoub Gorry\Desktop\powerbi\jobs-power-bi\dbt_project\profiles.yml
[0m15:07:28.998574 [info ] [MainThread]: Using dbt_project.yml file at C:\Users\Ayoub Gorry\Desktop\powerbi\jobs-power-bi\dbt_project\dbt_project.yml
[0m15:07:29.060486 [info ] [MainThread]: adapter type: duckdb
[0m15:07:29.064537 [info ] [MainThread]: adapter version: 1.10.0
[0m15:07:29.669568 [info ] [MainThread]: Configuration:
[0m15:07:29.672772 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m15:07:29.674791 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m15:07:29.676814 [info ] [MainThread]: Required dependencies:
[0m15:07:29.679569 [debug] [MainThread]: Executing "git --help"
[0m15:07:29.810810 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--no-lazy-fetch]\n           [--no-optional-locks] [--no-advice] [--bare] [--git-dir=<path>]\n           [--work-tree=<path>] [--namespace=<name>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone      Clone a repository into a new directory\n   init       Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add        Add file contents to the index\n   mv         Move or rename a file, a directory, or a symlink\n   restore    Restore working tree files\n   rm         Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect     Use binary search to find the commit that introduced a bug\n   diff       Show changes between commits, commit and working tree, etc\n   grep       Print lines matching a pattern\n   log        Show commit logs\n   show       Show various types of objects\n   status     Show the working tree status\n\ngrow, mark and tweak your common history\n   backfill   Download missing objects in a partial clone\n   branch     List, create, or delete branches\n   commit     Record changes to the repository\n   merge      Join two or more development histories together\n   rebase     Reapply commits on top of another base tip\n   reset      Reset current HEAD to the specified state\n   switch     Switch branches\n   tag        Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch      Download objects and refs from another repository\n   pull       Fetch from and integrate with another repository or a local branch\n   push       Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m15:07:29.813254 [debug] [MainThread]: STDERR: "b''"
[0m15:07:29.818660 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m15:07:29.819214 [info ] [MainThread]: Connection:
[0m15:07:29.819214 [info ] [MainThread]:   database: duckdb
[0m15:07:29.819214 [info ] [MainThread]:   schema: main
[0m15:07:29.826281 [info ] [MainThread]:   path: duckdb.db
[0m15:07:29.830436 [info ] [MainThread]:   config_options: None
[0m15:07:29.832548 [info ] [MainThread]:   extensions: None
[0m15:07:29.834580 [info ] [MainThread]:   settings: {}
[0m15:07:29.834580 [info ] [MainThread]:   external_root: .
[0m15:07:29.839355 [info ] [MainThread]:   use_credential_provider: None
[0m15:07:29.839355 [info ] [MainThread]:   attach: None
[0m15:07:29.844229 [info ] [MainThread]:   filesystems: None
[0m15:07:29.846346 [info ] [MainThread]:   remote: None
[0m15:07:29.850792 [info ] [MainThread]:   plugins: None
[0m15:07:29.850792 [info ] [MainThread]:   disable_transactions: False
[0m15:07:29.850792 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m15:07:31.733475 [debug] [MainThread]: Acquiring new duckdb connection 'debug'
[0m15:07:41.671312 [debug] [MainThread]: Using duckdb connection "debug"
[0m15:07:41.671312 [debug] [MainThread]: On debug: select 1 as id
[0m15:07:41.671312 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:07:41.840598 [debug] [MainThread]: SQL status: OK in 0.167 seconds
[0m15:07:41.840598 [debug] [MainThread]: On debug: Close
[0m15:07:41.850999 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m15:07:41.850999 [info ] [MainThread]: [32mAll checks passed![0m
[0m15:07:41.855132 [debug] [MainThread]: Command `dbt debug` succeeded at 15:07:41.855132 after 14.25 seconds
[0m15:07:41.858818 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m15:07:41.860854 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DB3F9E1190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DB41F4B190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DB3FD8F2E0>]}
[0m15:07:41.860854 [debug] [MainThread]: Flushing usage events
[0m15:07:42.679428 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:07:55.459943 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002BEB1E821C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002BEB31773D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002BEB3177220>]}


============================== 15:07:55.485539 | 02e4a55a-7bcd-4fa7-a733-94e629ec3331 ==============================
[0m15:07:55.485539 [info ] [MainThread]: Running with dbt=1.10.18
[0m15:07:55.489175 [debug] [MainThread]: running dbt with arguments {'log_path': 'C:\\Users\\Ayoub Gorry\\Desktop\\powerbi\\jobs-power-bi\\dbt_project\\logs', 'log_format': 'default', 'partial_parse': 'True', 'version_check': 'True', 'empty': 'False', 'log_cache_events': 'False', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Ayoub Gorry\\Desktop\\powerbi\\jobs-power-bi\\dbt_project', 'debug': 'False', 'printer_width': '80', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'no_print': 'None', 'introspect': 'True', 'invocation_command': 'dbt run', 'target_path': 'None', 'use_experimental_parser': 'False', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'static_parser': 'True', 'use_colors': 'True', 'warn_error': 'None', 'fail_fast': 'False', 'write_json': 'True', 'indirect_selection': 'eager'}
[0m15:07:57.412185 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '02e4a55a-7bcd-4fa7-a733-94e629ec3331', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002BEB3177490>]}
[0m15:07:57.892842 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '02e4a55a-7bcd-4fa7-a733-94e629ec3331', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002BEB325DCD0>]}
[0m15:07:57.907062 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m15:07:59.383602 [debug] [MainThread]: checksum: 9646db7d1a5f5b9389d9c545d3ebf898b26bc1f63f6b5366f34a6af01d52cb91, vars: {}, profile: , target: , version: 1.10.18
[0m15:08:04.415205 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:08:04.418384 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:08:04.450699 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- seeds
[0m15:08:04.567996 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '02e4a55a-7bcd-4fa7-a733-94e629ec3331', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002BEB47AC130>]}
[0m15:08:04.935547 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Ayoub Gorry\Desktop\powerbi\jobs-power-bi\dbt_project\target\manifest.json
[0m15:08:04.991837 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Ayoub Gorry\Desktop\powerbi\jobs-power-bi\dbt_project\target\semantic_manifest.json
[0m15:08:05.113194 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '02e4a55a-7bcd-4fa7-a733-94e629ec3331', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002BEB477F250>]}
[0m15:08:05.119650 [info ] [MainThread]: Found 13 models, 456 macros
[0m15:08:05.121088 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '02e4a55a-7bcd-4fa7-a733-94e629ec3331', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002BEB478AB80>]}
[0m15:08:05.129484 [info ] [MainThread]: 
[0m15:08:05.129484 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:08:05.129484 [info ] [MainThread]: 
[0m15:08:05.129484 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m15:08:05.160978 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_duckdb'
[0m15:08:05.160978 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_duckdb'
[0m15:08:05.196056 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_duckdb'
[0m15:08:05.539187 [debug] [ThreadPool]: Using duckdb connection "list_duckdb"
[0m15:08:05.542323 [debug] [ThreadPool]: Using duckdb connection "list_duckdb"
[0m15:08:05.544805 [debug] [ThreadPool]: Using duckdb connection "list_duckdb"
[0m15:08:05.546873 [debug] [ThreadPool]: On list_duckdb: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_duckdb"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"duckdb"'
    
  
  
[0m15:08:05.550502 [debug] [ThreadPool]: On list_duckdb: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_duckdb"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"duckdb"'
    
  
  
[0m15:08:05.553082 [debug] [ThreadPool]: On list_duckdb: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_duckdb"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"duckdb"'
    
  
  
[0m15:08:05.553082 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:08:05.558555 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:08:05.559291 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:08:05.650649 [debug] [ThreadPool]: SQL status: OK in 0.094 seconds
[0m15:08:05.650649 [debug] [ThreadPool]: SQL status: OK in 0.091 seconds
[0m15:08:05.654692 [debug] [ThreadPool]: SQL status: OK in 0.097 seconds
[0m15:08:05.659333 [debug] [ThreadPool]: On list_duckdb: Close
[0m15:08:05.674181 [debug] [ThreadPool]: On list_duckdb: Close
[0m15:08:05.678907 [debug] [ThreadPool]: On list_duckdb: Close
[0m15:08:05.684076 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_duckdb, now create_duckdb_main_gold)
[0m15:08:05.684938 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_duckdb, now create_duckdb_main_silver)
[0m15:08:05.687100 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_duckdb, now create_duckdb_main_bronze)
[0m15:08:05.690411 [debug] [ThreadPool]: Creating schema "database: "duckdb"
schema: "main_gold"
"
[0m15:08:05.693335 [debug] [ThreadPool]: Creating schema "database: "duckdb"
schema: "main_silver"
"
[0m15:08:05.693763 [debug] [ThreadPool]: Creating schema "database: "duckdb"
schema: "main_bronze"
"
[0m15:08:05.721537 [debug] [ThreadPool]: Using duckdb connection "create_duckdb_main_gold"
[0m15:08:05.730333 [debug] [ThreadPool]: Using duckdb connection "create_duckdb_main_silver"
[0m15:08:05.741070 [debug] [ThreadPool]: Using duckdb connection "create_duckdb_main_bronze"
[0m15:08:05.741070 [debug] [ThreadPool]: On create_duckdb_main_gold: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_duckdb_main_gold"} */

    
        select type from duckdb_databases()
        where lower(database_name)='duckdb'
        and type='sqlite'
    
  
[0m15:08:05.745383 [debug] [ThreadPool]: On create_duckdb_main_silver: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_duckdb_main_silver"} */

    
        select type from duckdb_databases()
        where lower(database_name)='duckdb'
        and type='sqlite'
    
  
[0m15:08:05.747286 [debug] [ThreadPool]: On create_duckdb_main_bronze: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_duckdb_main_bronze"} */

    
        select type from duckdb_databases()
        where lower(database_name)='duckdb'
        and type='sqlite'
    
  
[0m15:08:05.750630 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:08:05.751290 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:08:05.751290 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:08:05.758677 [debug] [ThreadPool]: SQL status: OK in 0.008 seconds
[0m15:08:05.761347 [debug] [ThreadPool]: SQL status: OK in 0.009 seconds
[0m15:08:05.762856 [debug] [ThreadPool]: SQL status: OK in 0.009 seconds
[0m15:08:05.768638 [debug] [ThreadPool]: Using duckdb connection "create_duckdb_main_gold"
[0m15:08:05.781209 [debug] [ThreadPool]: On create_duckdb_main_gold: BEGIN
[0m15:08:05.779455 [debug] [ThreadPool]: Using duckdb connection "create_duckdb_main_bronze"
[0m15:08:05.775907 [debug] [ThreadPool]: Using duckdb connection "create_duckdb_main_silver"
[0m15:08:05.787251 [debug] [ThreadPool]: On create_duckdb_main_bronze: BEGIN
[0m15:08:05.789296 [debug] [ThreadPool]: On create_duckdb_main_silver: BEGIN
[0m15:08:05.790330 [debug] [ThreadPool]: SQL status: OK in 0.009 seconds
[0m15:08:05.799455 [debug] [ThreadPool]: Using duckdb connection "create_duckdb_main_gold"
[0m15:08:05.797302 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m15:08:05.793290 [debug] [ThreadPool]: SQL status: OK in 0.003 seconds
[0m15:08:05.803528 [debug] [ThreadPool]: On create_duckdb_main_gold: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_duckdb_main_gold"} */

    
    
        create schema if not exists "duckdb"."main_gold"
    
[0m15:08:05.805882 [debug] [ThreadPool]: Using duckdb connection "create_duckdb_main_silver"
[0m15:08:05.808377 [debug] [ThreadPool]: Using duckdb connection "create_duckdb_main_bronze"
[0m15:08:05.813580 [debug] [ThreadPool]: On create_duckdb_main_silver: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_duckdb_main_silver"} */

    
    
        create schema if not exists "duckdb"."main_silver"
    
[0m15:08:05.813580 [debug] [ThreadPool]: SQL status: OK in 0.005 seconds
[0m15:08:05.819572 [debug] [ThreadPool]: On create_duckdb_main_bronze: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_duckdb_main_bronze"} */

    
    
        create schema if not exists "duckdb"."main_bronze"
    
[0m15:08:05.824973 [debug] [ThreadPool]: On create_duckdb_main_gold: COMMIT
[0m15:08:05.828497 [debug] [ThreadPool]: SQL status: OK in 0.007 seconds
[0m15:08:05.831102 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m15:08:05.831818 [debug] [ThreadPool]: Using duckdb connection "create_duckdb_main_gold"
[0m15:08:05.834886 [debug] [ThreadPool]: On create_duckdb_main_silver: COMMIT
[0m15:08:05.839599 [debug] [ThreadPool]: On create_duckdb_main_bronze: COMMIT
[0m15:08:05.841133 [debug] [ThreadPool]: On create_duckdb_main_gold: COMMIT
[0m15:08:05.841133 [debug] [ThreadPool]: Using duckdb connection "create_duckdb_main_silver"
[0m15:08:05.845468 [debug] [ThreadPool]: Using duckdb connection "create_duckdb_main_bronze"
[0m15:08:05.849100 [debug] [ThreadPool]: On create_duckdb_main_silver: COMMIT
[0m15:08:05.850835 [debug] [ThreadPool]: SQL status: OK in 0.002 seconds
[0m15:08:05.851185 [debug] [ThreadPool]: On create_duckdb_main_bronze: COMMIT
[0m15:08:05.855351 [debug] [ThreadPool]: On create_duckdb_main_gold: Close
[0m15:08:05.857226 [debug] [ThreadPool]: SQL status: OK in 0.002 seconds
[0m15:08:05.859208 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m15:08:05.860872 [debug] [ThreadPool]: On create_duckdb_main_silver: Close
[0m15:08:05.863543 [debug] [ThreadPool]: On create_duckdb_main_bronze: Close
[0m15:08:05.872914 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_duckdb_main_gold'
[0m15:08:05.888617 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_duckdb_main_silver'
[0m15:08:05.899304 [debug] [ThreadPool]: Using duckdb connection "list_duckdb_main_gold"
[0m15:08:05.901305 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_duckdb_main_bronze'
[0m15:08:05.914215 [debug] [ThreadPool]: Using duckdb connection "list_duckdb_main_silver"
[0m15:08:05.914215 [debug] [ThreadPool]: On list_duckdb_main_gold: BEGIN
[0m15:08:05.923021 [debug] [ThreadPool]: Using duckdb connection "list_duckdb_main_bronze"
[0m15:08:05.923021 [debug] [ThreadPool]: On list_duckdb_main_silver: BEGIN
[0m15:08:05.930739 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:08:05.937044 [debug] [ThreadPool]: On list_duckdb_main_bronze: BEGIN
[0m15:08:05.939198 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:08:05.939198 [debug] [ThreadPool]: SQL status: OK in 0.013 seconds
[0m15:08:05.945237 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:08:05.949668 [debug] [ThreadPool]: Using duckdb connection "list_duckdb_main_gold"
[0m15:08:05.951080 [debug] [ThreadPool]: SQL status: OK in 0.012 seconds
[0m15:08:05.955859 [debug] [ThreadPool]: On list_duckdb_main_gold: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_duckdb_main_gold"} */
select
      'duckdb' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main_gold'
    and lower(table_catalog) = 'duckdb'
  
[0m15:08:05.955859 [debug] [ThreadPool]: SQL status: OK in 0.012 seconds
[0m15:08:05.960154 [debug] [ThreadPool]: Using duckdb connection "list_duckdb_main_silver"
[0m15:08:05.961915 [debug] [ThreadPool]: Using duckdb connection "list_duckdb_main_bronze"
[0m15:08:05.965435 [debug] [ThreadPool]: On list_duckdb_main_silver: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_duckdb_main_silver"} */
select
      'duckdb' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main_silver'
    and lower(table_catalog) = 'duckdb'
  
[0m15:08:05.969601 [debug] [ThreadPool]: On list_duckdb_main_bronze: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_duckdb_main_bronze"} */
select
      'duckdb' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main_bronze'
    and lower(table_catalog) = 'duckdb'
  
[0m15:08:06.056718 [debug] [ThreadPool]: SQL status: OK in 0.082 seconds
[0m15:08:06.059547 [debug] [ThreadPool]: SQL status: OK in 0.098 seconds
[0m15:08:06.059547 [debug] [ThreadPool]: SQL status: OK in 0.087 seconds
[0m15:08:06.063507 [debug] [ThreadPool]: On list_duckdb_main_bronze: ROLLBACK
[0m15:08:06.072794 [debug] [ThreadPool]: On list_duckdb_main_gold: ROLLBACK
[0m15:08:06.072794 [debug] [ThreadPool]: On list_duckdb_main_silver: ROLLBACK
[0m15:08:06.099277 [debug] [ThreadPool]: Failed to rollback 'list_duckdb_main_bronze'
[0m15:08:06.104293 [debug] [ThreadPool]: Failed to rollback 'list_duckdb_main_silver'
[0m15:08:06.099277 [debug] [ThreadPool]: Failed to rollback 'list_duckdb_main_gold'
[0m15:08:06.106670 [debug] [ThreadPool]: On list_duckdb_main_bronze: Close
[0m15:08:06.106670 [debug] [ThreadPool]: On list_duckdb_main_silver: Close
[0m15:08:06.109410 [debug] [ThreadPool]: On list_duckdb_main_gold: Close
[0m15:08:06.125887 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '02e4a55a-7bcd-4fa7-a733-94e629ec3331', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002BEB4853AF0>]}
[0m15:08:06.129510 [debug] [MainThread]: Using duckdb connection "master"
[0m15:08:06.134032 [debug] [MainThread]: On master: BEGIN
[0m15:08:06.134032 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:08:06.138537 [debug] [MainThread]: SQL status: OK in 0.003 seconds
[0m15:08:06.139589 [debug] [MainThread]: On master: COMMIT
[0m15:08:06.140907 [debug] [MainThread]: Using duckdb connection "master"
[0m15:08:06.140907 [debug] [MainThread]: On master: COMMIT
[0m15:08:06.145048 [debug] [MainThread]: SQL status: OK in 0.001 seconds
[0m15:08:06.149254 [debug] [MainThread]: On master: Close
[0m15:08:06.160721 [debug] [Thread-1  ]: Began running node model.job_intelligent.stg_jobs_raw
[0m15:08:06.169444 [info ] [Thread-1  ]: 1 of 13 START sql view model main_bronze.stg_jobs_raw .......................... [RUN]
[0m15:08:06.169444 [debug] [Thread-1  ]: Acquiring new duckdb connection 'model.job_intelligent.stg_jobs_raw'
[0m15:08:06.173884 [debug] [Thread-1  ]: Began compiling node model.job_intelligent.stg_jobs_raw
[0m15:08:06.208453 [debug] [Thread-1  ]: Writing injected SQL for node "model.job_intelligent.stg_jobs_raw"
[0m15:08:06.212520 [debug] [Thread-1  ]: Began executing node model.job_intelligent.stg_jobs_raw
[0m15:08:06.335266 [debug] [Thread-1  ]: Writing runtime sql for node "model.job_intelligent.stg_jobs_raw"
[0m15:08:06.341098 [debug] [Thread-1  ]: Using duckdb connection "model.job_intelligent.stg_jobs_raw"
[0m15:08:06.341098 [debug] [Thread-1  ]: On model.job_intelligent.stg_jobs_raw: BEGIN
[0m15:08:06.348697 [debug] [Thread-1  ]: Opening a new connection, currently in state init
[0m15:08:06.352015 [debug] [Thread-1  ]: SQL status: OK in 0.005 seconds
[0m15:08:06.352015 [debug] [Thread-1  ]: Using duckdb connection "model.job_intelligent.stg_jobs_raw"
[0m15:08:06.359562 [debug] [Thread-1  ]: On model.job_intelligent.stg_jobs_raw: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.stg_jobs_raw"} */

  
  create view "duckdb"."main_bronze"."stg_jobs_raw__dbt_tmp" as (
    -- models/bronze/stg_jobs_raw.sql
-- Bronze layer: Lecture directe des données brutes depuis final_data.csv
-- Renommage et typage minimal



-- Read directly from CSV file
SELECT
    -- Renommer les colonnes pour cohérence
    title as job_title,
    location,
    postedTime as posted_time,
    publishedAt as published_at,
    jobUrl as job_url,
    companyName as company_name,
    companyUrl as company_url,
    description as job_description,
    contractType as contract_type,
    workType as work_type,
    
    -- Métadonnées de traçabilité
    NOW() as ingestion_timestamp,
    '2026-01-08 14:07:55.370386+00:00' as dbt_run_id

FROM read_csv_auto('D:/lab2/final_data.csv')

WHERE 1=1
    -- Filtrer les lignes vides
    AND title IS NOT NULL
    AND description IS NOT NULL
  );

[0m15:08:06.385266 [debug] [Thread-1  ]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.stg_jobs_raw"} */

  
  create view "duckdb"."main_bronze"."stg_jobs_raw__dbt_tmp" as (
    -- models/bronze/stg_jobs_raw.sql
-- Bronze layer: Lecture directe des données brutes depuis final_data.csv
-- Renommage et typage minimal



-- Read directly from CSV file
SELECT
    -- Renommer les colonnes pour cohérence
    title as job_title,
    location,
    postedTime as posted_time,
    publishedAt as published_at,
    jobUrl as job_url,
    companyName as company_name,
    companyUrl as company_url,
    description as job_description,
    contractType as contract_type,
    workType as work_type,
    
    -- Métadonnées de traçabilité
    NOW() as ingestion_timestamp,
    '2026-01-08 14:07:55.370386+00:00' as dbt_run_id

FROM read_csv_auto('D:/lab2/final_data.csv')

WHERE 1=1
    -- Filtrer les lignes vides
    AND title IS NOT NULL
    AND description IS NOT NULL
  );

[0m15:08:06.389296 [debug] [Thread-1  ]: DuckDB adapter: Rolling back transaction.
[0m15:08:06.389296 [debug] [Thread-1  ]: On model.job_intelligent.stg_jobs_raw: ROLLBACK
[0m15:08:06.821716 [debug] [Thread-1  ]: Failed to rollback 'model.job_intelligent.stg_jobs_raw'
[0m15:08:06.828551 [debug] [Thread-1  ]: On model.job_intelligent.stg_jobs_raw: Close
[0m15:08:06.834077 [debug] [Thread-1  ]: Runtime Error in model stg_jobs_raw (models\bronze\stg_jobs_raw.sql)
  IO Error: No files found that match the pattern "D:/lab2/final_data.csv"
  
  LINE 29: FROM read_csv_auto('D:/lab2/final_data.csv')
                ^
[0m15:08:06.843765 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '02e4a55a-7bcd-4fa7-a733-94e629ec3331', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002BEB221B580>]}
[0m15:08:06.849228 [error] [Thread-1  ]: 1 of 13 ERROR creating sql view model main_bronze.stg_jobs_raw ................. [[31mERROR[0m in 0.66s]
[0m15:08:06.850687 [debug] [Thread-1  ]: Finished running node model.job_intelligent.stg_jobs_raw
[0m15:08:06.850687 [debug] [Thread-7  ]: Marking all children of 'model.job_intelligent.stg_jobs_raw' to be skipped because of status 'error'.  Reason: Runtime Error in model stg_jobs_raw (models\bronze\stg_jobs_raw.sql)
  IO Error: No files found that match the pattern "D:/lab2/final_data.csv"
  
  LINE 29: FROM read_csv_auto('D:/lab2/final_data.csv')
                ^.
[0m15:08:06.859528 [debug] [Thread-3  ]: Began running node model.job_intelligent.int_jobs_cleaned
[0m15:08:06.859528 [info ] [Thread-3  ]: 2 of 13 SKIP relation main_silver.int_jobs_cleaned ............................. [[33mSKIP[0m]
[0m15:08:06.865026 [debug] [Thread-3  ]: Finished running node model.job_intelligent.int_jobs_cleaned
[0m15:08:06.870530 [debug] [Thread-2  ]: Began running node model.job_intelligent.int_job_title_normalization
[0m15:08:06.872553 [info ] [Thread-2  ]: 3 of 13 SKIP relation main_silver.int_job_title_normalization .................. [[33mSKIP[0m]
[0m15:08:06.877222 [debug] [Thread-2  ]: Finished running node model.job_intelligent.int_job_title_normalization
[0m15:08:06.882161 [debug] [Thread-4  ]: Began running node model.job_intelligent.dim_company
[0m15:08:06.884197 [debug] [Thread-3  ]: Began running node model.job_intelligent.dim_location
[0m15:08:06.886219 [debug] [Thread-1  ]: Began running node model.job_intelligent.dim_time
[0m15:08:06.889321 [debug] [Thread-2  ]: Began running node model.job_intelligent.int_skills_extraction
[0m15:08:06.891002 [info ] [Thread-4  ]: 4 of 13 SKIP relation main_gold.dim_company .................................... [[33mSKIP[0m]
[0m15:08:06.891002 [info ] [Thread-3  ]: 5 of 13 SKIP relation main_gold.dim_location ................................... [[33mSKIP[0m]
[0m15:08:06.899353 [info ] [Thread-1  ]: 6 of 13 SKIP relation main_gold.dim_time ....................................... [[33mSKIP[0m]
[0m15:08:06.899353 [info ] [Thread-2  ]: 7 of 13 SKIP relation main_silver.int_skills_extraction ........................ [[33mSKIP[0m]
[0m15:08:06.899353 [debug] [Thread-4  ]: Finished running node model.job_intelligent.dim_company
[0m15:08:06.899353 [debug] [Thread-3  ]: Finished running node model.job_intelligent.dim_location
[0m15:08:06.909331 [debug] [Thread-1  ]: Finished running node model.job_intelligent.dim_time
[0m15:08:06.912968 [debug] [Thread-2  ]: Finished running node model.job_intelligent.int_skills_extraction
[0m15:08:06.912968 [debug] [Thread-4  ]: Began running node model.job_intelligent.fact_job_offers
[0m15:08:06.919376 [debug] [Thread-1  ]: Began running node model.job_intelligent.dim_skills
[0m15:08:06.922751 [info ] [Thread-4  ]: 8 of 13 SKIP relation main_gold.fact_job_offers ................................ [[33mSKIP[0m]
[0m15:08:06.924686 [info ] [Thread-1  ]: 9 of 13 SKIP relation main_gold.dim_skills ..................................... [[33mSKIP[0m]
[0m15:08:06.930734 [debug] [Thread-4  ]: Finished running node model.job_intelligent.fact_job_offers
[0m15:08:06.931520 [debug] [Thread-1  ]: Finished running node model.job_intelligent.dim_skills
[0m15:08:06.937348 [debug] [Thread-2  ]: Began running node model.job_intelligent.agg_job_offers_by_category_time
[0m15:08:06.939403 [debug] [Thread-3  ]: Began running node model.job_intelligent.agg_location_analysis
[0m15:08:06.943134 [debug] [Thread-1  ]: Began running node model.job_intelligent.fact_job_skills
[0m15:08:06.945450 [info ] [Thread-2  ]: 10 of 13 SKIP relation main_gold.agg_job_offers_by_category_time ............... [[33mSKIP[0m]
[0m15:08:06.950494 [info ] [Thread-3  ]: 11 of 13 SKIP relation main_gold.agg_location_analysis ......................... [[33mSKIP[0m]
[0m15:08:06.953982 [info ] [Thread-1  ]: 12 of 13 SKIP relation main_gold.fact_job_skills ............................... [[33mSKIP[0m]
[0m15:08:06.953982 [debug] [Thread-2  ]: Finished running node model.job_intelligent.agg_job_offers_by_category_time
[0m15:08:06.959456 [debug] [Thread-3  ]: Finished running node model.job_intelligent.agg_location_analysis
[0m15:08:06.962321 [debug] [Thread-1  ]: Finished running node model.job_intelligent.fact_job_skills
[0m15:08:06.968905 [debug] [Thread-4  ]: Began running node model.job_intelligent.agg_skills_demand
[0m15:08:06.970773 [info ] [Thread-4  ]: 13 of 13 SKIP relation main_gold.agg_skills_demand ............................. [[33mSKIP[0m]
[0m15:08:06.970773 [debug] [Thread-4  ]: Finished running node model.job_intelligent.agg_skills_demand
[0m15:08:06.979452 [debug] [MainThread]: Using duckdb connection "master"
[0m15:08:06.979452 [debug] [MainThread]: On master: BEGIN
[0m15:08:06.984154 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m15:08:06.988835 [debug] [MainThread]: SQL status: OK in 0.004 seconds
[0m15:08:06.990854 [debug] [MainThread]: On master: COMMIT
[0m15:08:06.993452 [debug] [MainThread]: Using duckdb connection "master"
[0m15:08:06.993452 [debug] [MainThread]: On master: COMMIT
[0m15:08:06.998639 [debug] [MainThread]: SQL status: OK in 0.001 seconds
[0m15:08:06.999248 [debug] [MainThread]: On master: Close
[0m15:08:07.004297 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:08:07.006325 [debug] [MainThread]: Connection 'create_duckdb_main_gold' was properly closed.
[0m15:08:07.006325 [debug] [MainThread]: Connection 'create_duckdb_main_bronze' was properly closed.
[0m15:08:07.009413 [debug] [MainThread]: Connection 'create_duckdb_main_silver' was properly closed.
[0m15:08:07.010732 [debug] [MainThread]: Connection 'list_duckdb_main_gold' was properly closed.
[0m15:08:07.012799 [debug] [MainThread]: Connection 'list_duckdb_main_silver' was properly closed.
[0m15:08:07.014820 [debug] [MainThread]: Connection 'list_duckdb_main_bronze' was properly closed.
[0m15:08:07.019404 [debug] [MainThread]: Connection 'model.job_intelligent.stg_jobs_raw' was properly closed.
[0m15:08:07.021458 [info ] [MainThread]: 
[0m15:08:07.023477 [info ] [MainThread]: Finished running 12 table models, 1 view model in 0 hours 0 minutes and 1.89 seconds (1.89s).
[0m15:08:07.028533 [debug] [MainThread]: Command end result
[0m15:08:07.139135 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Ayoub Gorry\Desktop\powerbi\jobs-power-bi\dbt_project\target\manifest.json
[0m15:08:07.150621 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Ayoub Gorry\Desktop\powerbi\jobs-power-bi\dbt_project\target\semantic_manifest.json
[0m15:08:07.186221 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\Ayoub Gorry\Desktop\powerbi\jobs-power-bi\dbt_project\target\run_results.json
[0m15:08:07.189574 [info ] [MainThread]: 
[0m15:08:07.192635 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m15:08:07.192635 [info ] [MainThread]: 
[0m15:08:07.192635 [error] [MainThread]: [31mFailure in model stg_jobs_raw (models\bronze\stg_jobs_raw.sql)[0m
[0m15:08:07.199379 [error] [MainThread]:   Runtime Error in model stg_jobs_raw (models\bronze\stg_jobs_raw.sql)
  IO Error: No files found that match the pattern "D:/lab2/final_data.csv"
  
  LINE 29: FROM read_csv_auto('D:/lab2/final_data.csv')
                ^
[0m15:08:07.203467 [info ] [MainThread]: 
[0m15:08:07.205510 [info ] [MainThread]:   compiled code at target\compiled\job_intelligent\models\bronze\stg_jobs_raw.sql
[0m15:08:07.209229 [info ] [MainThread]: 
[0m15:08:07.212747 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=12 NO-OP=0 TOTAL=13
[0m15:08:07.217030 [debug] [MainThread]: Command `dbt run` failed at 15:08:07.217030 after 12.28 seconds
[0m15:08:07.219311 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002BEB1E821C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002BEB325DCD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002BEB4650310>]}
[0m15:08:07.222427 [debug] [MainThread]: Flushing usage events
[0m15:08:07.967264 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:10:05.239430 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002DB935D20D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002DB948C7370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002DB948C71C0>]}


============================== 15:10:05.240480 | 9b50e0b0-59e2-4f23-91ac-c2e418e79da1 ==============================
[0m15:10:05.240480 [info ] [MainThread]: Running with dbt=1.10.18
[0m15:10:05.240480 [debug] [MainThread]: running dbt with arguments {'log_path': 'C:\\Users\\Ayoub Gorry\\Desktop\\powerbi\\jobs-power-bi\\dbt_project\\logs', 'log_format': 'default', 'partial_parse': 'True', 'version_check': 'True', 'empty': 'False', 'log_cache_events': 'False', 'cache_selected_only': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'debug': 'False', 'send_anonymous_usage_stats': 'True', 'profiles_dir': 'C:\\Users\\Ayoub Gorry\\Desktop\\powerbi\\jobs-power-bi\\dbt_project', 'use_experimental_parser': 'False', 'introspect': 'True', 'invocation_command': 'dbt run', 'printer_width': '80', 'no_print': 'None', 'target_path': 'None', 'use_colors': 'True', 'quiet': 'False', 'static_parser': 'True', 'warn_error': 'None', 'fail_fast': 'False', 'write_json': 'True', 'indirect_selection': 'eager'}
[0m15:10:05.852021 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '9b50e0b0-59e2-4f23-91ac-c2e418e79da1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002DB94873070>]}
[0m15:10:06.026841 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '9b50e0b0-59e2-4f23-91ac-c2e418e79da1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002DB94951AF0>]}
[0m15:10:06.030450 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m15:10:06.524541 [debug] [MainThread]: checksum: 9646db7d1a5f5b9389d9c545d3ebf898b26bc1f63f6b5366f34a6af01d52cb91, vars: {}, profile: , target: , version: 1.10.18
[0m15:10:06.869315 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:10:06.873978 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:10:06.891941 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- seeds
[0m15:10:06.969262 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '9b50e0b0-59e2-4f23-91ac-c2e418e79da1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002DB95EFB130>]}
[0m15:10:07.083471 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Ayoub Gorry\Desktop\powerbi\jobs-power-bi\dbt_project\target\manifest.json
[0m15:10:07.089203 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Ayoub Gorry\Desktop\powerbi\jobs-power-bi\dbt_project\target\semantic_manifest.json
[0m15:10:07.140399 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '9b50e0b0-59e2-4f23-91ac-c2e418e79da1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002DB95F11C70>]}
[0m15:10:07.140399 [info ] [MainThread]: Found 13 models, 456 macros
[0m15:10:07.140399 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9b50e0b0-59e2-4f23-91ac-c2e418e79da1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002DB95D7B250>]}
[0m15:10:07.149315 [info ] [MainThread]: 
[0m15:10:07.150337 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:10:07.152244 [info ] [MainThread]: 
[0m15:10:07.153912 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m15:10:07.168523 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_duckdb'
[0m15:10:07.170914 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_duckdb'
[0m15:10:07.179214 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_duckdb'
[0m15:10:07.319389 [debug] [ThreadPool]: Using duckdb connection "list_duckdb"
[0m15:10:07.321594 [debug] [ThreadPool]: Using duckdb connection "list_duckdb"
[0m15:10:07.321594 [debug] [ThreadPool]: Using duckdb connection "list_duckdb"
[0m15:10:07.321594 [debug] [ThreadPool]: On list_duckdb: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_duckdb"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"duckdb"'
    
  
  
[0m15:10:07.321594 [debug] [ThreadPool]: On list_duckdb: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_duckdb"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"duckdb"'
    
  
  
[0m15:10:07.326324 [debug] [ThreadPool]: On list_duckdb: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_duckdb"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"duckdb"'
    
  
  
[0m15:10:07.328552 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:10:07.329589 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:10:07.330780 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:10:07.373512 [debug] [ThreadPool]: SQL status: OK in 0.046 seconds
[0m15:10:07.374660 [debug] [ThreadPool]: SQL status: OK in 0.043 seconds
[0m15:10:07.374660 [debug] [ThreadPool]: SQL status: OK in 0.046 seconds
[0m15:10:07.379271 [debug] [ThreadPool]: On list_duckdb: Close
[0m15:10:07.382368 [debug] [ThreadPool]: On list_duckdb: Close
[0m15:10:07.382368 [debug] [ThreadPool]: On list_duckdb: Close
[0m15:10:07.388437 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_duckdb, now create_duckdb_main_gold)
[0m15:10:07.389525 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_duckdb, now create_duckdb_main_silver)
[0m15:10:07.390742 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_duckdb, now create_duckdb_main_bronze)
[0m15:10:07.390742 [debug] [ThreadPool]: Creating schema "database: "duckdb"
schema: "main_gold"
"
[0m15:10:07.392767 [debug] [ThreadPool]: Creating schema "database: "duckdb"
schema: "main_silver"
"
[0m15:10:07.392767 [debug] [ThreadPool]: Creating schema "database: "duckdb"
schema: "main_bronze"
"
[0m15:10:07.404938 [debug] [ThreadPool]: Using duckdb connection "create_duckdb_main_gold"
[0m15:10:07.410227 [debug] [ThreadPool]: Using duckdb connection "create_duckdb_main_silver"
[0m15:10:07.413398 [debug] [ThreadPool]: Using duckdb connection "create_duckdb_main_bronze"
[0m15:10:07.413398 [debug] [ThreadPool]: On create_duckdb_main_gold: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_duckdb_main_gold"} */

    
        select type from duckdb_databases()
        where lower(database_name)='duckdb'
        and type='sqlite'
    
  
[0m15:10:07.413398 [debug] [ThreadPool]: On create_duckdb_main_silver: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_duckdb_main_silver"} */

    
        select type from duckdb_databases()
        where lower(database_name)='duckdb'
        and type='sqlite'
    
  
[0m15:10:07.419507 [debug] [ThreadPool]: On create_duckdb_main_bronze: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_duckdb_main_bronze"} */

    
        select type from duckdb_databases()
        where lower(database_name)='duckdb'
        and type='sqlite'
    
  
[0m15:10:07.419507 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:10:07.419507 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:10:07.422374 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:10:07.425570 [debug] [ThreadPool]: SQL status: OK in 0.006 seconds
[0m15:10:07.426899 [debug] [ThreadPool]: SQL status: OK in 0.006 seconds
[0m15:10:07.428438 [debug] [ThreadPool]: SQL status: OK in 0.005 seconds
[0m15:10:07.430704 [debug] [ThreadPool]: Using duckdb connection "create_duckdb_main_gold"
[0m15:10:07.432928 [debug] [ThreadPool]: Using duckdb connection "create_duckdb_main_silver"
[0m15:10:07.436492 [debug] [ThreadPool]: Using duckdb connection "create_duckdb_main_bronze"
[0m15:10:07.438668 [debug] [ThreadPool]: On create_duckdb_main_gold: BEGIN
[0m15:10:07.439197 [debug] [ThreadPool]: On create_duckdb_main_silver: BEGIN
[0m15:10:07.440010 [debug] [ThreadPool]: On create_duckdb_main_bronze: BEGIN
[0m15:10:07.440010 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m15:10:07.443417 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m15:10:07.444426 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m15:10:07.444426 [debug] [ThreadPool]: Using duckdb connection "create_duckdb_main_gold"
[0m15:10:07.444426 [debug] [ThreadPool]: Using duckdb connection "create_duckdb_main_silver"
[0m15:10:07.444426 [debug] [ThreadPool]: Using duckdb connection "create_duckdb_main_bronze"
[0m15:10:07.444426 [debug] [ThreadPool]: On create_duckdb_main_gold: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_duckdb_main_gold"} */

    
    
        create schema if not exists "duckdb"."main_gold"
    
[0m15:10:07.448496 [debug] [ThreadPool]: On create_duckdb_main_silver: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_duckdb_main_silver"} */

    
    
        create schema if not exists "duckdb"."main_silver"
    
[0m15:10:07.450633 [debug] [ThreadPool]: On create_duckdb_main_bronze: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_duckdb_main_bronze"} */

    
    
        create schema if not exists "duckdb"."main_bronze"
    
[0m15:10:07.453446 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m15:10:07.455422 [debug] [ThreadPool]: SQL status: OK in 0.002 seconds
[0m15:10:07.455422 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m15:10:07.459101 [debug] [ThreadPool]: On create_duckdb_main_gold: COMMIT
[0m15:10:07.461681 [debug] [ThreadPool]: On create_duckdb_main_silver: COMMIT
[0m15:10:07.461681 [debug] [ThreadPool]: On create_duckdb_main_bronze: COMMIT
[0m15:10:07.463697 [debug] [ThreadPool]: Using duckdb connection "create_duckdb_main_gold"
[0m15:10:07.463697 [debug] [ThreadPool]: Using duckdb connection "create_duckdb_main_silver"
[0m15:10:07.463697 [debug] [ThreadPool]: Using duckdb connection "create_duckdb_main_bronze"
[0m15:10:07.463697 [debug] [ThreadPool]: On create_duckdb_main_gold: COMMIT
[0m15:10:07.463697 [debug] [ThreadPool]: On create_duckdb_main_silver: COMMIT
[0m15:10:07.467859 [debug] [ThreadPool]: On create_duckdb_main_bronze: COMMIT
[0m15:10:07.469424 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m15:10:07.470644 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m15:10:07.470644 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:10:07.470644 [debug] [ThreadPool]: On create_duckdb_main_gold: Close
[0m15:10:07.470644 [debug] [ThreadPool]: On create_duckdb_main_silver: Close
[0m15:10:07.470644 [debug] [ThreadPool]: On create_duckdb_main_bronze: Close
[0m15:10:07.476285 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_duckdb_main_gold'
[0m15:10:07.484627 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_duckdb_main_silver'
[0m15:10:07.484627 [debug] [ThreadPool]: Using duckdb connection "list_duckdb_main_gold"
[0m15:10:07.484627 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_duckdb_main_bronze'
[0m15:10:07.490433 [debug] [ThreadPool]: Using duckdb connection "list_duckdb_main_silver"
[0m15:10:07.490433 [debug] [ThreadPool]: On list_duckdb_main_gold: BEGIN
[0m15:10:07.492977 [debug] [ThreadPool]: Using duckdb connection "list_duckdb_main_bronze"
[0m15:10:07.492977 [debug] [ThreadPool]: On list_duckdb_main_silver: BEGIN
[0m15:10:07.492977 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:10:07.492977 [debug] [ThreadPool]: On list_duckdb_main_bronze: BEGIN
[0m15:10:07.492977 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:10:07.499175 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:10:07.499175 [debug] [ThreadPool]: SQL status: OK in 0.003 seconds
[0m15:10:07.499175 [debug] [ThreadPool]: SQL status: OK in 0.003 seconds
[0m15:10:07.501210 [debug] [ThreadPool]: SQL status: OK in 0.003 seconds
[0m15:10:07.501210 [debug] [ThreadPool]: Using duckdb connection "list_duckdb_main_gold"
[0m15:10:07.501210 [debug] [ThreadPool]: Using duckdb connection "list_duckdb_main_silver"
[0m15:10:07.501210 [debug] [ThreadPool]: Using duckdb connection "list_duckdb_main_bronze"
[0m15:10:07.501210 [debug] [ThreadPool]: On list_duckdb_main_gold: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_duckdb_main_gold"} */
select
      'duckdb' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main_gold'
    and lower(table_catalog) = 'duckdb'
  
[0m15:10:07.501210 [debug] [ThreadPool]: On list_duckdb_main_silver: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_duckdb_main_silver"} */
select
      'duckdb' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main_silver'
    and lower(table_catalog) = 'duckdb'
  
[0m15:10:07.501210 [debug] [ThreadPool]: On list_duckdb_main_bronze: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_duckdb_main_bronze"} */
select
      'duckdb' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main_bronze'
    and lower(table_catalog) = 'duckdb'
  
[0m15:10:07.533960 [debug] [ThreadPool]: SQL status: OK in 0.026 seconds
[0m15:10:07.533960 [debug] [ThreadPool]: SQL status: OK in 0.028 seconds
[0m15:10:07.539090 [debug] [ThreadPool]: On list_duckdb_main_silver: ROLLBACK
[0m15:10:07.539090 [debug] [ThreadPool]: SQL status: OK in 0.032 seconds
[0m15:10:07.540854 [debug] [ThreadPool]: On list_duckdb_main_bronze: ROLLBACK
[0m15:10:07.540854 [debug] [ThreadPool]: Failed to rollback 'list_duckdb_main_silver'
[0m15:10:07.540854 [debug] [ThreadPool]: On list_duckdb_main_gold: ROLLBACK
[0m15:10:07.540854 [debug] [ThreadPool]: Failed to rollback 'list_duckdb_main_bronze'
[0m15:10:07.540854 [debug] [ThreadPool]: On list_duckdb_main_silver: Close
[0m15:10:07.540854 [debug] [ThreadPool]: Failed to rollback 'list_duckdb_main_gold'
[0m15:10:07.540854 [debug] [ThreadPool]: On list_duckdb_main_bronze: Close
[0m15:10:07.549572 [debug] [ThreadPool]: On list_duckdb_main_gold: Close
[0m15:10:07.550625 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9b50e0b0-59e2-4f23-91ac-c2e418e79da1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002DB9142AE50>]}
[0m15:10:07.550625 [debug] [MainThread]: Using duckdb connection "master"
[0m15:10:07.550625 [debug] [MainThread]: On master: BEGIN
[0m15:10:07.550625 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:10:07.556798 [debug] [MainThread]: SQL status: OK in 0.001 seconds
[0m15:10:07.556798 [debug] [MainThread]: On master: COMMIT
[0m15:10:07.558725 [debug] [MainThread]: Using duckdb connection "master"
[0m15:10:07.559250 [debug] [MainThread]: On master: COMMIT
[0m15:10:07.559250 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:10:07.559250 [debug] [MainThread]: On master: Close
[0m15:10:07.559250 [debug] [Thread-1  ]: Began running node model.job_intelligent.stg_jobs_raw
[0m15:10:07.568444 [info ] [Thread-1  ]: 1 of 13 START sql view model main_bronze.stg_jobs_raw .......................... [RUN]
[0m15:10:07.569474 [debug] [Thread-1  ]: Acquiring new duckdb connection 'model.job_intelligent.stg_jobs_raw'
[0m15:10:07.570589 [debug] [Thread-1  ]: Began compiling node model.job_intelligent.stg_jobs_raw
[0m15:10:07.581169 [debug] [Thread-1  ]: Writing injected SQL for node "model.job_intelligent.stg_jobs_raw"
[0m15:10:07.583176 [debug] [Thread-1  ]: Began executing node model.job_intelligent.stg_jobs_raw
[0m15:10:07.650787 [debug] [Thread-1  ]: Writing runtime sql for node "model.job_intelligent.stg_jobs_raw"
[0m15:10:07.650787 [debug] [Thread-1  ]: Using duckdb connection "model.job_intelligent.stg_jobs_raw"
[0m15:10:07.656144 [debug] [Thread-1  ]: On model.job_intelligent.stg_jobs_raw: BEGIN
[0m15:10:07.656144 [debug] [Thread-1  ]: Opening a new connection, currently in state init
[0m15:10:07.659095 [debug] [Thread-1  ]: SQL status: OK in 0.002 seconds
[0m15:10:07.659095 [debug] [Thread-1  ]: Using duckdb connection "model.job_intelligent.stg_jobs_raw"
[0m15:10:07.659095 [debug] [Thread-1  ]: On model.job_intelligent.stg_jobs_raw: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.stg_jobs_raw"} */

  
  create view "duckdb"."main_bronze"."stg_jobs_raw__dbt_tmp" as (
    -- models/bronze/stg_jobs_raw.sql
-- Bronze layer: Lecture directe des données brutes depuis final_data.csv
-- Renommage et typage minimal



-- Read directly from CSV file
SELECT
    -- Renommer les colonnes pour cohérence
    title as job_title,
    location,
    postedTime as posted_time,
    publishedAt as published_at,
    jobUrl as job_url,
    companyName as company_name,
    companyUrl as company_url,
    description as job_description,
    contractType as contract_type,
    workType as work_type,
    
    -- Métadonnées de traçabilité
    NOW() as ingestion_timestamp,
    '2026-01-08 14:10:05.180756+00:00' as dbt_run_id

FROM read_csv_auto('D:/lab2/final_data.csv')

WHERE 1=1
    -- Filtrer les lignes vides
    AND title IS NOT NULL
    AND description IS NOT NULL
  );

[0m15:10:07.663345 [debug] [Thread-1  ]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.stg_jobs_raw"} */

  
  create view "duckdb"."main_bronze"."stg_jobs_raw__dbt_tmp" as (
    -- models/bronze/stg_jobs_raw.sql
-- Bronze layer: Lecture directe des données brutes depuis final_data.csv
-- Renommage et typage minimal



-- Read directly from CSV file
SELECT
    -- Renommer les colonnes pour cohérence
    title as job_title,
    location,
    postedTime as posted_time,
    publishedAt as published_at,
    jobUrl as job_url,
    companyName as company_name,
    companyUrl as company_url,
    description as job_description,
    contractType as contract_type,
    workType as work_type,
    
    -- Métadonnées de traçabilité
    NOW() as ingestion_timestamp,
    '2026-01-08 14:10:05.180756+00:00' as dbt_run_id

FROM read_csv_auto('D:/lab2/final_data.csv')

WHERE 1=1
    -- Filtrer les lignes vides
    AND title IS NOT NULL
    AND description IS NOT NULL
  );

[0m15:10:07.665361 [debug] [Thread-1  ]: DuckDB adapter: Rolling back transaction.
[0m15:10:07.665361 [debug] [Thread-1  ]: On model.job_intelligent.stg_jobs_raw: ROLLBACK
[0m15:10:07.670370 [debug] [Thread-1  ]: Failed to rollback 'model.job_intelligent.stg_jobs_raw'
[0m15:10:07.673567 [debug] [Thread-1  ]: On model.job_intelligent.stg_jobs_raw: Close
[0m15:10:07.673567 [debug] [Thread-1  ]: Runtime Error in model stg_jobs_raw (models\bronze\stg_jobs_raw.sql)
  IO Error: No files found that match the pattern "D:/lab2/final_data.csv"
  
  LINE 29: FROM read_csv_auto('D:/lab2/final_data.csv')
                ^
[0m15:10:07.679269 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9b50e0b0-59e2-4f23-91ac-c2e418e79da1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002DB924E3CD0>]}
[0m15:10:07.679269 [error] [Thread-1  ]: 1 of 13 ERROR creating sql view model main_bronze.stg_jobs_raw ................. [[31mERROR[0m in 0.11s]
[0m15:10:07.679269 [debug] [Thread-1  ]: Finished running node model.job_intelligent.stg_jobs_raw
[0m15:10:07.688289 [debug] [Thread-7  ]: Marking all children of 'model.job_intelligent.stg_jobs_raw' to be skipped because of status 'error'.  Reason: Runtime Error in model stg_jobs_raw (models\bronze\stg_jobs_raw.sql)
  IO Error: No files found that match the pattern "D:/lab2/final_data.csv"
  
  LINE 29: FROM read_csv_auto('D:/lab2/final_data.csv')
                ^.
[0m15:10:07.690651 [debug] [Thread-3  ]: Began running node model.job_intelligent.int_jobs_cleaned
[0m15:10:07.691431 [info ] [Thread-3  ]: 2 of 13 SKIP relation main_silver.int_jobs_cleaned ............................. [[33mSKIP[0m]
[0m15:10:07.691431 [debug] [Thread-3  ]: Finished running node model.job_intelligent.int_jobs_cleaned
[0m15:10:07.694936 [debug] [Thread-2  ]: Began running node model.job_intelligent.int_job_title_normalization
[0m15:10:07.697259 [info ] [Thread-2  ]: 3 of 13 SKIP relation main_silver.int_job_title_normalization .................. [[33mSKIP[0m]
[0m15:10:07.700046 [debug] [Thread-2  ]: Finished running node model.job_intelligent.int_job_title_normalization
[0m15:10:07.701263 [debug] [Thread-4  ]: Began running node model.job_intelligent.dim_company
[0m15:10:07.701263 [debug] [Thread-3  ]: Began running node model.job_intelligent.dim_location
[0m15:10:07.705078 [debug] [Thread-1  ]: Began running node model.job_intelligent.dim_time
[0m15:10:07.707907 [debug] [Thread-2  ]: Began running node model.job_intelligent.int_skills_extraction
[0m15:10:07.709456 [info ] [Thread-4  ]: 4 of 13 SKIP relation main_gold.dim_company .................................... [[33mSKIP[0m]
[0m15:10:07.711496 [info ] [Thread-3  ]: 5 of 13 SKIP relation main_gold.dim_location ................................... [[33mSKIP[0m]
[0m15:10:07.713527 [info ] [Thread-1  ]: 6 of 13 SKIP relation main_gold.dim_time ....................................... [[33mSKIP[0m]
[0m15:10:07.713527 [info ] [Thread-2  ]: 7 of 13 SKIP relation main_silver.int_skills_extraction ........................ [[33mSKIP[0m]
[0m15:10:07.718577 [debug] [Thread-4  ]: Finished running node model.job_intelligent.dim_company
[0m15:10:07.721308 [debug] [Thread-3  ]: Finished running node model.job_intelligent.dim_location
[0m15:10:07.724040 [debug] [Thread-1  ]: Finished running node model.job_intelligent.dim_time
[0m15:10:07.724040 [debug] [Thread-2  ]: Finished running node model.job_intelligent.int_skills_extraction
[0m15:10:07.729970 [debug] [Thread-4  ]: Began running node model.job_intelligent.fact_job_offers
[0m15:10:07.731215 [debug] [Thread-1  ]: Began running node model.job_intelligent.dim_skills
[0m15:10:07.732533 [info ] [Thread-4  ]: 8 of 13 SKIP relation main_gold.fact_job_offers ................................ [[33mSKIP[0m]
[0m15:10:07.732533 [info ] [Thread-1  ]: 9 of 13 SKIP relation main_gold.dim_skills ..................................... [[33mSKIP[0m]
[0m15:10:07.739586 [debug] [Thread-4  ]: Finished running node model.job_intelligent.fact_job_offers
[0m15:10:07.740949 [debug] [Thread-1  ]: Finished running node model.job_intelligent.dim_skills
[0m15:10:07.740949 [debug] [Thread-1  ]: Began running node model.job_intelligent.agg_job_offers_by_category_time
[0m15:10:07.740949 [debug] [Thread-2  ]: Began running node model.job_intelligent.agg_location_analysis
[0m15:10:07.745565 [debug] [Thread-3  ]: Began running node model.job_intelligent.fact_job_skills
[0m15:10:07.747602 [info ] [Thread-1  ]: 10 of 13 SKIP relation main_gold.agg_job_offers_by_category_time ............... [[33mSKIP[0m]
[0m15:10:07.749147 [info ] [Thread-2  ]: 11 of 13 SKIP relation main_gold.agg_location_analysis ......................... [[33mSKIP[0m]
[0m15:10:07.752066 [info ] [Thread-3  ]: 12 of 13 SKIP relation main_gold.fact_job_skills ............................... [[33mSKIP[0m]
[0m15:10:07.755105 [debug] [Thread-1  ]: Finished running node model.job_intelligent.agg_job_offers_by_category_time
[0m15:10:07.758522 [debug] [Thread-2  ]: Finished running node model.job_intelligent.agg_location_analysis
[0m15:10:07.760809 [debug] [Thread-3  ]: Finished running node model.job_intelligent.fact_job_skills
[0m15:10:07.765231 [debug] [Thread-4  ]: Began running node model.job_intelligent.agg_skills_demand
[0m15:10:07.765231 [info ] [Thread-4  ]: 13 of 13 SKIP relation main_gold.agg_skills_demand ............................. [[33mSKIP[0m]
[0m15:10:07.769098 [debug] [Thread-4  ]: Finished running node model.job_intelligent.agg_skills_demand
[0m15:10:07.773737 [debug] [MainThread]: Using duckdb connection "master"
[0m15:10:07.773737 [debug] [MainThread]: On master: BEGIN
[0m15:10:07.778380 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m15:10:07.780586 [debug] [MainThread]: SQL status: OK in 0.002 seconds
[0m15:10:07.781847 [debug] [MainThread]: On master: COMMIT
[0m15:10:07.781847 [debug] [MainThread]: Using duckdb connection "master"
[0m15:10:07.785213 [debug] [MainThread]: On master: COMMIT
[0m15:10:07.789290 [debug] [MainThread]: SQL status: OK in 0.001 seconds
[0m15:10:07.790333 [debug] [MainThread]: On master: Close
[0m15:10:07.790333 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:10:07.790333 [debug] [MainThread]: Connection 'create_duckdb_main_gold' was properly closed.
[0m15:10:07.790333 [debug] [MainThread]: Connection 'create_duckdb_main_silver' was properly closed.
[0m15:10:07.790333 [debug] [MainThread]: Connection 'create_duckdb_main_bronze' was properly closed.
[0m15:10:07.790333 [debug] [MainThread]: Connection 'list_duckdb_main_gold' was properly closed.
[0m15:10:07.790333 [debug] [MainThread]: Connection 'list_duckdb_main_silver' was properly closed.
[0m15:10:07.790333 [debug] [MainThread]: Connection 'list_duckdb_main_bronze' was properly closed.
[0m15:10:07.790333 [debug] [MainThread]: Connection 'model.job_intelligent.stg_jobs_raw' was properly closed.
[0m15:10:07.799203 [info ] [MainThread]: 
[0m15:10:07.800246 [info ] [MainThread]: Finished running 12 table models, 1 view model in 0 hours 0 minutes and 0.64 seconds (0.64s).
[0m15:10:07.800246 [debug] [MainThread]: Command end result
[0m15:10:07.859767 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Ayoub Gorry\Desktop\powerbi\jobs-power-bi\dbt_project\target\manifest.json
[0m15:10:07.864475 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Ayoub Gorry\Desktop\powerbi\jobs-power-bi\dbt_project\target\semantic_manifest.json
[0m15:10:07.881079 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\Ayoub Gorry\Desktop\powerbi\jobs-power-bi\dbt_project\target\run_results.json
[0m15:10:07.883912 [info ] [MainThread]: 
[0m15:10:07.886203 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m15:10:07.889260 [info ] [MainThread]: 
[0m15:10:07.891747 [error] [MainThread]: [31mFailure in model stg_jobs_raw (models\bronze\stg_jobs_raw.sql)[0m
[0m15:10:07.891747 [error] [MainThread]:   Runtime Error in model stg_jobs_raw (models\bronze\stg_jobs_raw.sql)
  IO Error: No files found that match the pattern "D:/lab2/final_data.csv"
  
  LINE 29: FROM read_csv_auto('D:/lab2/final_data.csv')
                ^
[0m15:10:07.891747 [info ] [MainThread]: 
[0m15:10:07.900779 [info ] [MainThread]:   compiled code at target\compiled\job_intelligent\models\bronze\stg_jobs_raw.sql
[0m15:10:07.904906 [info ] [MainThread]: 
[0m15:10:07.908238 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=12 NO-OP=0 TOTAL=13
[0m15:10:07.912940 [debug] [MainThread]: Command `dbt run` failed at 15:10:07.912355 after 2.85 seconds
[0m15:10:07.915086 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002DB935D20D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002DB9136D9D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002DB939A3370>]}
[0m15:10:07.916652 [debug] [MainThread]: Flushing usage events
[0m15:10:08.750560 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:13:08.651877 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019D92BC10D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019D93EC4CA0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019D93EC4E50>]}


============================== 15:13:08.660369 | 272558a1-80cc-4e47-abd4-2ca5be3d416e ==============================
[0m15:13:08.660369 [info ] [MainThread]: Running with dbt=1.10.18
[0m15:13:08.660369 [debug] [MainThread]: running dbt with arguments {'log_path': 'C:\\Users\\Ayoub Gorry\\Desktop\\powerbi\\jobs-power-bi\\dbt_project\\logs', 'log_format': 'default', 'partial_parse': 'True', 'log_cache_events': 'False', 'empty': 'None', 'version_check': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Ayoub Gorry\\Desktop\\powerbi\\jobs-power-bi\\dbt_project', 'debug': 'False', 'send_anonymous_usage_stats': 'True', 'target_path': 'None', 'no_print': 'None', 'introspect': 'True', 'invocation_command': 'dbt debug', 'use_experimental_parser': 'False', 'printer_width': '80', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'static_parser': 'True', 'quiet': 'False', 'use_colors': 'True', 'warn_error': 'None', 'fail_fast': 'False', 'write_json': 'True', 'indirect_selection': 'eager'}
[0m15:13:08.706445 [info ] [MainThread]: dbt version: 1.10.18
[0m15:13:08.706445 [info ] [MainThread]: python version: 3.9.13
[0m15:13:08.708361 [info ] [MainThread]: python path: C:\Users\Ayoub Gorry\AppData\Local\Programs\Python\Python39\python.exe
[0m15:13:08.709406 [info ] [MainThread]: os info: Windows-10-10.0.26100-SP0
[0m15:13:08.930797 [info ] [MainThread]: Using profiles dir at C:\Users\Ayoub Gorry\Desktop\powerbi\jobs-power-bi\dbt_project
[0m15:13:08.930797 [info ] [MainThread]: Using profiles.yml file at C:\Users\Ayoub Gorry\Desktop\powerbi\jobs-power-bi\dbt_project\profiles.yml
[0m15:13:08.930797 [info ] [MainThread]: Using dbt_project.yml file at C:\Users\Ayoub Gorry\Desktop\powerbi\jobs-power-bi\dbt_project\dbt_project.yml
[0m15:13:08.949352 [info ] [MainThread]: adapter type: duckdb
[0m15:13:08.952269 [info ] [MainThread]: adapter version: 1.10.0
[0m15:13:09.152426 [info ] [MainThread]: Configuration:
[0m15:13:09.152426 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m15:13:09.152426 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m15:13:09.158730 [info ] [MainThread]: Required dependencies:
[0m15:13:09.159252 [debug] [MainThread]: Executing "git --help"
[0m15:13:09.219240 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--no-lazy-fetch]\n           [--no-optional-locks] [--no-advice] [--bare] [--git-dir=<path>]\n           [--work-tree=<path>] [--namespace=<name>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone      Clone a repository into a new directory\n   init       Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add        Add file contents to the index\n   mv         Move or rename a file, a directory, or a symlink\n   restore    Restore working tree files\n   rm         Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect     Use binary search to find the commit that introduced a bug\n   diff       Show changes between commits, commit and working tree, etc\n   grep       Print lines matching a pattern\n   log        Show commit logs\n   show       Show various types of objects\n   status     Show the working tree status\n\ngrow, mark and tweak your common history\n   backfill   Download missing objects in a partial clone\n   branch     List, create, or delete branches\n   commit     Record changes to the repository\n   merge      Join two or more development histories together\n   rebase     Reapply commits on top of another base tip\n   reset      Reset current HEAD to the specified state\n   switch     Switch branches\n   tag        Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch      Download objects and refs from another repository\n   pull       Fetch from and integrate with another repository or a local branch\n   push       Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m15:13:09.219240 [debug] [MainThread]: STDERR: "b''"
[0m15:13:09.219240 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m15:13:09.224595 [info ] [MainThread]: Connection:
[0m15:13:09.224595 [info ] [MainThread]:   database: duckdb
[0m15:13:09.227474 [info ] [MainThread]:   schema: main
[0m15:13:09.228492 [info ] [MainThread]:   path: duckdb.db
[0m15:13:09.230752 [info ] [MainThread]:   config_options: None
[0m15:13:09.230752 [info ] [MainThread]:   extensions: None
[0m15:13:09.232768 [info ] [MainThread]:   settings: {}
[0m15:13:09.235197 [info ] [MainThread]:   external_root: .
[0m15:13:09.235197 [info ] [MainThread]:   use_credential_provider: None
[0m15:13:09.238323 [info ] [MainThread]:   attach: None
[0m15:13:09.239657 [info ] [MainThread]:   filesystems: None
[0m15:13:09.240804 [info ] [MainThread]:   remote: None
[0m15:13:09.240804 [info ] [MainThread]:   plugins: None
[0m15:13:09.240804 [info ] [MainThread]:   disable_transactions: False
[0m15:13:09.244145 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m15:13:09.706485 [debug] [MainThread]: Acquiring new duckdb connection 'debug'
[0m15:13:09.861288 [debug] [MainThread]: Using duckdb connection "debug"
[0m15:13:09.864137 [debug] [MainThread]: On debug: select 1 as id
[0m15:13:09.864137 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:13:09.900232 [debug] [MainThread]: SQL status: OK in 0.035 seconds
[0m15:13:09.900232 [debug] [MainThread]: On debug: Close
[0m15:13:09.902949 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m15:13:09.902949 [info ] [MainThread]: [32mAll checks passed![0m
[0m15:13:09.902949 [debug] [MainThread]: Command `dbt debug` succeeded at 15:13:09.902949 after 1.41 seconds
[0m15:13:09.908397 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m15:13:09.909492 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019D92BC10D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019D95020F40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019D92F5F220>]}
[0m15:13:09.910702 [debug] [MainThread]: Flushing usage events
[0m15:13:10.829314 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:14:16.903997 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FD3B5C10A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FD3C8B73A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FD3C8B71F0>]}


============================== 15:14:16.913737 | ab1f0716-63a5-45ca-8dd4-d9fc9cac367f ==============================
[0m15:14:16.913737 [info ] [MainThread]: Running with dbt=1.10.18
[0m15:14:16.913737 [debug] [MainThread]: running dbt with arguments {'log_path': 'C:\\Users\\Ayoub Gorry\\Desktop\\powerbi\\jobs-power-bi\\dbt_project\\logs', 'log_format': 'default', 'partial_parse': 'True', 'log_cache_events': 'False', 'empty': 'False', 'version_check': 'True', 'cache_selected_only': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'use_experimental_parser': 'False', 'send_anonymous_usage_stats': 'True', 'no_print': 'None', 'profiles_dir': 'C:\\Users\\Ayoub Gorry\\Desktop\\powerbi\\jobs-power-bi\\dbt_project', 'introspect': 'True', 'invocation_command': 'dbt run', 'target_path': 'None', 'debug': 'False', 'printer_width': '80', 'static_parser': 'True', 'quiet': 'False', 'use_colors': 'True', 'fail_fast': 'False', 'warn_error': 'None', 'write_json': 'True', 'indirect_selection': 'eager'}
[0m15:14:17.558490 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'ab1f0716-63a5-45ca-8dd4-d9fc9cac367f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FD39352C70>]}
[0m15:14:17.700437 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'ab1f0716-63a5-45ca-8dd4-d9fc9cac367f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FD3C82E7C0>]}
[0m15:14:17.703831 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m15:14:18.249196 [debug] [MainThread]: checksum: 9646db7d1a5f5b9389d9c545d3ebf898b26bc1f63f6b5366f34a6af01d52cb91, vars: {}, profile: , target: , version: 1.10.18
[0m15:14:18.598270 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:14:18.599302 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:14:18.615975 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- seeds
[0m15:14:18.671297 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ab1f0716-63a5-45ca-8dd4-d9fc9cac367f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FD3DEEB130>]}
[0m15:14:18.815100 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Ayoub Gorry\Desktop\powerbi\jobs-power-bi\dbt_project\target\manifest.json
[0m15:14:18.820724 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Ayoub Gorry\Desktop\powerbi\jobs-power-bi\dbt_project\target\semantic_manifest.json
[0m15:14:18.960457 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ab1f0716-63a5-45ca-8dd4-d9fc9cac367f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FD3DF02C40>]}
[0m15:14:18.960457 [info ] [MainThread]: Found 13 models, 456 macros
[0m15:14:18.963737 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ab1f0716-63a5-45ca-8dd4-d9fc9cac367f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FD3DD8E3D0>]}
[0m15:14:18.966748 [info ] [MainThread]: 
[0m15:14:18.969281 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:14:18.969281 [info ] [MainThread]: 
[0m15:14:18.969281 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m15:14:18.992021 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_duckdb'
[0m15:14:18.992021 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_duckdb'
[0m15:14:19.048830 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_duckdb'
[0m15:14:19.213759 [debug] [ThreadPool]: Using duckdb connection "list_duckdb"
[0m15:14:19.218326 [debug] [ThreadPool]: Using duckdb connection "list_duckdb"
[0m15:14:19.219450 [debug] [ThreadPool]: Using duckdb connection "list_duckdb"
[0m15:14:19.220679 [debug] [ThreadPool]: On list_duckdb: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_duckdb"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"duckdb"'
    
  
  
[0m15:14:19.222675 [debug] [ThreadPool]: On list_duckdb: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_duckdb"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"duckdb"'
    
  
  
[0m15:14:19.222675 [debug] [ThreadPool]: On list_duckdb: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_duckdb"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"duckdb"'
    
  
  
[0m15:14:19.222675 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:14:19.222675 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:14:19.222675 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:14:19.258720 [debug] [ThreadPool]: SQL status: OK in 0.033 seconds
[0m15:14:19.259247 [debug] [ThreadPool]: SQL status: OK in 0.033 seconds
[0m15:14:19.259247 [debug] [ThreadPool]: SQL status: OK in 0.032 seconds
[0m15:14:19.259247 [debug] [ThreadPool]: On list_duckdb: Close
[0m15:14:19.259247 [debug] [ThreadPool]: On list_duckdb: Close
[0m15:14:19.266583 [debug] [ThreadPool]: On list_duckdb: Close
[0m15:14:19.271842 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_duckdb, now create_duckdb_main_silver)
[0m15:14:19.271842 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_duckdb, now create_duckdb_main_bronze)
[0m15:14:19.274006 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_duckdb, now create_duckdb_main_gold)
[0m15:14:19.274006 [debug] [ThreadPool]: Creating schema "database: "duckdb"
schema: "main_silver"
"
[0m15:14:19.275818 [debug] [ThreadPool]: Creating schema "database: "duckdb"
schema: "main_bronze"
"
[0m15:14:19.278335 [debug] [ThreadPool]: Creating schema "database: "duckdb"
schema: "main_gold"
"
[0m15:14:19.290383 [debug] [ThreadPool]: Using duckdb connection "create_duckdb_main_silver"
[0m15:14:19.299202 [debug] [ThreadPool]: Using duckdb connection "create_duckdb_main_bronze"
[0m15:14:19.305449 [debug] [ThreadPool]: Using duckdb connection "create_duckdb_main_gold"
[0m15:14:19.305974 [debug] [ThreadPool]: On create_duckdb_main_silver: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_duckdb_main_silver"} */

    
        select type from duckdb_databases()
        where lower(database_name)='duckdb'
        and type='sqlite'
    
  
[0m15:14:19.306761 [debug] [ThreadPool]: On create_duckdb_main_bronze: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_duckdb_main_bronze"} */

    
        select type from duckdb_databases()
        where lower(database_name)='duckdb'
        and type='sqlite'
    
  
[0m15:14:19.306761 [debug] [ThreadPool]: On create_duckdb_main_gold: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_duckdb_main_gold"} */

    
        select type from duckdb_databases()
        where lower(database_name)='duckdb'
        and type='sqlite'
    
  
[0m15:14:19.308315 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:14:19.309419 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:14:19.310383 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:14:19.311705 [debug] [ThreadPool]: SQL status: OK in 0.003 seconds
[0m15:14:19.313862 [debug] [ThreadPool]: SQL status: OK in 0.005 seconds
[0m15:14:19.313862 [debug] [ThreadPool]: SQL status: OK in 0.004 seconds
[0m15:14:19.316792 [debug] [ThreadPool]: Using duckdb connection "create_duckdb_main_bronze"
[0m15:14:19.320519 [debug] [ThreadPool]: Using duckdb connection "create_duckdb_main_silver"
[0m15:14:19.320519 [debug] [ThreadPool]: Using duckdb connection "create_duckdb_main_gold"
[0m15:14:19.323803 [debug] [ThreadPool]: On create_duckdb_main_bronze: BEGIN
[0m15:14:19.323803 [debug] [ThreadPool]: On create_duckdb_main_silver: BEGIN
[0m15:14:19.323803 [debug] [ThreadPool]: On create_duckdb_main_gold: BEGIN
[0m15:14:19.323803 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:14:19.323803 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m15:14:19.328280 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:14:19.328280 [debug] [ThreadPool]: Using duckdb connection "create_duckdb_main_bronze"
[0m15:14:19.329299 [debug] [ThreadPool]: Using duckdb connection "create_duckdb_main_silver"
[0m15:14:19.330429 [debug] [ThreadPool]: Using duckdb connection "create_duckdb_main_gold"
[0m15:14:19.330429 [debug] [ThreadPool]: On create_duckdb_main_bronze: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_duckdb_main_bronze"} */

    
    
        create schema if not exists "duckdb"."main_bronze"
    
[0m15:14:19.332613 [debug] [ThreadPool]: On create_duckdb_main_silver: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_duckdb_main_silver"} */

    
    
        create schema if not exists "duckdb"."main_silver"
    
[0m15:14:19.333343 [debug] [ThreadPool]: On create_duckdb_main_gold: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_duckdb_main_gold"} */

    
    
        create schema if not exists "duckdb"."main_gold"
    
[0m15:14:19.333343 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m15:14:19.336531 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m15:14:19.339061 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m15:14:19.340810 [debug] [ThreadPool]: On create_duckdb_main_bronze: COMMIT
[0m15:14:19.341559 [debug] [ThreadPool]: On create_duckdb_main_silver: COMMIT
[0m15:14:19.341559 [debug] [ThreadPool]: On create_duckdb_main_gold: COMMIT
[0m15:14:19.341559 [debug] [ThreadPool]: Using duckdb connection "create_duckdb_main_bronze"
[0m15:14:19.341559 [debug] [ThreadPool]: Using duckdb connection "create_duckdb_main_silver"
[0m15:14:19.341559 [debug] [ThreadPool]: Using duckdb connection "create_duckdb_main_gold"
[0m15:14:19.341559 [debug] [ThreadPool]: On create_duckdb_main_bronze: COMMIT
[0m15:14:19.348603 [debug] [ThreadPool]: On create_duckdb_main_silver: COMMIT
[0m15:14:19.349126 [debug] [ThreadPool]: On create_duckdb_main_gold: COMMIT
[0m15:14:19.350175 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:14:19.353282 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m15:14:19.353282 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:14:19.353282 [debug] [ThreadPool]: On create_duckdb_main_bronze: Close
[0m15:14:19.353282 [debug] [ThreadPool]: On create_duckdb_main_silver: Close
[0m15:14:19.353282 [debug] [ThreadPool]: On create_duckdb_main_gold: Close
[0m15:14:19.359422 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_duckdb_main_silver'
[0m15:14:19.370440 [debug] [ThreadPool]: Using duckdb connection "list_duckdb_main_silver"
[0m15:14:19.372732 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_duckdb_main_bronze'
[0m15:14:19.372732 [debug] [ThreadPool]: On list_duckdb_main_silver: BEGIN
[0m15:14:19.372732 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_duckdb_main_gold'
[0m15:14:19.379474 [debug] [ThreadPool]: Using duckdb connection "list_duckdb_main_bronze"
[0m15:14:19.379474 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:14:19.379474 [debug] [ThreadPool]: Using duckdb connection "list_duckdb_main_gold"
[0m15:14:19.379474 [debug] [ThreadPool]: On list_duckdb_main_bronze: BEGIN
[0m15:14:19.386106 [debug] [ThreadPool]: On list_duckdb_main_gold: BEGIN
[0m15:14:19.387005 [debug] [ThreadPool]: SQL status: OK in 0.008 seconds
[0m15:14:19.388514 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:14:19.389544 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:14:19.390595 [debug] [ThreadPool]: Using duckdb connection "list_duckdb_main_silver"
[0m15:14:19.390595 [debug] [ThreadPool]: SQL status: OK in 0.003 seconds
[0m15:14:19.390595 [debug] [ThreadPool]: SQL status: OK in 0.003 seconds
[0m15:14:19.390595 [debug] [ThreadPool]: On list_duckdb_main_silver: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_duckdb_main_silver"} */
select
      'duckdb' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main_silver'
    and lower(table_catalog) = 'duckdb'
  
[0m15:14:19.390595 [debug] [ThreadPool]: Using duckdb connection "list_duckdb_main_bronze"
[0m15:14:19.395505 [debug] [ThreadPool]: Using duckdb connection "list_duckdb_main_gold"
[0m15:14:19.395505 [debug] [ThreadPool]: On list_duckdb_main_bronze: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_duckdb_main_bronze"} */
select
      'duckdb' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main_bronze'
    and lower(table_catalog) = 'duckdb'
  
[0m15:14:19.395505 [debug] [ThreadPool]: On list_duckdb_main_gold: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_duckdb_main_gold"} */
select
      'duckdb' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main_gold'
    and lower(table_catalog) = 'duckdb'
  
[0m15:14:19.421235 [debug] [ThreadPool]: SQL status: OK in 0.026 seconds
[0m15:14:19.421235 [debug] [ThreadPool]: SQL status: OK in 0.029 seconds
[0m15:14:19.421235 [debug] [ThreadPool]: SQL status: OK in 0.028 seconds
[0m15:14:19.429560 [debug] [ThreadPool]: On list_duckdb_main_gold: ROLLBACK
[0m15:14:19.430614 [debug] [ThreadPool]: On list_duckdb_main_silver: ROLLBACK
[0m15:14:19.430614 [debug] [ThreadPool]: On list_duckdb_main_bronze: ROLLBACK
[0m15:14:19.434956 [debug] [ThreadPool]: Failed to rollback 'list_duckdb_main_gold'
[0m15:14:19.438479 [debug] [ThreadPool]: Failed to rollback 'list_duckdb_main_silver'
[0m15:14:19.439560 [debug] [ThreadPool]: Failed to rollback 'list_duckdb_main_bronze'
[0m15:14:19.439560 [debug] [ThreadPool]: On list_duckdb_main_gold: Close
[0m15:14:19.439560 [debug] [ThreadPool]: On list_duckdb_main_silver: Close
[0m15:14:19.439560 [debug] [ThreadPool]: On list_duckdb_main_bronze: Close
[0m15:14:19.439560 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ab1f0716-63a5-45ca-8dd4-d9fc9cac367f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FD3C978CD0>]}
[0m15:14:19.448350 [debug] [MainThread]: Using duckdb connection "master"
[0m15:14:19.448350 [debug] [MainThread]: On master: BEGIN
[0m15:14:19.449372 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:14:19.451219 [debug] [MainThread]: SQL status: OK in 0.002 seconds
[0m15:14:19.454396 [debug] [MainThread]: On master: COMMIT
[0m15:14:19.454396 [debug] [MainThread]: Using duckdb connection "master"
[0m15:14:19.454396 [debug] [MainThread]: On master: COMMIT
[0m15:14:19.454396 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:14:19.458423 [debug] [MainThread]: On master: Close
[0m15:14:19.459531 [debug] [Thread-1  ]: Began running node model.job_intelligent.stg_jobs_raw
[0m15:14:19.459531 [info ] [Thread-1  ]: 1 of 13 START sql view model main_bronze.stg_jobs_raw .......................... [RUN]
[0m15:14:19.467118 [debug] [Thread-1  ]: Acquiring new duckdb connection 'model.job_intelligent.stg_jobs_raw'
[0m15:14:19.469158 [debug] [Thread-1  ]: Began compiling node model.job_intelligent.stg_jobs_raw
[0m15:14:19.485960 [debug] [Thread-1  ]: Writing injected SQL for node "model.job_intelligent.stg_jobs_raw"
[0m15:14:19.488744 [debug] [Thread-1  ]: Began executing node model.job_intelligent.stg_jobs_raw
[0m15:14:19.536301 [debug] [Thread-1  ]: Writing runtime sql for node "model.job_intelligent.stg_jobs_raw"
[0m15:14:19.539393 [debug] [Thread-1  ]: Using duckdb connection "model.job_intelligent.stg_jobs_raw"
[0m15:14:19.539393 [debug] [Thread-1  ]: On model.job_intelligent.stg_jobs_raw: BEGIN
[0m15:14:19.539393 [debug] [Thread-1  ]: Opening a new connection, currently in state init
[0m15:14:19.539393 [debug] [Thread-1  ]: SQL status: OK in 0.001 seconds
[0m15:14:19.539393 [debug] [Thread-1  ]: Using duckdb connection "model.job_intelligent.stg_jobs_raw"
[0m15:14:19.544323 [debug] [Thread-1  ]: On model.job_intelligent.stg_jobs_raw: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.stg_jobs_raw"} */

  
  create view "duckdb"."main_bronze"."stg_jobs_raw__dbt_tmp" as (
    -- models/bronze/stg_jobs_raw.sql
-- Bronze layer: Lecture directe des données brutes depuis final_data.csv
-- Renommage et typage minimal



-- Read directly from CSV file
SELECT
    -- Renommer les colonnes pour cohérence
    title as job_title,
    location,
    postedTime as posted_time,
    publishedAt as published_at,
    jobUrl as job_url,
    companyName as company_name,
    companyUrl as company_url,
    description as job_description,
    contractType as contract_type,
    workType as work_type,
    
    -- Métadonnées de traçabilité
    NOW() as ingestion_timestamp,
    '2026-01-08 14:14:16.855467+00:00' as dbt_run_id

FROM read_csv_auto('D:/lab2/final_data.csv')

WHERE 1=1
    -- Filtrer les lignes vides
    AND title IS NOT NULL
    AND description IS NOT NULL
  );

[0m15:14:19.546334 [debug] [Thread-1  ]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.stg_jobs_raw"} */

  
  create view "duckdb"."main_bronze"."stg_jobs_raw__dbt_tmp" as (
    -- models/bronze/stg_jobs_raw.sql
-- Bronze layer: Lecture directe des données brutes depuis final_data.csv
-- Renommage et typage minimal



-- Read directly from CSV file
SELECT
    -- Renommer les colonnes pour cohérence
    title as job_title,
    location,
    postedTime as posted_time,
    publishedAt as published_at,
    jobUrl as job_url,
    companyName as company_name,
    companyUrl as company_url,
    description as job_description,
    contractType as contract_type,
    workType as work_type,
    
    -- Métadonnées de traçabilité
    NOW() as ingestion_timestamp,
    '2026-01-08 14:14:16.855467+00:00' as dbt_run_id

FROM read_csv_auto('D:/lab2/final_data.csv')

WHERE 1=1
    -- Filtrer les lignes vides
    AND title IS NOT NULL
    AND description IS NOT NULL
  );

[0m15:14:19.546334 [debug] [Thread-1  ]: DuckDB adapter: Rolling back transaction.
[0m15:14:19.548346 [debug] [Thread-1  ]: On model.job_intelligent.stg_jobs_raw: ROLLBACK
[0m15:14:19.554742 [debug] [Thread-1  ]: Failed to rollback 'model.job_intelligent.stg_jobs_raw'
[0m15:14:19.558268 [debug] [Thread-1  ]: On model.job_intelligent.stg_jobs_raw: Close
[0m15:14:19.559346 [debug] [Thread-1  ]: Runtime Error in model stg_jobs_raw (models\bronze\stg_jobs_raw.sql)
  IO Error: No files found that match the pattern "D:/lab2/final_data.csv"
  
  LINE 29: FROM read_csv_auto('D:/lab2/final_data.csv')
                ^
[0m15:14:19.559346 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ab1f0716-63a5-45ca-8dd4-d9fc9cac367f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FD3A4C2AC0>]}
[0m15:14:19.559346 [error] [Thread-1  ]: 1 of 13 ERROR creating sql view model main_bronze.stg_jobs_raw ................. [[31mERROR[0m in 0.09s]
[0m15:14:19.567453 [debug] [Thread-1  ]: Finished running node model.job_intelligent.stg_jobs_raw
[0m15:14:19.569424 [debug] [Thread-7  ]: Marking all children of 'model.job_intelligent.stg_jobs_raw' to be skipped because of status 'error'.  Reason: Runtime Error in model stg_jobs_raw (models\bronze\stg_jobs_raw.sql)
  IO Error: No files found that match the pattern "D:/lab2/final_data.csv"
  
  LINE 29: FROM read_csv_auto('D:/lab2/final_data.csv')
                ^.
[0m15:14:19.570585 [debug] [Thread-3  ]: Began running node model.job_intelligent.int_jobs_cleaned
[0m15:14:19.570585 [info ] [Thread-3  ]: 2 of 13 SKIP relation main_silver.int_jobs_cleaned ............................. [[33mSKIP[0m]
[0m15:14:19.574671 [debug] [Thread-3  ]: Finished running node model.job_intelligent.int_jobs_cleaned
[0m15:14:19.574671 [debug] [Thread-2  ]: Began running node model.job_intelligent.int_job_title_normalization
[0m15:14:19.574671 [info ] [Thread-2  ]: 3 of 13 SKIP relation main_silver.int_job_title_normalization .................. [[33mSKIP[0m]
[0m15:14:19.579233 [debug] [Thread-2  ]: Finished running node model.job_intelligent.int_job_title_normalization
[0m15:14:19.579233 [debug] [Thread-4  ]: Began running node model.job_intelligent.dim_company
[0m15:14:19.579233 [debug] [Thread-3  ]: Began running node model.job_intelligent.dim_location
[0m15:14:19.579233 [debug] [Thread-1  ]: Began running node model.job_intelligent.dim_time
[0m15:14:19.582922 [debug] [Thread-2  ]: Began running node model.job_intelligent.int_skills_extraction
[0m15:14:19.582922 [info ] [Thread-4  ]: 4 of 13 SKIP relation main_gold.dim_company .................................... [[33mSKIP[0m]
[0m15:14:19.586232 [info ] [Thread-3  ]: 5 of 13 SKIP relation main_gold.dim_location ................................... [[33mSKIP[0m]
[0m15:14:19.590431 [info ] [Thread-1  ]: 6 of 13 SKIP relation main_gold.dim_time ....................................... [[33mSKIP[0m]
[0m15:14:19.590431 [info ] [Thread-2  ]: 7 of 13 SKIP relation main_silver.int_skills_extraction ........................ [[33mSKIP[0m]
[0m15:14:19.593550 [debug] [Thread-4  ]: Finished running node model.job_intelligent.dim_company
[0m15:14:19.595565 [debug] [Thread-3  ]: Finished running node model.job_intelligent.dim_location
[0m15:14:19.598335 [debug] [Thread-1  ]: Finished running node model.job_intelligent.dim_time
[0m15:14:19.599432 [debug] [Thread-2  ]: Finished running node model.job_intelligent.int_skills_extraction
[0m15:14:19.602296 [debug] [Thread-4  ]: Began running node model.job_intelligent.fact_job_offers
[0m15:14:19.604326 [debug] [Thread-1  ]: Began running node model.job_intelligent.dim_skills
[0m15:14:19.606682 [info ] [Thread-4  ]: 8 of 13 SKIP relation main_gold.fact_job_offers ................................ [[33mSKIP[0m]
[0m15:14:19.609228 [info ] [Thread-1  ]: 9 of 13 SKIP relation main_gold.dim_skills ..................................... [[33mSKIP[0m]
[0m15:14:19.610338 [debug] [Thread-4  ]: Finished running node model.job_intelligent.fact_job_offers
[0m15:14:19.610338 [debug] [Thread-1  ]: Finished running node model.job_intelligent.dim_skills
[0m15:14:19.610338 [debug] [Thread-2  ]: Began running node model.job_intelligent.agg_job_offers_by_category_time
[0m15:14:19.610338 [debug] [Thread-3  ]: Began running node model.job_intelligent.agg_location_analysis
[0m15:14:19.615507 [debug] [Thread-4  ]: Began running node model.job_intelligent.fact_job_skills
[0m15:14:19.616934 [info ] [Thread-2  ]: 10 of 13 SKIP relation main_gold.agg_job_offers_by_category_time ............... [[33mSKIP[0m]
[0m15:14:19.618445 [info ] [Thread-3  ]: 11 of 13 SKIP relation main_gold.agg_location_analysis ......................... [[33mSKIP[0m]
[0m15:14:19.623281 [debug] [Thread-2  ]: Finished running node model.job_intelligent.agg_job_offers_by_category_time
[0m15:14:19.621527 [info ] [Thread-4  ]: 12 of 13 SKIP relation main_gold.fact_job_skills ............................... [[33mSKIP[0m]
[0m15:14:19.627012 [debug] [Thread-3  ]: Finished running node model.job_intelligent.agg_location_analysis
[0m15:14:19.628532 [debug] [Thread-4  ]: Finished running node model.job_intelligent.fact_job_skills
[0m15:14:19.630587 [debug] [Thread-1  ]: Began running node model.job_intelligent.agg_skills_demand
[0m15:14:19.631621 [info ] [Thread-1  ]: 13 of 13 SKIP relation main_gold.agg_skills_demand ............................. [[33mSKIP[0m]
[0m15:14:19.631621 [debug] [Thread-1  ]: Finished running node model.job_intelligent.agg_skills_demand
[0m15:14:19.633394 [debug] [MainThread]: Using duckdb connection "master"
[0m15:14:19.636532 [debug] [MainThread]: On master: BEGIN
[0m15:14:19.639456 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m15:14:19.641841 [debug] [MainThread]: SQL status: OK in 0.003 seconds
[0m15:14:19.641841 [debug] [MainThread]: On master: COMMIT
[0m15:14:19.643852 [debug] [MainThread]: Using duckdb connection "master"
[0m15:14:19.643852 [debug] [MainThread]: On master: COMMIT
[0m15:14:19.643852 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:14:19.645861 [debug] [MainThread]: On master: Close
[0m15:14:19.645861 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:14:19.645861 [debug] [MainThread]: Connection 'create_duckdb_main_silver' was properly closed.
[0m15:14:19.647879 [debug] [MainThread]: Connection 'create_duckdb_main_bronze' was properly closed.
[0m15:14:19.648386 [debug] [MainThread]: Connection 'create_duckdb_main_gold' was properly closed.
[0m15:14:19.648386 [debug] [MainThread]: Connection 'list_duckdb_main_silver' was properly closed.
[0m15:14:19.649406 [debug] [MainThread]: Connection 'list_duckdb_main_bronze' was properly closed.
[0m15:14:19.649406 [debug] [MainThread]: Connection 'list_duckdb_main_gold' was properly closed.
[0m15:14:19.650549 [debug] [MainThread]: Connection 'model.job_intelligent.stg_jobs_raw' was properly closed.
[0m15:14:19.651664 [info ] [MainThread]: 
[0m15:14:19.655523 [info ] [MainThread]: Finished running 12 table models, 1 view model in 0 hours 0 minutes and 0.68 seconds (0.68s).
[0m15:14:19.658378 [debug] [MainThread]: Command end result
[0m15:14:19.690647 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Ayoub Gorry\Desktop\powerbi\jobs-power-bi\dbt_project\target\manifest.json
[0m15:14:19.690647 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Ayoub Gorry\Desktop\powerbi\jobs-power-bi\dbt_project\target\semantic_manifest.json
[0m15:14:19.708305 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\Ayoub Gorry\Desktop\powerbi\jobs-power-bi\dbt_project\target\run_results.json
[0m15:14:19.709326 [info ] [MainThread]: 
[0m15:14:19.710544 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m15:14:19.712470 [info ] [MainThread]: 
[0m15:14:19.712470 [error] [MainThread]: [31mFailure in model stg_jobs_raw (models\bronze\stg_jobs_raw.sql)[0m
[0m15:14:19.714488 [error] [MainThread]:   Runtime Error in model stg_jobs_raw (models\bronze\stg_jobs_raw.sql)
  IO Error: No files found that match the pattern "D:/lab2/final_data.csv"
  
  LINE 29: FROM read_csv_auto('D:/lab2/final_data.csv')
                ^
[0m15:14:19.714488 [info ] [MainThread]: 
[0m15:14:19.718647 [info ] [MainThread]:   compiled code at target\compiled\job_intelligent\models\bronze\stg_jobs_raw.sql
[0m15:14:19.720950 [info ] [MainThread]: 
[0m15:14:19.723277 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=12 NO-OP=0 TOTAL=13
[0m15:14:19.724932 [debug] [MainThread]: Command `dbt run` failed at 15:14:19.724932 after 3.02 seconds
[0m15:14:19.724932 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FD3B5C10A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FD3C98B5E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FD3DEEB880>]}
[0m15:14:19.724932 [debug] [MainThread]: Flushing usage events
[0m15:14:20.550628 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:17:23.981181 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000283C6D71100>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000283C80783A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000283C80781F0>]}


============================== 15:17:23.985856 | bc798df8-6777-422a-8102-4de2b0ddb294 ==============================
[0m15:17:23.985856 [info ] [MainThread]: Running with dbt=1.10.18
[0m15:17:23.989631 [debug] [MainThread]: running dbt with arguments {'log_path': 'C:\\Users\\Ayoub Gorry\\Desktop\\powerbi\\jobs-power-bi\\dbt_project\\logs', 'log_format': 'default', 'partial_parse': 'True', 'version_check': 'True', 'empty': 'False', 'log_cache_events': 'False', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Ayoub Gorry\\Desktop\\powerbi\\jobs-power-bi\\dbt_project', 'target_path': 'None', 'send_anonymous_usage_stats': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'introspect': 'True', 'invocation_command': 'dbt run', 'use_experimental_parser': 'False', 'no_print': 'None', 'debug': 'False', 'quiet': 'False', 'use_colors': 'True', 'static_parser': 'True', 'warn_error': 'None', 'fail_fast': 'False', 'write_json': 'True', 'indirect_selection': 'eager'}
[0m15:17:24.569112 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'bc798df8-6777-422a-8102-4de2b0ddb294', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000283C7143550>]}
[0m15:17:24.700717 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'bc798df8-6777-422a-8102-4de2b0ddb294', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000283C7777280>]}
[0m15:17:24.709230 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m15:17:25.178526 [debug] [MainThread]: checksum: 9646db7d1a5f5b9389d9c545d3ebf898b26bc1f63f6b5366f34a6af01d52cb91, vars: {}, profile: , target: , version: 1.10.18
[0m15:17:25.509459 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m15:17:25.512637 [debug] [MainThread]: Partial parsing: updated file: job_intelligent://models\bronze\stg_jobs_raw.sql
[0m15:17:26.059616 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- seeds
[0m15:17:26.069571 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'bc798df8-6777-422a-8102-4de2b0ddb294', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000283C98439A0>]}
[0m15:17:26.182107 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Ayoub Gorry\Desktop\powerbi\jobs-power-bi\dbt_project\target\manifest.json
[0m15:17:26.188517 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Ayoub Gorry\Desktop\powerbi\jobs-power-bi\dbt_project\target\semantic_manifest.json
[0m15:17:26.239068 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'bc798df8-6777-422a-8102-4de2b0ddb294', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000283C9854370>]}
[0m15:17:26.239068 [info ] [MainThread]: Found 13 models, 456 macros
[0m15:17:26.239068 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'bc798df8-6777-422a-8102-4de2b0ddb294', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000283C81A0D90>]}
[0m15:17:26.250794 [info ] [MainThread]: 
[0m15:17:26.251821 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:17:26.252980 [info ] [MainThread]: 
[0m15:17:26.252980 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m15:17:26.270696 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_duckdb'
[0m15:17:26.275928 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_duckdb'
[0m15:17:26.299383 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_duckdb'
[0m15:17:26.390425 [debug] [ThreadPool]: Using duckdb connection "list_duckdb"
[0m15:17:26.390425 [debug] [ThreadPool]: Using duckdb connection "list_duckdb"
[0m15:17:26.390425 [debug] [ThreadPool]: Using duckdb connection "list_duckdb"
[0m15:17:26.399161 [debug] [ThreadPool]: On list_duckdb: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_duckdb"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"duckdb"'
    
  
  
[0m15:17:26.399161 [debug] [ThreadPool]: On list_duckdb: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_duckdb"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"duckdb"'
    
  
  
[0m15:17:26.400686 [debug] [ThreadPool]: On list_duckdb: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_duckdb"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"duckdb"'
    
  
  
[0m15:17:26.400686 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:17:26.400686 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:17:26.400686 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:17:26.432489 [debug] [ThreadPool]: SQL status: OK in 0.032 seconds
[0m15:17:26.432489 [debug] [ThreadPool]: SQL status: OK in 0.033 seconds
[0m15:17:26.432489 [debug] [ThreadPool]: SQL status: OK in 0.035 seconds
[0m15:17:26.438531 [debug] [ThreadPool]: On list_duckdb: Close
[0m15:17:26.439054 [debug] [ThreadPool]: On list_duckdb: Close
[0m15:17:26.439054 [debug] [ThreadPool]: On list_duckdb: Close
[0m15:17:26.445209 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_duckdb, now create_duckdb_main_silver)
[0m15:17:26.445209 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_duckdb, now create_duckdb_main_bronze)
[0m15:17:26.445209 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_duckdb, now create_duckdb_main_gold)
[0m15:17:26.445209 [debug] [ThreadPool]: Creating schema "database: "duckdb"
schema: "main_silver"
"
[0m15:17:26.448459 [debug] [ThreadPool]: Creating schema "database: "duckdb"
schema: "main_bronze"
"
[0m15:17:26.450833 [debug] [ThreadPool]: Creating schema "database: "duckdb"
schema: "main_gold"
"
[0m15:17:26.461037 [debug] [ThreadPool]: Using duckdb connection "create_duckdb_main_silver"
[0m15:17:26.464882 [debug] [ThreadPool]: Using duckdb connection "create_duckdb_main_bronze"
[0m15:17:26.470636 [debug] [ThreadPool]: Using duckdb connection "create_duckdb_main_gold"
[0m15:17:26.470636 [debug] [ThreadPool]: On create_duckdb_main_silver: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_duckdb_main_silver"} */

    
        select type from duckdb_databases()
        where lower(database_name)='duckdb'
        and type='sqlite'
    
  
[0m15:17:26.470636 [debug] [ThreadPool]: On create_duckdb_main_bronze: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_duckdb_main_bronze"} */

    
        select type from duckdb_databases()
        where lower(database_name)='duckdb'
        and type='sqlite'
    
  
[0m15:17:26.472721 [debug] [ThreadPool]: On create_duckdb_main_gold: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_duckdb_main_gold"} */

    
        select type from duckdb_databases()
        where lower(database_name)='duckdb'
        and type='sqlite'
    
  
[0m15:17:26.473334 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:17:26.474177 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:17:26.474177 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:17:26.477675 [debug] [ThreadPool]: SQL status: OK in 0.005 seconds
[0m15:17:26.479296 [debug] [ThreadPool]: SQL status: OK in 0.005 seconds
[0m15:17:26.479296 [debug] [ThreadPool]: SQL status: OK in 0.005 seconds
[0m15:17:26.481045 [debug] [ThreadPool]: Using duckdb connection "create_duckdb_main_silver"
[0m15:17:26.483058 [debug] [ThreadPool]: Using duckdb connection "create_duckdb_main_bronze"
[0m15:17:26.483058 [debug] [ThreadPool]: Using duckdb connection "create_duckdb_main_gold"
[0m15:17:26.488551 [debug] [ThreadPool]: On create_duckdb_main_silver: BEGIN
[0m15:17:26.489183 [debug] [ThreadPool]: On create_duckdb_main_bronze: BEGIN
[0m15:17:26.490220 [debug] [ThreadPool]: On create_duckdb_main_gold: BEGIN
[0m15:17:26.490220 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m15:17:26.490220 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m15:17:26.490220 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:17:26.494026 [debug] [ThreadPool]: Using duckdb connection "create_duckdb_main_silver"
[0m15:17:26.494026 [debug] [ThreadPool]: Using duckdb connection "create_duckdb_main_bronze"
[0m15:17:26.494026 [debug] [ThreadPool]: Using duckdb connection "create_duckdb_main_gold"
[0m15:17:26.496682 [debug] [ThreadPool]: On create_duckdb_main_silver: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_duckdb_main_silver"} */

    
    
        create schema if not exists "duckdb"."main_silver"
    
[0m15:17:26.496682 [debug] [ThreadPool]: On create_duckdb_main_bronze: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_duckdb_main_bronze"} */

    
    
        create schema if not exists "duckdb"."main_bronze"
    
[0m15:17:26.498688 [debug] [ThreadPool]: On create_duckdb_main_gold: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_duckdb_main_gold"} */

    
    
        create schema if not exists "duckdb"."main_gold"
    
[0m15:17:26.499255 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m15:17:26.499255 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:17:26.499255 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:17:26.502355 [debug] [ThreadPool]: On create_duckdb_main_silver: COMMIT
[0m15:17:26.503450 [debug] [ThreadPool]: On create_duckdb_main_bronze: COMMIT
[0m15:17:26.504808 [debug] [ThreadPool]: On create_duckdb_main_gold: COMMIT
[0m15:17:26.506557 [debug] [ThreadPool]: Using duckdb connection "create_duckdb_main_silver"
[0m15:17:26.506557 [debug] [ThreadPool]: Using duckdb connection "create_duckdb_main_bronze"
[0m15:17:26.508567 [debug] [ThreadPool]: Using duckdb connection "create_duckdb_main_gold"
[0m15:17:26.509093 [debug] [ThreadPool]: On create_duckdb_main_silver: COMMIT
[0m15:17:26.510169 [debug] [ThreadPool]: On create_duckdb_main_bronze: COMMIT
[0m15:17:26.512197 [debug] [ThreadPool]: On create_duckdb_main_gold: COMMIT
[0m15:17:26.513936 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m15:17:26.513936 [debug] [ThreadPool]: SQL status: OK in 0.002 seconds
[0m15:17:26.513936 [debug] [ThreadPool]: SQL status: OK in 0.002 seconds
[0m15:17:26.513936 [debug] [ThreadPool]: On create_duckdb_main_bronze: Close
[0m15:17:26.513936 [debug] [ThreadPool]: On create_duckdb_main_silver: Close
[0m15:17:26.513936 [debug] [ThreadPool]: On create_duckdb_main_gold: Close
[0m15:17:26.519516 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_duckdb_main_silver'
[0m15:17:26.519516 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_duckdb_main_bronze'
[0m15:17:26.530382 [debug] [ThreadPool]: Using duckdb connection "list_duckdb_main_silver"
[0m15:17:26.533692 [debug] [ThreadPool]: Using duckdb connection "list_duckdb_main_bronze"
[0m15:17:26.534902 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_duckdb_main_gold'
[0m15:17:26.535623 [debug] [ThreadPool]: On list_duckdb_main_silver: BEGIN
[0m15:17:26.536625 [debug] [ThreadPool]: On list_duckdb_main_bronze: BEGIN
[0m15:17:26.539157 [debug] [ThreadPool]: Using duckdb connection "list_duckdb_main_gold"
[0m15:17:26.539157 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:17:26.539157 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:17:26.543900 [debug] [ThreadPool]: On list_duckdb_main_gold: BEGIN
[0m15:17:26.545132 [debug] [ThreadPool]: SQL status: OK in 0.004 seconds
[0m15:17:26.545132 [debug] [ThreadPool]: SQL status: OK in 0.004 seconds
[0m15:17:26.545132 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:17:26.545132 [debug] [ThreadPool]: Using duckdb connection "list_duckdb_main_silver"
[0m15:17:26.549797 [debug] [ThreadPool]: Using duckdb connection "list_duckdb_main_bronze"
[0m15:17:26.550441 [debug] [ThreadPool]: On list_duckdb_main_silver: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_duckdb_main_silver"} */
select
      'duckdb' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main_silver'
    and lower(table_catalog) = 'duckdb'
  
[0m15:17:26.550441 [debug] [ThreadPool]: SQL status: OK in 0.004 seconds
[0m15:17:26.550441 [debug] [ThreadPool]: On list_duckdb_main_bronze: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_duckdb_main_bronze"} */
select
      'duckdb' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main_bronze'
    and lower(table_catalog) = 'duckdb'
  
[0m15:17:26.553163 [debug] [ThreadPool]: Using duckdb connection "list_duckdb_main_gold"
[0m15:17:26.555175 [debug] [ThreadPool]: On list_duckdb_main_gold: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_duckdb_main_gold"} */
select
      'duckdb' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main_gold'
    and lower(table_catalog) = 'duckdb'
  
[0m15:17:26.573567 [debug] [ThreadPool]: SQL status: OK in 0.019 seconds
[0m15:17:26.573567 [debug] [ThreadPool]: SQL status: OK in 0.021 seconds
[0m15:17:26.573567 [debug] [ThreadPool]: SQL status: OK in 0.023 seconds
[0m15:17:26.579611 [debug] [ThreadPool]: On list_duckdb_main_gold: ROLLBACK
[0m15:17:26.579611 [debug] [ThreadPool]: On list_duckdb_main_bronze: ROLLBACK
[0m15:17:26.579611 [debug] [ThreadPool]: On list_duckdb_main_silver: ROLLBACK
[0m15:17:26.584367 [debug] [ThreadPool]: Failed to rollback 'list_duckdb_main_gold'
[0m15:17:26.584367 [debug] [ThreadPool]: Failed to rollback 'list_duckdb_main_bronze'
[0m15:17:26.584367 [debug] [ThreadPool]: Failed to rollback 'list_duckdb_main_silver'
[0m15:17:26.588261 [debug] [ThreadPool]: On list_duckdb_main_gold: Close
[0m15:17:26.588261 [debug] [ThreadPool]: On list_duckdb_main_bronze: Close
[0m15:17:26.589283 [debug] [ThreadPool]: On list_duckdb_main_silver: Close
[0m15:17:26.590271 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'bc798df8-6777-422a-8102-4de2b0ddb294', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000283C98A90D0>]}
[0m15:17:26.590271 [debug] [MainThread]: Using duckdb connection "master"
[0m15:17:26.590271 [debug] [MainThread]: On master: BEGIN
[0m15:17:26.590271 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:17:26.590271 [debug] [MainThread]: SQL status: OK in 0.001 seconds
[0m15:17:26.598520 [debug] [MainThread]: On master: COMMIT
[0m15:17:26.599045 [debug] [MainThread]: Using duckdb connection "master"
[0m15:17:26.599045 [debug] [MainThread]: On master: COMMIT
[0m15:17:26.600062 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:17:26.600062 [debug] [MainThread]: On master: Close
[0m15:17:26.608466 [debug] [Thread-1  ]: Began running node model.job_intelligent.stg_jobs_raw
[0m15:17:26.609516 [info ] [Thread-1  ]: 1 of 13 START sql view model main_bronze.stg_jobs_raw .......................... [RUN]
[0m15:17:26.610546 [debug] [Thread-1  ]: Acquiring new duckdb connection 'model.job_intelligent.stg_jobs_raw'
[0m15:17:26.610546 [debug] [Thread-1  ]: Began compiling node model.job_intelligent.stg_jobs_raw
[0m15:17:26.622589 [debug] [Thread-1  ]: Writing injected SQL for node "model.job_intelligent.stg_jobs_raw"
[0m15:17:26.630380 [debug] [Thread-1  ]: Began executing node model.job_intelligent.stg_jobs_raw
[0m15:17:26.670729 [debug] [Thread-1  ]: Writing runtime sql for node "model.job_intelligent.stg_jobs_raw"
[0m15:17:26.676278 [debug] [Thread-1  ]: Using duckdb connection "model.job_intelligent.stg_jobs_raw"
[0m15:17:26.678290 [debug] [Thread-1  ]: On model.job_intelligent.stg_jobs_raw: BEGIN
[0m15:17:26.679545 [debug] [Thread-1  ]: Opening a new connection, currently in state init
[0m15:17:26.679545 [debug] [Thread-1  ]: SQL status: OK in 0.002 seconds
[0m15:17:26.679545 [debug] [Thread-1  ]: Using duckdb connection "model.job_intelligent.stg_jobs_raw"
[0m15:17:26.679545 [debug] [Thread-1  ]: On model.job_intelligent.stg_jobs_raw: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.stg_jobs_raw"} */

  
  create view "duckdb"."main_bronze"."stg_jobs_raw__dbt_tmp" as (
    -- models/bronze/stg_jobs_raw.sql
-- Bronze layer: Lecture directe des données brutes depuis final_data.csv
-- Renommage et typage minimal



-- Read directly from CSV file
SELECT
    -- Renommer les colonnes pour cohérence
    title as job_title,
    location,
    postedTime as posted_time,
    publishedAt as published_at,
    jobUrl as job_url,
    companyName as company_name,
    companyUrl as company_url,
    description as job_description,
    contractType as contract_type,
    workType as work_type,
    
    -- Métadonnées de traçabilité
    NOW() as ingestion_timestamp,
    '2026-01-08 14:17:23.930382+00:00' as dbt_run_id

SELECT *
FROM read_csv_auto('../data/bronze/final_data.csv')

WHERE 1=1
    -- Filtrer les lignes vides
    AND title IS NOT NULL
    AND description IS NOT NULL
  );

[0m15:17:26.692449 [debug] [Thread-1  ]: DuckDB adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.stg_jobs_raw"} */

  
  create view "duckdb"."main_bronze"."stg_jobs_raw__dbt_tmp" as (
    -- models/bronze/stg_jobs_raw.sql
-- Bronze layer: Lecture directe des données brutes depuis final_data.csv
-- Renommage et typage minimal



-- Read directly from CSV file
SELECT
    -- Renommer les colonnes pour cohérence
    title as job_title,
    location,
    postedTime as posted_time,
    publishedAt as published_at,
    jobUrl as job_url,
    companyName as company_name,
    companyUrl as company_url,
    description as job_description,
    contractType as contract_type,
    workType as work_type,
    
    -- Métadonnées de traçabilité
    NOW() as ingestion_timestamp,
    '2026-01-08 14:17:23.930382+00:00' as dbt_run_id

SELECT *
FROM read_csv_auto('../data/bronze/final_data.csv')

WHERE 1=1
    -- Filtrer les lignes vides
    AND title IS NOT NULL
    AND description IS NOT NULL
  );

[0m15:17:26.696621 [debug] [Thread-1  ]: DuckDB adapter: Rolling back transaction.
[0m15:17:26.698634 [debug] [Thread-1  ]: On model.job_intelligent.stg_jobs_raw: ROLLBACK
[0m15:17:26.705219 [debug] [Thread-1  ]: Failed to rollback 'model.job_intelligent.stg_jobs_raw'
[0m15:17:26.705219 [debug] [Thread-1  ]: On model.job_intelligent.stg_jobs_raw: Close
[0m15:17:26.709330 [debug] [Thread-1  ]: Runtime Error in model stg_jobs_raw (models\bronze\stg_jobs_raw.sql)
  Parser Error: syntax error at or near "SELECT"
  
  LINE 27:     '2026-01-08 14:17:23.930382+00:00' as dbt_run_id
                                                           ^
[0m15:17:26.712702 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'bc798df8-6777-422a-8102-4de2b0ddb294', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000283CAE35CD0>]}
[0m15:17:26.712702 [error] [Thread-1  ]: 1 of 13 ERROR creating sql view model main_bronze.stg_jobs_raw ................. [[31mERROR[0m in 0.10s]
[0m15:17:26.718381 [debug] [Thread-1  ]: Finished running node model.job_intelligent.stg_jobs_raw
[0m15:17:26.719485 [debug] [Thread-7  ]: Marking all children of 'model.job_intelligent.stg_jobs_raw' to be skipped because of status 'error'.  Reason: Runtime Error in model stg_jobs_raw (models\bronze\stg_jobs_raw.sql)
  Parser Error: syntax error at or near "SELECT"
  
  LINE 27:     '2026-01-08 14:17:23.930382+00:00' as dbt_run_id
                                                           ^.
[0m15:17:26.719485 [debug] [Thread-3  ]: Began running node model.job_intelligent.int_jobs_cleaned
[0m15:17:26.719485 [info ] [Thread-3  ]: 2 of 13 SKIP relation main_silver.int_jobs_cleaned ............................. [[33mSKIP[0m]
[0m15:17:26.724567 [debug] [Thread-3  ]: Finished running node model.job_intelligent.int_jobs_cleaned
[0m15:17:26.727169 [debug] [Thread-2  ]: Began running node model.job_intelligent.int_job_title_normalization
[0m15:17:26.727169 [info ] [Thread-2  ]: 3 of 13 SKIP relation main_silver.int_job_title_normalization .................. [[33mSKIP[0m]
[0m15:17:26.730561 [debug] [Thread-2  ]: Finished running node model.job_intelligent.int_job_title_normalization
[0m15:17:26.732127 [debug] [Thread-4  ]: Began running node model.job_intelligent.dim_company
[0m15:17:26.735447 [debug] [Thread-3  ]: Began running node model.job_intelligent.dim_location
[0m15:17:26.735447 [debug] [Thread-1  ]: Began running node model.job_intelligent.dim_time
[0m15:17:26.735447 [debug] [Thread-2  ]: Began running node model.job_intelligent.int_skills_extraction
[0m15:17:26.735447 [info ] [Thread-4  ]: 4 of 13 SKIP relation main_gold.dim_company .................................... [[33mSKIP[0m]
[0m15:17:26.739072 [info ] [Thread-3  ]: 5 of 13 SKIP relation main_gold.dim_location ................................... [[33mSKIP[0m]
[0m15:17:26.740761 [info ] [Thread-1  ]: 6 of 13 SKIP relation main_gold.dim_time ....................................... [[33mSKIP[0m]
[0m15:17:26.740761 [info ] [Thread-2  ]: 7 of 13 SKIP relation main_silver.int_skills_extraction ........................ [[33mSKIP[0m]
[0m15:17:26.740761 [debug] [Thread-4  ]: Finished running node model.job_intelligent.dim_company
[0m15:17:26.744362 [debug] [Thread-3  ]: Finished running node model.job_intelligent.dim_location
[0m15:17:26.746378 [debug] [Thread-1  ]: Finished running node model.job_intelligent.dim_time
[0m15:17:26.749411 [debug] [Thread-2  ]: Finished running node model.job_intelligent.int_skills_extraction
[0m15:17:26.749939 [debug] [Thread-4  ]: Began running node model.job_intelligent.fact_job_offers
[0m15:17:26.749939 [debug] [Thread-1  ]: Began running node model.job_intelligent.dim_skills
[0m15:17:26.749939 [info ] [Thread-4  ]: 8 of 13 SKIP relation main_gold.fact_job_offers ................................ [[33mSKIP[0m]
[0m15:17:26.749939 [info ] [Thread-1  ]: 9 of 13 SKIP relation main_gold.dim_skills ..................................... [[33mSKIP[0m]
[0m15:17:26.756260 [debug] [Thread-4  ]: Finished running node model.job_intelligent.fact_job_offers
[0m15:17:26.756260 [debug] [Thread-1  ]: Finished running node model.job_intelligent.dim_skills
[0m15:17:26.759297 [debug] [Thread-2  ]: Began running node model.job_intelligent.agg_job_offers_by_category_time
[0m15:17:26.760971 [debug] [Thread-3  ]: Began running node model.job_intelligent.agg_location_analysis
[0m15:17:26.765037 [debug] [Thread-4  ]: Began running node model.job_intelligent.fact_job_skills
[0m15:17:26.766333 [info ] [Thread-2  ]: 10 of 13 SKIP relation main_gold.agg_job_offers_by_category_time ............... [[33mSKIP[0m]
[0m15:17:26.766333 [info ] [Thread-3  ]: 11 of 13 SKIP relation main_gold.agg_location_analysis ......................... [[33mSKIP[0m]
[0m15:17:26.768341 [info ] [Thread-4  ]: 12 of 13 SKIP relation main_gold.fact_job_skills ............................... [[33mSKIP[0m]
[0m15:17:26.769389 [debug] [Thread-2  ]: Finished running node model.job_intelligent.agg_job_offers_by_category_time
[0m15:17:26.769389 [debug] [Thread-3  ]: Finished running node model.job_intelligent.agg_location_analysis
[0m15:17:26.769389 [debug] [Thread-4  ]: Finished running node model.job_intelligent.fact_job_skills
[0m15:17:26.774034 [debug] [Thread-1  ]: Began running node model.job_intelligent.agg_skills_demand
[0m15:17:26.774034 [info ] [Thread-1  ]: 13 of 13 SKIP relation main_gold.agg_skills_demand ............................. [[33mSKIP[0m]
[0m15:17:26.775363 [debug] [Thread-1  ]: Finished running node model.job_intelligent.agg_skills_demand
[0m15:17:26.780997 [debug] [MainThread]: Using duckdb connection "master"
[0m15:17:26.780997 [debug] [MainThread]: On master: BEGIN
[0m15:17:26.780997 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m15:17:26.780997 [debug] [MainThread]: SQL status: OK in 0.001 seconds
[0m15:17:26.780997 [debug] [MainThread]: On master: COMMIT
[0m15:17:26.780997 [debug] [MainThread]: Using duckdb connection "master"
[0m15:17:26.780997 [debug] [MainThread]: On master: COMMIT
[0m15:17:26.780997 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:17:26.789232 [debug] [MainThread]: On master: Close
[0m15:17:26.789232 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:17:26.789232 [debug] [MainThread]: Connection 'create_duckdb_main_gold' was properly closed.
[0m15:17:26.789232 [debug] [MainThread]: Connection 'create_duckdb_main_bronze' was properly closed.
[0m15:17:26.789232 [debug] [MainThread]: Connection 'create_duckdb_main_silver' was properly closed.
[0m15:17:26.789232 [debug] [MainThread]: Connection 'list_duckdb_main_silver' was properly closed.
[0m15:17:26.789232 [debug] [MainThread]: Connection 'list_duckdb_main_bronze' was properly closed.
[0m15:17:26.796822 [debug] [MainThread]: Connection 'list_duckdb_main_gold' was properly closed.
[0m15:17:26.796822 [debug] [MainThread]: Connection 'model.job_intelligent.stg_jobs_raw' was properly closed.
[0m15:17:26.798468 [info ] [MainThread]: 
[0m15:17:26.799496 [info ] [MainThread]: Finished running 12 table models, 1 view model in 0 hours 0 minutes and 0.55 seconds (0.55s).
[0m15:17:26.800785 [debug] [MainThread]: Command end result
[0m15:17:26.831170 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Ayoub Gorry\Desktop\powerbi\jobs-power-bi\dbt_project\target\manifest.json
[0m15:17:26.840783 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Ayoub Gorry\Desktop\powerbi\jobs-power-bi\dbt_project\target\semantic_manifest.json
[0m15:17:26.854257 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\Ayoub Gorry\Desktop\powerbi\jobs-power-bi\dbt_project\target\run_results.json
[0m15:17:26.856271 [info ] [MainThread]: 
[0m15:17:26.856271 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m15:17:26.859343 [info ] [MainThread]: 
[0m15:17:26.861162 [error] [MainThread]: [31mFailure in model stg_jobs_raw (models\bronze\stg_jobs_raw.sql)[0m
[0m15:17:26.864034 [error] [MainThread]:   Runtime Error in model stg_jobs_raw (models\bronze\stg_jobs_raw.sql)
  Parser Error: syntax error at or near "SELECT"
  
  LINE 27:     '2026-01-08 14:17:23.930382+00:00' as dbt_run_id
                                                           ^
[0m15:17:26.866199 [info ] [MainThread]: 
[0m15:17:26.866199 [info ] [MainThread]:   compiled code at target\compiled\job_intelligent\models\bronze\stg_jobs_raw.sql
[0m15:17:26.869248 [info ] [MainThread]: 
[0m15:17:26.869248 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=12 NO-OP=0 TOTAL=13
[0m15:17:26.872079 [debug] [MainThread]: Command `dbt run` failed at 15:17:26.872079 after 3.03 seconds
[0m15:17:26.874091 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000283C6D71100>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000283C818BF10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000283C710F130>]}
[0m15:17:26.874571 [debug] [MainThread]: Flushing usage events
[0m15:17:27.799554 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:18:28.202204 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024BA5542070>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024BA6848280>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024BA68480D0>]}


============================== 15:18:28.212792 | 677da618-2f5c-4a4a-9ba5-8a53f5d835fb ==============================
[0m15:18:28.212792 [info ] [MainThread]: Running with dbt=1.10.18
[0m15:18:28.214304 [debug] [MainThread]: running dbt with arguments {'log_path': 'C:\\Users\\Ayoub Gorry\\Desktop\\powerbi\\jobs-power-bi\\dbt_project\\logs', 'log_format': 'default', 'partial_parse': 'True', 'version_check': 'True', 'empty': 'False', 'log_cache_events': 'False', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Ayoub Gorry\\Desktop\\powerbi\\jobs-power-bi\\dbt_project', 'target_path': 'None', 'send_anonymous_usage_stats': 'True', 'no_print': 'None', 'use_experimental_parser': 'False', 'introspect': 'True', 'invocation_command': 'dbt run', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'debug': 'False', 'printer_width': '80', 'quiet': 'False', 'use_colors': 'True', 'static_parser': 'True', 'warn_error': 'None', 'fail_fast': 'False', 'write_json': 'True', 'indirect_selection': 'eager'}
[0m15:18:28.769163 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '677da618-2f5c-4a4a-9ba5-8a53f5d835fb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024BA7A326D0>]}
[0m15:18:28.964477 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '677da618-2f5c-4a4a-9ba5-8a53f5d835fb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024BA5F448E0>]}
[0m15:18:28.970625 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m15:18:29.473602 [debug] [MainThread]: checksum: 9646db7d1a5f5b9389d9c545d3ebf898b26bc1f63f6b5366f34a6af01d52cb91, vars: {}, profile: , target: , version: 1.10.18
[0m15:18:29.984456 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m15:18:29.985755 [debug] [MainThread]: Partial parsing: updated file: job_intelligent://models\bronze\stg_jobs_raw.sql
[0m15:18:30.429232 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- seeds
[0m15:18:30.430321 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '677da618-2f5c-4a4a-9ba5-8a53f5d835fb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024BA8017130>]}
[0m15:18:30.554908 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Ayoub Gorry\Desktop\powerbi\jobs-power-bi\dbt_project\target\manifest.json
[0m15:18:30.559426 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Ayoub Gorry\Desktop\powerbi\jobs-power-bi\dbt_project\target\semantic_manifest.json
[0m15:18:30.612177 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '677da618-2f5c-4a4a-9ba5-8a53f5d835fb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024BA8017640>]}
[0m15:18:30.614742 [info ] [MainThread]: Found 13 models, 456 macros
[0m15:18:30.615744 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '677da618-2f5c-4a4a-9ba5-8a53f5d835fb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024BA7E43E50>]}
[0m15:18:30.620969 [info ] [MainThread]: 
[0m15:18:30.622153 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:18:30.623958 [info ] [MainThread]: 
[0m15:18:30.625875 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m15:18:30.638576 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_duckdb'
[0m15:18:30.640180 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_duckdb'
[0m15:18:30.656565 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_duckdb'
[0m15:18:30.769126 [debug] [ThreadPool]: Using duckdb connection "list_duckdb"
[0m15:18:30.771070 [debug] [ThreadPool]: Using duckdb connection "list_duckdb"
[0m15:18:30.771853 [debug] [ThreadPool]: Using duckdb connection "list_duckdb"
[0m15:18:30.772973 [debug] [ThreadPool]: On list_duckdb: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_duckdb"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"duckdb"'
    
  
  
[0m15:18:30.773971 [debug] [ThreadPool]: On list_duckdb: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_duckdb"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"duckdb"'
    
  
  
[0m15:18:30.774972 [debug] [ThreadPool]: On list_duckdb: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_duckdb"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"duckdb"'
    
  
  
[0m15:18:30.774972 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:18:30.775972 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:18:30.776973 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:18:30.806556 [debug] [ThreadPool]: SQL status: OK in 0.030 seconds
[0m15:18:30.807559 [debug] [ThreadPool]: SQL status: OK in 0.030 seconds
[0m15:18:30.809071 [debug] [ThreadPool]: SQL status: OK in 0.034 seconds
[0m15:18:30.812094 [debug] [ThreadPool]: On list_duckdb: Close
[0m15:18:30.815096 [debug] [ThreadPool]: On list_duckdb: Close
[0m15:18:30.818618 [debug] [ThreadPool]: On list_duckdb: Close
[0m15:18:30.823461 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_duckdb, now create_duckdb_main_bronze)
[0m15:18:30.824472 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_duckdb, now create_duckdb_main_silver)
[0m15:18:30.825461 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_duckdb, now create_duckdb_main_gold)
[0m15:18:30.827465 [debug] [ThreadPool]: Creating schema "database: "duckdb"
schema: "main_bronze"
"
[0m15:18:30.828459 [debug] [ThreadPool]: Creating schema "database: "duckdb"
schema: "main_silver"
"
[0m15:18:30.829802 [debug] [ThreadPool]: Creating schema "database: "duckdb"
schema: "main_gold"
"
[0m15:18:30.844297 [debug] [ThreadPool]: Using duckdb connection "create_duckdb_main_bronze"
[0m15:18:30.848292 [debug] [ThreadPool]: Using duckdb connection "create_duckdb_main_silver"
[0m15:18:30.853851 [debug] [ThreadPool]: Using duckdb connection "create_duckdb_main_gold"
[0m15:18:30.855308 [debug] [ThreadPool]: On create_duckdb_main_bronze: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_duckdb_main_bronze"} */

    
        select type from duckdb_databases()
        where lower(database_name)='duckdb'
        and type='sqlite'
    
  
[0m15:18:30.856634 [debug] [ThreadPool]: On create_duckdb_main_silver: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_duckdb_main_silver"} */

    
        select type from duckdb_databases()
        where lower(database_name)='duckdb'
        and type='sqlite'
    
  
[0m15:18:30.858681 [debug] [ThreadPool]: On create_duckdb_main_gold: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_duckdb_main_gold"} */

    
        select type from duckdb_databases()
        where lower(database_name)='duckdb'
        and type='sqlite'
    
  
[0m15:18:30.859210 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:18:30.860866 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:18:30.861877 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:18:30.865080 [debug] [ThreadPool]: SQL status: OK in 0.006 seconds
[0m15:18:30.865919 [debug] [ThreadPool]: SQL status: OK in 0.006 seconds
[0m15:18:30.871669 [debug] [ThreadPool]: Using duckdb connection "create_duckdb_main_bronze"
[0m15:18:30.872908 [debug] [ThreadPool]: SQL status: OK in 0.011 seconds
[0m15:18:30.875914 [debug] [ThreadPool]: Using duckdb connection "create_duckdb_main_silver"
[0m15:18:30.876918 [debug] [ThreadPool]: On create_duckdb_main_bronze: BEGIN
[0m15:18:30.879390 [debug] [ThreadPool]: Using duckdb connection "create_duckdb_main_gold"
[0m15:18:30.881016 [debug] [ThreadPool]: On create_duckdb_main_silver: BEGIN
[0m15:18:30.883183 [debug] [ThreadPool]: On create_duckdb_main_gold: BEGIN
[0m15:18:30.883183 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m15:18:30.885559 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m15:18:30.887752 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m15:18:30.888421 [debug] [ThreadPool]: Using duckdb connection "create_duckdb_main_bronze"
[0m15:18:30.890998 [debug] [ThreadPool]: Using duckdb connection "create_duckdb_main_silver"
[0m15:18:30.892007 [debug] [ThreadPool]: Using duckdb connection "create_duckdb_main_gold"
[0m15:18:30.893854 [debug] [ThreadPool]: On create_duckdb_main_bronze: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_duckdb_main_bronze"} */

    
    
        create schema if not exists "duckdb"."main_bronze"
    
[0m15:18:30.895063 [debug] [ThreadPool]: On create_duckdb_main_silver: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_duckdb_main_silver"} */

    
    
        create schema if not exists "duckdb"."main_silver"
    
[0m15:18:30.897064 [debug] [ThreadPool]: On create_duckdb_main_gold: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_duckdb_main_gold"} */

    
    
        create schema if not exists "duckdb"."main_gold"
    
[0m15:18:30.898297 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m15:18:30.899309 [debug] [ThreadPool]: SQL status: OK in 0.002 seconds
[0m15:18:30.901607 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m15:18:30.904461 [debug] [ThreadPool]: On create_duckdb_main_bronze: COMMIT
[0m15:18:30.906930 [debug] [ThreadPool]: On create_duckdb_main_silver: COMMIT
[0m15:18:30.909463 [debug] [ThreadPool]: On create_duckdb_main_gold: COMMIT
[0m15:18:30.910628 [debug] [ThreadPool]: Using duckdb connection "create_duckdb_main_bronze"
[0m15:18:30.911835 [debug] [ThreadPool]: Using duckdb connection "create_duckdb_main_silver"
[0m15:18:30.912856 [debug] [ThreadPool]: Using duckdb connection "create_duckdb_main_gold"
[0m15:18:30.913857 [debug] [ThreadPool]: On create_duckdb_main_bronze: COMMIT
[0m15:18:30.914857 [debug] [ThreadPool]: On create_duckdb_main_silver: COMMIT
[0m15:18:30.915856 [debug] [ThreadPool]: On create_duckdb_main_gold: COMMIT
[0m15:18:30.917599 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m15:18:30.917599 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m15:18:30.919917 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m15:18:30.921147 [debug] [ThreadPool]: On create_duckdb_main_bronze: Close
[0m15:18:30.922300 [debug] [ThreadPool]: On create_duckdb_main_silver: Close
[0m15:18:30.923735 [debug] [ThreadPool]: On create_duckdb_main_gold: Close
[0m15:18:30.929552 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_duckdb_main_bronze'
[0m15:18:30.930702 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_duckdb_main_silver'
[0m15:18:30.946826 [debug] [ThreadPool]: Using duckdb connection "list_duckdb_main_bronze"
[0m15:18:30.946826 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_duckdb_main_gold'
[0m15:18:30.954722 [debug] [ThreadPool]: Using duckdb connection "list_duckdb_main_silver"
[0m15:18:30.956720 [debug] [ThreadPool]: On list_duckdb_main_bronze: BEGIN
[0m15:18:30.960931 [debug] [ThreadPool]: Using duckdb connection "list_duckdb_main_gold"
[0m15:18:30.961950 [debug] [ThreadPool]: On list_duckdb_main_silver: BEGIN
[0m15:18:30.963202 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:18:30.965213 [debug] [ThreadPool]: On list_duckdb_main_gold: BEGIN
[0m15:18:30.966223 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:18:30.969284 [debug] [ThreadPool]: SQL status: OK in 0.005 seconds
[0m15:18:30.970305 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:18:30.972316 [debug] [ThreadPool]: SQL status: OK in 0.006 seconds
[0m15:18:30.973323 [debug] [ThreadPool]: Using duckdb connection "list_duckdb_main_bronze"
[0m15:18:30.975316 [debug] [ThreadPool]: Using duckdb connection "list_duckdb_main_silver"
[0m15:18:30.976316 [debug] [ThreadPool]: SQL status: OK in 0.006 seconds
[0m15:18:30.977316 [debug] [ThreadPool]: On list_duckdb_main_bronze: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_duckdb_main_bronze"} */
select
      'duckdb' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main_bronze'
    and lower(table_catalog) = 'duckdb'
  
[0m15:18:30.978316 [debug] [ThreadPool]: On list_duckdb_main_silver: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_duckdb_main_silver"} */
select
      'duckdb' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main_silver'
    and lower(table_catalog) = 'duckdb'
  
[0m15:18:30.978825 [debug] [ThreadPool]: Using duckdb connection "list_duckdb_main_gold"
[0m15:18:30.980958 [debug] [ThreadPool]: On list_duckdb_main_gold: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_duckdb_main_gold"} */
select
      'duckdb' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main_gold'
    and lower(table_catalog) = 'duckdb'
  
[0m15:18:31.015842 [debug] [ThreadPool]: SQL status: OK in 0.033 seconds
[0m15:18:31.016843 [debug] [ThreadPool]: SQL status: OK in 0.037 seconds
[0m15:18:31.017752 [debug] [ThreadPool]: SQL status: OK in 0.037 seconds
[0m15:18:31.022097 [debug] [ThreadPool]: On list_duckdb_main_gold: ROLLBACK
[0m15:18:31.025613 [debug] [ThreadPool]: On list_duckdb_main_bronze: ROLLBACK
[0m15:18:31.029729 [debug] [ThreadPool]: On list_duckdb_main_silver: ROLLBACK
[0m15:18:31.031855 [debug] [ThreadPool]: Failed to rollback 'list_duckdb_main_gold'
[0m15:18:31.033857 [debug] [ThreadPool]: Failed to rollback 'list_duckdb_main_bronze'
[0m15:18:31.033857 [debug] [ThreadPool]: Failed to rollback 'list_duckdb_main_silver'
[0m15:18:31.035494 [debug] [ThreadPool]: On list_duckdb_main_gold: Close
[0m15:18:31.036593 [debug] [ThreadPool]: On list_duckdb_main_bronze: Close
[0m15:18:31.038363 [debug] [ThreadPool]: On list_duckdb_main_silver: Close
[0m15:18:31.046896 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '677da618-2f5c-4a4a-9ba5-8a53f5d835fb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024BA7E0CF70>]}
[0m15:18:31.048416 [debug] [MainThread]: Using duckdb connection "master"
[0m15:18:31.049895 [debug] [MainThread]: On master: BEGIN
[0m15:18:31.051053 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:18:31.054346 [debug] [MainThread]: SQL status: OK in 0.003 seconds
[0m15:18:31.055636 [debug] [MainThread]: On master: COMMIT
[0m15:18:31.055636 [debug] [MainThread]: Using duckdb connection "master"
[0m15:18:31.057734 [debug] [MainThread]: On master: COMMIT
[0m15:18:31.059499 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:18:31.060519 [debug] [MainThread]: On master: Close
[0m15:18:31.067146 [debug] [Thread-1  ]: Began running node model.job_intelligent.stg_jobs_raw
[0m15:18:31.069100 [info ] [Thread-1  ]: 1 of 13 START sql view model main_bronze.stg_jobs_raw .......................... [RUN]
[0m15:18:31.072123 [debug] [Thread-1  ]: Acquiring new duckdb connection 'model.job_intelligent.stg_jobs_raw'
[0m15:18:31.074127 [debug] [Thread-1  ]: Began compiling node model.job_intelligent.stg_jobs_raw
[0m15:18:31.089312 [debug] [Thread-1  ]: Writing injected SQL for node "model.job_intelligent.stg_jobs_raw"
[0m15:18:31.092830 [debug] [Thread-1  ]: Began executing node model.job_intelligent.stg_jobs_raw
[0m15:18:31.148603 [debug] [Thread-1  ]: Writing runtime sql for node "model.job_intelligent.stg_jobs_raw"
[0m15:18:31.151138 [debug] [Thread-1  ]: Using duckdb connection "model.job_intelligent.stg_jobs_raw"
[0m15:18:31.152132 [debug] [Thread-1  ]: On model.job_intelligent.stg_jobs_raw: BEGIN
[0m15:18:31.154223 [debug] [Thread-1  ]: Opening a new connection, currently in state init
[0m15:18:31.156593 [debug] [Thread-1  ]: SQL status: OK in 0.002 seconds
[0m15:18:31.157604 [debug] [Thread-1  ]: Using duckdb connection "model.job_intelligent.stg_jobs_raw"
[0m15:18:31.159117 [debug] [Thread-1  ]: On model.job_intelligent.stg_jobs_raw: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.stg_jobs_raw"} */

  
  create view "duckdb"."main_bronze"."stg_jobs_raw__dbt_tmp" as (
    -- models/bronze/stg_jobs_raw.sql
-- Bronze layer: Lecture directe des données brutes depuis final_data.csv
-- Renommage et typage minimal



-- Read directly from CSV file
SELECT
    -- Renommer les colonnes pour cohérence
    title as job_title,
    location,
    postedTime as posted_time,
    publishedAt as published_at,
    jobUrl as job_url,
    companyName as company_name,
    companyUrl as company_url,
    description as job_description,
    contractType as contract_type,
    workType as work_type,
    
    -- Métadonnées de traçabilité
    NOW() as ingestion_timestamp,
    '2026-01-08 14:18:28.158720+00:00' as dbt_run_id

FROM read_csv_auto('../data/bronze/final_data.csv')

WHERE 1=1
    -- Filtrer les lignes vides
    AND title IS NOT NULL
    AND description IS NOT NULL
  );

[0m15:18:31.688716 [debug] [Thread-1  ]: SQL status: OK in 0.528 seconds
[0m15:18:31.696383 [debug] [Thread-1  ]: Using duckdb connection "model.job_intelligent.stg_jobs_raw"
[0m15:18:31.696383 [debug] [Thread-1  ]: On model.job_intelligent.stg_jobs_raw: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.stg_jobs_raw"} */
alter view "duckdb"."main_bronze"."stg_jobs_raw" rename to "stg_jobs_raw__dbt_backup"
[0m15:18:31.700129 [debug] [Thread-1  ]: SQL status: OK in 0.002 seconds
[0m15:18:31.706445 [debug] [Thread-1  ]: Using duckdb connection "model.job_intelligent.stg_jobs_raw"
[0m15:18:31.708455 [debug] [Thread-1  ]: On model.job_intelligent.stg_jobs_raw: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.stg_jobs_raw"} */
alter view "duckdb"."main_bronze"."stg_jobs_raw__dbt_tmp" rename to "stg_jobs_raw"
[0m15:18:31.709482 [debug] [Thread-1  ]: SQL status: OK in 0.001 seconds
[0m15:18:31.726810 [debug] [Thread-1  ]: On model.job_intelligent.stg_jobs_raw: COMMIT
[0m15:18:31.726810 [debug] [Thread-1  ]: Using duckdb connection "model.job_intelligent.stg_jobs_raw"
[0m15:18:31.728347 [debug] [Thread-1  ]: On model.job_intelligent.stg_jobs_raw: COMMIT
[0m15:18:31.732111 [debug] [Thread-1  ]: SQL status: OK in 0.004 seconds
[0m15:18:31.741575 [debug] [Thread-1  ]: Using duckdb connection "model.job_intelligent.stg_jobs_raw"
[0m15:18:31.742578 [debug] [Thread-1  ]: On model.job_intelligent.stg_jobs_raw: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.stg_jobs_raw"} */

      drop view if exists "duckdb"."main_bronze"."stg_jobs_raw__dbt_backup" cascade
    
[0m15:18:31.745915 [debug] [Thread-1  ]: SQL status: OK in 0.002 seconds
[0m15:18:31.749916 [debug] [Thread-1  ]: On model.job_intelligent.stg_jobs_raw: Close
[0m15:18:31.752791 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '677da618-2f5c-4a4a-9ba5-8a53f5d835fb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024BA84FEB20>]}
[0m15:18:31.756467 [info ] [Thread-1  ]: 1 of 13 OK created sql view model main_bronze.stg_jobs_raw ..................... [[32mOK[0m in 0.68s]
[0m15:18:31.758498 [debug] [Thread-1  ]: Finished running node model.job_intelligent.stg_jobs_raw
[0m15:18:31.759549 [debug] [Thread-3  ]: Began running node model.job_intelligent.int_jobs_cleaned
[0m15:18:31.759549 [info ] [Thread-3  ]: 2 of 13 START sql table model main_silver.int_jobs_cleaned ..................... [RUN]
[0m15:18:31.763199 [debug] [Thread-3  ]: Acquiring new duckdb connection 'model.job_intelligent.int_jobs_cleaned'
[0m15:18:31.764573 [debug] [Thread-3  ]: Began compiling node model.job_intelligent.int_jobs_cleaned
[0m15:18:31.770982 [debug] [Thread-3  ]: Writing injected SQL for node "model.job_intelligent.int_jobs_cleaned"
[0m15:18:31.774998 [debug] [Thread-3  ]: Began executing node model.job_intelligent.int_jobs_cleaned
[0m15:18:31.826942 [debug] [Thread-3  ]: Writing runtime sql for node "model.job_intelligent.int_jobs_cleaned"
[0m15:18:31.829526 [debug] [Thread-3  ]: Using duckdb connection "model.job_intelligent.int_jobs_cleaned"
[0m15:18:31.830657 [debug] [Thread-3  ]: On model.job_intelligent.int_jobs_cleaned: BEGIN
[0m15:18:31.830657 [debug] [Thread-3  ]: Opening a new connection, currently in state init
[0m15:18:31.830657 [debug] [Thread-3  ]: SQL status: OK in 0.001 seconds
[0m15:18:31.834647 [debug] [Thread-3  ]: Using duckdb connection "model.job_intelligent.int_jobs_cleaned"
[0m15:18:31.835542 [debug] [Thread-3  ]: On model.job_intelligent.int_jobs_cleaned: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.int_jobs_cleaned"} */

  
    
    

    create  table
      "duckdb"."main_silver"."int_jobs_cleaned__dbt_tmp"
  
    as (
      -- models/silver/int_jobs_cleaned.sql
-- Silver layer: Nettoyage et normalisation des données brutes



WITH raw_jobs AS (
    SELECT * FROM "duckdb"."main_bronze"."stg_jobs_raw"
),

cleaned_jobs AS (
    SELECT
        -- Texte: lowercase, trim, remove special characters
        LOWER(TRIM(job_title)) as job_title_cleaned,
        LOWER(TRIM(location)) as location_cleaned,
        LOWER(TRIM(company_name)) as company_name_cleaned,
        LOWER(TRIM(job_description)) as job_description_cleaned,
        LOWER(TRIM(contract_type)) as contract_type_cleaned,
        LOWER(TRIM(work_type)) as work_type_cleaned,
        
        -- URLs as-is
        job_url,
        company_url,
        
        -- Dates
        TRY_CAST(published_at AS DATE) as published_date,
        posted_time,
        
        -- Extract year-month for time-based analysis
        DATE_TRUNC('month', TRY_CAST(published_at AS DATE)) as published_year_month,
        EXTRACT(YEAR FROM TRY_CAST(published_at AS DATE)) as published_year,
        EXTRACT(MONTH FROM TRY_CAST(published_at AS DATE)) as published_month,
        
        -- Métadonnées
        ingestion_timestamp
    FROM raw_jobs
)

SELECT
    *,
    -- Deduplication flag
    ROW_NUMBER() OVER (
        PARTITION BY job_title_cleaned, company_name_cleaned, location_cleaned 
        ORDER BY published_date DESC
    ) as dedup_rank
FROM cleaned_jobs
    );
  
  
[0m15:18:32.089326 [debug] [Thread-3  ]: SQL status: OK in 0.256 seconds
[0m15:18:32.100674 [debug] [Thread-3  ]: Using duckdb connection "model.job_intelligent.int_jobs_cleaned"
[0m15:18:32.102750 [debug] [Thread-3  ]: On model.job_intelligent.int_jobs_cleaned: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.int_jobs_cleaned"} */

    SELECT index_name
    FROM duckdb_indexes()
    WHERE schema_name = 'main_silver'
      AND table_name = 'int_jobs_cleaned'
  
[0m15:18:32.104777 [debug] [Thread-3  ]: SQL status: OK in 0.001 seconds
[0m15:18:32.106843 [debug] [Thread-3  ]: Using duckdb connection "model.job_intelligent.int_jobs_cleaned"
[0m15:18:32.106843 [debug] [Thread-3  ]: On model.job_intelligent.int_jobs_cleaned: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.int_jobs_cleaned"} */

    SELECT COUNT(*) as remaining_indexes
    FROM duckdb_indexes()
    WHERE schema_name = 'main_silver'
      AND table_name = 'int_jobs_cleaned'
  
[0m15:18:32.109419 [debug] [Thread-3  ]: SQL status: OK in 0.001 seconds
[0m15:18:32.111471 [debug] [Thread-3  ]: Using duckdb connection "model.job_intelligent.int_jobs_cleaned"
[0m15:18:32.111471 [debug] [Thread-3  ]: On model.job_intelligent.int_jobs_cleaned: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.int_jobs_cleaned"} */
alter table "duckdb"."main_silver"."int_jobs_cleaned" rename to "int_jobs_cleaned__dbt_backup"
[0m15:18:32.117125 [debug] [Thread-3  ]: SQL status: OK in 0.001 seconds
[0m15:18:32.120205 [debug] [Thread-3  ]: Using duckdb connection "model.job_intelligent.int_jobs_cleaned"
[0m15:18:32.120205 [debug] [Thread-3  ]: On model.job_intelligent.int_jobs_cleaned: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.int_jobs_cleaned"} */
alter table "duckdb"."main_silver"."int_jobs_cleaned__dbt_tmp" rename to "int_jobs_cleaned"
[0m15:18:32.120205 [debug] [Thread-3  ]: SQL status: OK in 0.000 seconds
[0m15:18:32.132835 [debug] [Thread-3  ]: On model.job_intelligent.int_jobs_cleaned: COMMIT
[0m15:18:32.133847 [debug] [Thread-3  ]: Using duckdb connection "model.job_intelligent.int_jobs_cleaned"
[0m15:18:32.135338 [debug] [Thread-3  ]: On model.job_intelligent.int_jobs_cleaned: COMMIT
[0m15:18:32.206736 [debug] [Thread-3  ]: SQL status: OK in 0.070 seconds
[0m15:18:32.209268 [debug] [Thread-3  ]: Using duckdb connection "model.job_intelligent.int_jobs_cleaned"
[0m15:18:32.211289 [debug] [Thread-3  ]: On model.job_intelligent.int_jobs_cleaned: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.int_jobs_cleaned"} */

      drop table if exists "duckdb"."main_silver"."int_jobs_cleaned__dbt_backup" cascade
    
[0m15:18:32.214557 [debug] [Thread-3  ]: SQL status: OK in 0.002 seconds
[0m15:18:32.217183 [debug] [Thread-3  ]: On model.job_intelligent.int_jobs_cleaned: Close
[0m15:18:32.218696 [debug] [Thread-3  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '677da618-2f5c-4a4a-9ba5-8a53f5d835fb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024BA9606C40>]}
[0m15:18:32.220449 [info ] [Thread-3  ]: 2 of 13 OK created sql table model main_silver.int_jobs_cleaned ................ [[32mOK[0m in 0.45s]
[0m15:18:32.220449 [debug] [Thread-3  ]: Finished running node model.job_intelligent.int_jobs_cleaned
[0m15:18:32.222507 [debug] [Thread-2  ]: Began running node model.job_intelligent.int_job_title_normalization
[0m15:18:32.224530 [info ] [Thread-2  ]: 3 of 13 START sql table model main_silver.int_job_title_normalization .......... [RUN]
[0m15:18:32.224530 [debug] [Thread-2  ]: Acquiring new duckdb connection 'model.job_intelligent.int_job_title_normalization'
[0m15:18:32.224530 [debug] [Thread-2  ]: Began compiling node model.job_intelligent.int_job_title_normalization
[0m15:18:32.232131 [debug] [Thread-2  ]: Writing injected SQL for node "model.job_intelligent.int_job_title_normalization"
[0m15:18:32.235141 [debug] [Thread-2  ]: Began executing node model.job_intelligent.int_job_title_normalization
[0m15:18:32.245175 [debug] [Thread-2  ]: Writing runtime sql for node "model.job_intelligent.int_job_title_normalization"
[0m15:18:32.246181 [debug] [Thread-2  ]: Using duckdb connection "model.job_intelligent.int_job_title_normalization"
[0m15:18:32.249378 [debug] [Thread-2  ]: On model.job_intelligent.int_job_title_normalization: BEGIN
[0m15:18:32.249378 [debug] [Thread-2  ]: Opening a new connection, currently in state init
[0m15:18:32.252021 [debug] [Thread-2  ]: SQL status: OK in 0.001 seconds
[0m15:18:32.253037 [debug] [Thread-2  ]: Using duckdb connection "model.job_intelligent.int_job_title_normalization"
[0m15:18:32.254723 [debug] [Thread-2  ]: On model.job_intelligent.int_job_title_normalization: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.int_job_title_normalization"} */

  
    
    

    create  table
      "duckdb"."main_silver"."int_job_title_normalization__dbt_tmp"
  
    as (
      -- models/silver/int_job_title_normalization.sql
-- Silver layer: Normaliser les intitulés de postes



WITH cleaned_jobs AS (
    SELECT * FROM "duckdb"."main_silver"."int_jobs_cleaned"
),

title_normalized AS (
    SELECT
        *,
        CASE
            WHEN job_title_cleaned LIKE '%data engineer%' THEN 'Data Engineer'
            WHEN job_title_cleaned LIKE '%data scientist%' THEN 'Data Scientist'
            WHEN job_title_cleaned LIKE '%data analyst%' THEN 'Data Analyst'
            WHEN job_title_cleaned LIKE '%analytics engineer%' THEN 'Analytics Engineer'
            WHEN job_title_cleaned LIKE '%ml engineer%' OR job_title_cleaned LIKE '%machine learning%' THEN 'ML Engineer'
            WHEN job_title_cleaned LIKE '%data architect%' THEN 'Data Architect'
            WHEN job_title_cleaned LIKE '%bi developer%' OR job_title_cleaned LIKE '%business intelligence%' THEN 'BI Developer'
            WHEN job_title_cleaned LIKE '%etl%' OR job_title_cleaned LIKE '%pipeline%' THEN 'ETL/Pipeline Engineer'
            WHEN job_title_cleaned LIKE '%data engineer%' THEN 'Data Engineer'
            ELSE 'Other Data Role'
        END as job_category,
        
        CASE
            WHEN contract_type_cleaned LIKE '%cdi%' OR contract_type_cleaned LIKE '%permanent%' THEN 'Permanent'
            WHEN contract_type_cleaned LIKE '%cdd%' OR contract_type_cleaned LIKE '%contract%' THEN 'Contract'
            WHEN contract_type_cleaned LIKE '%stage%' OR contract_type_cleaned LIKE '%internship%' THEN 'Internship'
            WHEN contract_type_cleaned LIKE '%freelance%' THEN 'Freelance'
            ELSE 'Not Specified'
        END as contract_type_normalized,
        
        CASE
            WHEN work_type_cleaned LIKE '%remote%' THEN 'Remote'
            WHEN work_type_cleaned LIKE '%hybrid%' THEN 'Hybrid'
            WHEN work_type_cleaned LIKE '%onsite%' OR work_type_cleaned LIKE '%on-site%' THEN 'On-site'
            ELSE 'Not Specified'
        END as work_type_normalized
    
    FROM cleaned_jobs
    WHERE dedup_rank = 1  -- Keep only first occurrence (most recent)
)

SELECT * FROM title_normalized
    );
  
  
[0m15:18:32.296745 [debug] [Thread-2  ]: SQL status: OK in 0.040 seconds
[0m15:18:32.298750 [debug] [Thread-2  ]: Using duckdb connection "model.job_intelligent.int_job_title_normalization"
[0m15:18:32.299256 [debug] [Thread-2  ]: On model.job_intelligent.int_job_title_normalization: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.int_job_title_normalization"} */

    SELECT index_name
    FROM duckdb_indexes()
    WHERE schema_name = 'main_silver'
      AND table_name = 'int_job_title_normalization'
  
[0m15:18:32.300828 [debug] [Thread-2  ]: SQL status: OK in 0.001 seconds
[0m15:18:32.304909 [debug] [Thread-2  ]: Using duckdb connection "model.job_intelligent.int_job_title_normalization"
[0m15:18:32.305923 [debug] [Thread-2  ]: On model.job_intelligent.int_job_title_normalization: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.int_job_title_normalization"} */

    SELECT COUNT(*) as remaining_indexes
    FROM duckdb_indexes()
    WHERE schema_name = 'main_silver'
      AND table_name = 'int_job_title_normalization'
  
[0m15:18:32.308431 [debug] [Thread-2  ]: SQL status: OK in 0.001 seconds
[0m15:18:32.314554 [debug] [Thread-2  ]: Using duckdb connection "model.job_intelligent.int_job_title_normalization"
[0m15:18:32.315543 [debug] [Thread-2  ]: On model.job_intelligent.int_job_title_normalization: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.int_job_title_normalization"} */
alter table "duckdb"."main_silver"."int_job_title_normalization" rename to "int_job_title_normalization__dbt_backup"
[0m15:18:32.316556 [debug] [Thread-2  ]: SQL status: OK in 0.000 seconds
[0m15:18:32.323226 [debug] [Thread-2  ]: Using duckdb connection "model.job_intelligent.int_job_title_normalization"
[0m15:18:32.324234 [debug] [Thread-2  ]: On model.job_intelligent.int_job_title_normalization: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.int_job_title_normalization"} */
alter table "duckdb"."main_silver"."int_job_title_normalization__dbt_tmp" rename to "int_job_title_normalization"
[0m15:18:32.326732 [debug] [Thread-2  ]: SQL status: OK in 0.001 seconds
[0m15:18:32.330426 [debug] [Thread-2  ]: On model.job_intelligent.int_job_title_normalization: COMMIT
[0m15:18:32.331426 [debug] [Thread-2  ]: Using duckdb connection "model.job_intelligent.int_job_title_normalization"
[0m15:18:32.332421 [debug] [Thread-2  ]: On model.job_intelligent.int_job_title_normalization: COMMIT
[0m15:18:32.384214 [debug] [Thread-2  ]: SQL status: OK in 0.051 seconds
[0m15:18:32.390148 [debug] [Thread-2  ]: Using duckdb connection "model.job_intelligent.int_job_title_normalization"
[0m15:18:32.390994 [debug] [Thread-2  ]: On model.job_intelligent.int_job_title_normalization: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.int_job_title_normalization"} */

      drop table if exists "duckdb"."main_silver"."int_job_title_normalization__dbt_backup" cascade
    
[0m15:18:32.627730 [debug] [Thread-2  ]: SQL status: OK in 0.235 seconds
[0m15:18:32.630460 [debug] [Thread-2  ]: On model.job_intelligent.int_job_title_normalization: Close
[0m15:18:32.631471 [debug] [Thread-2  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '677da618-2f5c-4a4a-9ba5-8a53f5d835fb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024BA84FE820>]}
[0m15:18:32.632473 [info ] [Thread-2  ]: 3 of 13 OK created sql table model main_silver.int_job_title_normalization ..... [[32mOK[0m in 0.41s]
[0m15:18:32.634771 [debug] [Thread-2  ]: Finished running node model.job_intelligent.int_job_title_normalization
[0m15:18:32.638314 [debug] [Thread-4  ]: Began running node model.job_intelligent.dim_company
[0m15:18:32.639836 [debug] [Thread-3  ]: Began running node model.job_intelligent.dim_location
[0m15:18:32.640870 [debug] [Thread-1  ]: Began running node model.job_intelligent.dim_time
[0m15:18:32.641869 [debug] [Thread-2  ]: Began running node model.job_intelligent.int_skills_extraction
[0m15:18:32.642863 [info ] [Thread-4  ]: 4 of 13 START sql table model main_gold.dim_company ............................ [RUN]
[0m15:18:32.644848 [info ] [Thread-3  ]: 5 of 13 START sql table model main_gold.dim_location ........................... [RUN]
[0m15:18:32.646846 [info ] [Thread-1  ]: 6 of 13 START sql table model main_gold.dim_time ............................... [RUN]
[0m15:18:32.648356 [info ] [Thread-2  ]: 7 of 13 START sql table model main_silver.int_skills_extraction ................ [RUN]
[0m15:18:32.649877 [debug] [Thread-4  ]: Acquiring new duckdb connection 'model.job_intelligent.dim_company'
[0m15:18:32.650909 [debug] [Thread-3  ]: Re-using an available connection from the pool (formerly model.job_intelligent.int_jobs_cleaned, now model.job_intelligent.dim_location)
[0m15:18:32.654059 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.job_intelligent.stg_jobs_raw, now model.job_intelligent.dim_time)
[0m15:18:32.656255 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly model.job_intelligent.int_job_title_normalization, now model.job_intelligent.int_skills_extraction)
[0m15:18:32.657250 [debug] [Thread-4  ]: Began compiling node model.job_intelligent.dim_company
[0m15:18:32.659826 [debug] [Thread-3  ]: Began compiling node model.job_intelligent.dim_location
[0m15:18:32.660837 [debug] [Thread-1  ]: Began compiling node model.job_intelligent.dim_time
[0m15:18:32.663395 [debug] [Thread-2  ]: Began compiling node model.job_intelligent.int_skills_extraction
[0m15:18:32.675485 [debug] [Thread-4  ]: Writing injected SQL for node "model.job_intelligent.dim_company"
[0m15:18:32.683403 [debug] [Thread-3  ]: Writing injected SQL for node "model.job_intelligent.dim_location"
[0m15:18:32.692694 [debug] [Thread-1  ]: Writing injected SQL for node "model.job_intelligent.dim_time"
[0m15:18:32.699965 [debug] [Thread-2  ]: Writing injected SQL for node "model.job_intelligent.int_skills_extraction"
[0m15:18:32.705622 [debug] [Thread-4  ]: Began executing node model.job_intelligent.dim_company
[0m15:18:32.708391 [debug] [Thread-3  ]: Began executing node model.job_intelligent.dim_location
[0m15:18:32.727377 [debug] [Thread-4  ]: Writing runtime sql for node "model.job_intelligent.dim_company"
[0m15:18:32.728387 [debug] [Thread-2  ]: Began executing node model.job_intelligent.int_skills_extraction
[0m15:18:32.729400 [debug] [Thread-1  ]: Began executing node model.job_intelligent.dim_time
[0m15:18:32.828440 [debug] [Thread-3  ]: Writing runtime sql for node "model.job_intelligent.dim_location"
[0m15:18:32.838416 [debug] [Thread-2  ]: Writing runtime sql for node "model.job_intelligent.int_skills_extraction"
[0m15:18:32.851168 [debug] [Thread-4  ]: Using duckdb connection "model.job_intelligent.dim_company"
[0m15:18:32.849368 [debug] [Thread-1  ]: Writing runtime sql for node "model.job_intelligent.dim_time"
[0m15:18:32.853749 [debug] [Thread-4  ]: On model.job_intelligent.dim_company: BEGIN
[0m15:18:32.855764 [debug] [Thread-3  ]: Using duckdb connection "model.job_intelligent.dim_location"
[0m15:18:32.858422 [debug] [Thread-2  ]: Using duckdb connection "model.job_intelligent.int_skills_extraction"
[0m15:18:32.860182 [debug] [Thread-4  ]: Opening a new connection, currently in state init
[0m15:18:32.860182 [debug] [Thread-1  ]: Using duckdb connection "model.job_intelligent.dim_time"
[0m15:18:32.862196 [debug] [Thread-3  ]: On model.job_intelligent.dim_location: BEGIN
[0m15:18:32.864210 [debug] [Thread-2  ]: On model.job_intelligent.int_skills_extraction: BEGIN
[0m15:18:32.867521 [debug] [Thread-1  ]: On model.job_intelligent.dim_time: BEGIN
[0m15:18:32.868539 [debug] [Thread-4  ]: SQL status: OK in 0.009 seconds
[0m15:18:32.870735 [debug] [Thread-3  ]: Opening a new connection, currently in state closed
[0m15:18:32.870735 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m15:18:32.873354 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m15:18:32.873354 [debug] [Thread-4  ]: Using duckdb connection "model.job_intelligent.dim_company"
[0m15:18:32.873354 [debug] [Thread-3  ]: SQL status: OK in 0.006 seconds
[0m15:18:32.877284 [debug] [Thread-2  ]: SQL status: OK in 0.005 seconds
[0m15:18:32.878294 [debug] [Thread-1  ]: SQL status: OK in 0.005 seconds
[0m15:18:32.879321 [debug] [Thread-4  ]: On model.job_intelligent.dim_company: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_company"} */

  
    
    

    create  table
      "duckdb"."main_gold"."dim_company__dbt_tmp"
  
    as (
      -- models/gold/dim_company.sql
-- Gold layer: Dimension Company



WITH jobs AS (
    SELECT DISTINCT
        company_name_cleaned,
        company_url
    FROM "duckdb"."main_silver"."int_job_title_normalization"
    WHERE company_name_cleaned IS NOT NULL
),

ranked_companies AS (
    SELECT
        ROW_NUMBER() OVER (ORDER BY company_name_cleaned) as company_id,
        company_name_cleaned as company_name,
        company_url,
        NOW() as created_at
    FROM jobs
)

SELECT
    company_id,
    company_name,
    company_url,
    created_at
FROM ranked_companies
ORDER BY company_id
    );
  
  
[0m15:18:32.879321 [debug] [Thread-3  ]: Using duckdb connection "model.job_intelligent.dim_location"
[0m15:18:32.882448 [debug] [Thread-2  ]: Using duckdb connection "model.job_intelligent.int_skills_extraction"
[0m15:18:32.883096 [debug] [Thread-1  ]: Using duckdb connection "model.job_intelligent.dim_time"
[0m15:18:32.887123 [debug] [Thread-3  ]: On model.job_intelligent.dim_location: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_location"} */

  
    
    

    create  table
      "duckdb"."main_gold"."dim_location__dbt_tmp"
  
    as (
      -- models/gold/dim_location.sql
-- Gold layer: Dimension Location



WITH jobs AS (
    SELECT DISTINCT
        location_cleaned as location_raw
    FROM "duckdb"."main_silver"."int_job_title_normalization"
    WHERE location_cleaned IS NOT NULL
),

location_parsed AS (
    SELECT
        location_raw,
        -- Extract city (before comma)
        CASE 
            WHEN POSITION(',' IN location_raw) > 0 
            THEN TRIM(SUBSTRING(location_raw, 1, POSITION(',' IN location_raw) - 1))
            ELSE location_raw
        END as city,
        
        -- Extract country (after comma)
        CASE 
            WHEN POSITION(',' IN location_raw) > 0 
            THEN TRIM(SUBSTRING(location_raw, POSITION(',' IN location_raw) + 1))
            ELSE 'Not Specified'
        END as country,
        
        -- Detect if remote
        CASE 
            WHEN location_raw LIKE '%remote%' 
            THEN 'Remote'
            ELSE 'On-site'
        END as work_location_type
    FROM jobs
),

ranked_locations AS (
    SELECT
        ROW_NUMBER() OVER (ORDER BY location_raw) as location_id,
        location_raw,
        city,
        country,
        work_location_type,
        NOW() as created_at
    FROM location_parsed
)

SELECT
    location_id,
    location_raw,
    city,
    country,
    work_location_type,
    created_at
FROM ranked_locations
ORDER BY location_id
    );
  
  
[0m15:18:32.890658 [debug] [Thread-2  ]: On model.job_intelligent.int_skills_extraction: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.int_skills_extraction"} */

  
    
    

    create  table
      "duckdb"."main_silver"."int_skills_extraction__dbt_tmp"
  
    as (
      -- models/silver/int_skills_extraction.sql
-- Silver layer: Extraction des compétences depuis la description



WITH jobs_with_titles AS (
    SELECT * FROM "duckdb"."main_silver"."int_job_title_normalization"
),

skills_mapping AS (
    -- Définir un mapping de compétences communes Data/Tech
    SELECT
        'Python' as skill_name,
        'python|py |\.py' as skill_pattern
    UNION ALL SELECT 'SQL', 'sql|sql|sql server|postgres|oracle'
    UNION ALL SELECT 'Spark', 'spark|pyspark'
    UNION ALL SELECT 'Hadoop', 'hadoop|hdfs'
    UNION ALL SELECT 'Scala', 'scala'
    UNION ALL SELECT 'Java', '\bjava\b'
    UNION ALL SELECT 'R', '\br\b|r programming'
    UNION ALL SELECT 'Tableau', 'tableau'
    UNION ALL SELECT 'Power BI', 'power bi|powerbi'
    UNION ALL SELECT 'Looker', 'looker'
    UNION ALL SELECT 'AWS', 'aws|amazon web|s3 |ec2|redshift'
    UNION ALL SELECT 'Azure', 'azure|microsoft azure|synapse|cosmos'
    UNION ALL SELECT 'GCP', 'gcp|google cloud|bigquery'
    UNION ALL SELECT 'Airflow', 'airflow'
    UNION ALL SELECT 'DBT', '\bdbt\b|dbt'
    UNION ALL SELECT 'Kubernetes', 'kubernetes|k8s'
    UNION ALL SELECT 'Docker', 'docker'
    UNION ALL SELECT 'Git', 'git|github|gitlab'
    UNION ALL SELECT 'TensorFlow', 'tensorflow'
    UNION ALL SELECT 'PyTorch', 'pytorch'
    UNION ALL SELECT 'Scikit-learn', 'scikit|sklearn'
    UNION ALL SELECT 'Pandas', 'pandas'
    UNION ALL SELECT 'NumPy', 'numpy'
    UNION ALL SELECT 'Machine Learning', 'machine learning|deep learning|ml|artificial intelligence'
    UNION ALL SELECT 'Statistics', 'statistics|statistical|probability'
    UNION ALL SELECT 'Data Visualization', 'data visualization|visualization|charts|graphs'
),

jobs_exploded AS (
    SELECT
        j.*,
        s.skill_name,
        CASE 
            WHEN job_description_cleaned ILIKE '%' || s.skill_pattern || '%' THEN 1 
            ELSE 0 
        END as has_skill
    FROM jobs_with_titles j
    CROSS JOIN skills_mapping s
)

SELECT
    *
FROM jobs_exploded
WHERE has_skill = 1

ORDER BY job_title_cleaned, published_date DESC
    );
  
  
[0m15:18:32.893296 [debug] [Thread-1  ]: On model.job_intelligent.dim_time: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_time"} */

  
    
    

    create  table
      "duckdb"."main_gold"."dim_time__dbt_tmp"
  
    as (
      -- models/gold/dim_time.sql
-- Gold layer: Dimension Time



-- Generate a date dimension for time-based analysis
WITH date_spine AS (
    SELECT
        published_date,
        EXTRACT(YEAR FROM published_date) as year,
        EXTRACT(MONTH FROM published_date) as month,
        EXTRACT(QUARTER FROM published_date) as quarter,
        EXTRACT(WEEK FROM published_date) as week,
        EXTRACT(DAYOFWEEK FROM published_date) as day_of_week,
        DATE_TRUNC('month', published_date) as month_start,
        DATE_TRUNC('quarter', published_date) as quarter_start,
        DATE_TRUNC('year', published_date) as year_start,
        
        CASE 
            WHEN EXTRACT(MONTH FROM published_date) IN (1,2,3) THEN 'Q1'
            WHEN EXTRACT(MONTH FROM published_date) IN (4,5,6) THEN 'Q2'
            WHEN EXTRACT(MONTH FROM published_date) IN (7,8,9) THEN 'Q3'
            ELSE 'Q4'
        END as quarter_name,
        
        CASE EXTRACT(DAYOFWEEK FROM published_date)
            WHEN 0 THEN 'Sunday'
            WHEN 1 THEN 'Monday'
            WHEN 2 THEN 'Tuesday'
            WHEN 3 THEN 'Wednesday'
            WHEN 4 THEN 'Thursday'
            WHEN 5 THEN 'Friday'
            WHEN 6 THEN 'Saturday'
        END as day_name,
        
        CASE EXTRACT(MONTH FROM published_date)
            WHEN 1 THEN 'January'
            WHEN 2 THEN 'February'
            WHEN 3 THEN 'March'
            WHEN 4 THEN 'April'
            WHEN 5 THEN 'May'
            WHEN 6 THEN 'June'
            WHEN 7 THEN 'July'
            WHEN 8 THEN 'August'
            WHEN 9 THEN 'September'
            WHEN 10 THEN 'October'
            WHEN 11 THEN 'November'
            WHEN 12 THEN 'December'
        END as month_name
        
    FROM (
        SELECT DISTINCT published_date
        FROM "duckdb"."main_silver"."int_job_title_normalization"
        WHERE published_date IS NOT NULL
    )
)

SELECT
    published_date as date_id,
    year,
    month,
    quarter,
    week,
    day_of_week,
    day_name,
    month_name,
    quarter_name,
    month_start,
    quarter_start,
    year_start,
    NOW() as created_at
FROM date_spine
ORDER BY published_date
    );
  
  
[0m15:18:32.898505 [debug] [Thread-4  ]: SQL status: OK in 0.013 seconds
[0m15:18:32.904673 [debug] [Thread-4  ]: Using duckdb connection "model.job_intelligent.dim_company"
[0m15:18:32.907391 [debug] [Thread-4  ]: On model.job_intelligent.dim_company: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_company"} */

    SELECT index_name
    FROM duckdb_indexes()
    WHERE schema_name = 'main_gold'
      AND table_name = 'dim_company'
  
[0m15:18:32.910571 [debug] [Thread-4  ]: SQL status: OK in 0.001 seconds
[0m15:18:32.915149 [debug] [Thread-3  ]: SQL status: OK in 0.019 seconds
[0m15:18:32.919120 [debug] [Thread-4  ]: Using duckdb connection "model.job_intelligent.dim_company"
[0m15:18:32.922671 [debug] [Thread-3  ]: Using duckdb connection "model.job_intelligent.dim_location"
[0m15:18:32.922671 [debug] [Thread-4  ]: On model.job_intelligent.dim_company: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_company"} */

    SELECT COUNT(*) as remaining_indexes
    FROM duckdb_indexes()
    WHERE schema_name = 'main_gold'
      AND table_name = 'dim_company'
  
[0m15:18:32.926610 [debug] [Thread-1  ]: SQL status: OK in 0.025 seconds
[0m15:18:32.929239 [debug] [Thread-3  ]: On model.job_intelligent.dim_location: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_location"} */

    SELECT index_name
    FROM duckdb_indexes()
    WHERE schema_name = 'main_gold'
      AND table_name = 'dim_location'
  
[0m15:18:32.934532 [debug] [Thread-1  ]: Using duckdb connection "model.job_intelligent.dim_time"
[0m15:18:32.935336 [debug] [Thread-4  ]: SQL status: OK in 0.005 seconds
[0m15:18:32.939167 [debug] [Thread-1  ]: On model.job_intelligent.dim_time: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_time"} */

    SELECT index_name
    FROM duckdb_indexes()
    WHERE schema_name = 'main_gold'
      AND table_name = 'dim_time'
  
[0m15:18:32.940179 [debug] [Thread-3  ]: SQL status: OK in 0.002 seconds
[0m15:18:32.947189 [debug] [Thread-4  ]: Using duckdb connection "model.job_intelligent.dim_company"
[0m15:18:32.950903 [debug] [Thread-3  ]: Using duckdb connection "model.job_intelligent.dim_location"
[0m15:18:32.952916 [debug] [Thread-1  ]: SQL status: OK in 0.004 seconds
[0m15:18:32.954019 [debug] [Thread-4  ]: On model.job_intelligent.dim_company: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_company"} */
alter table "duckdb"."main_gold"."dim_company" rename to "dim_company__dbt_backup"
[0m15:18:32.956125 [debug] [Thread-3  ]: On model.job_intelligent.dim_location: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_location"} */

    SELECT COUNT(*) as remaining_indexes
    FROM duckdb_indexes()
    WHERE schema_name = 'main_gold'
      AND table_name = 'dim_location'
  
[0m15:18:32.959178 [debug] [Thread-1  ]: Using duckdb connection "model.job_intelligent.dim_time"
[0m15:18:32.959178 [debug] [Thread-4  ]: SQL status: OK in 0.001 seconds
[0m15:18:32.959178 [debug] [Thread-3  ]: SQL status: OK in 0.001 seconds
[0m15:18:32.959178 [debug] [Thread-1  ]: On model.job_intelligent.dim_time: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_time"} */

    SELECT COUNT(*) as remaining_indexes
    FROM duckdb_indexes()
    WHERE schema_name = 'main_gold'
      AND table_name = 'dim_time'
  
[0m15:18:32.968339 [debug] [Thread-4  ]: Using duckdb connection "model.job_intelligent.dim_company"
[0m15:18:32.976264 [debug] [Thread-3  ]: Using duckdb connection "model.job_intelligent.dim_location"
[0m15:18:32.978776 [debug] [Thread-4  ]: On model.job_intelligent.dim_company: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_company"} */
alter table "duckdb"."main_gold"."dim_company__dbt_tmp" rename to "dim_company"
[0m15:18:32.979304 [debug] [Thread-1  ]: SQL status: OK in 0.002 seconds
[0m15:18:32.980320 [debug] [Thread-3  ]: On model.job_intelligent.dim_location: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_location"} */
alter table "duckdb"."main_gold"."dim_location" rename to "dim_location__dbt_backup"
[0m15:18:32.988487 [debug] [Thread-1  ]: Using duckdb connection "model.job_intelligent.dim_time"
[0m15:18:32.989513 [debug] [Thread-4  ]: SQL status: OK in 0.007 seconds
[0m15:18:32.991076 [debug] [Thread-3  ]: SQL status: OK in 0.001 seconds
[0m15:18:32.992093 [debug] [Thread-1  ]: On model.job_intelligent.dim_time: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_time"} */
alter table "duckdb"."main_gold"."dim_time" rename to "dim_time__dbt_backup"
[0m15:18:32.995092 [debug] [Thread-4  ]: On model.job_intelligent.dim_company: COMMIT
[0m15:18:33.001175 [debug] [Thread-3  ]: Using duckdb connection "model.job_intelligent.dim_location"
[0m15:18:33.003175 [debug] [Thread-4  ]: Using duckdb connection "model.job_intelligent.dim_company"
[0m15:18:33.004771 [debug] [Thread-1  ]: SQL status: OK in 0.002 seconds
[0m15:18:33.004771 [debug] [Thread-3  ]: On model.job_intelligent.dim_location: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_location"} */
alter table "duckdb"."main_gold"."dim_location__dbt_tmp" rename to "dim_location"
[0m15:18:33.004771 [debug] [Thread-4  ]: On model.job_intelligent.dim_company: COMMIT
[0m15:18:33.010472 [debug] [Thread-1  ]: Using duckdb connection "model.job_intelligent.dim_time"
[0m15:18:33.010472 [debug] [Thread-3  ]: SQL status: OK in 0.001 seconds
[0m15:18:33.010472 [debug] [Thread-1  ]: On model.job_intelligent.dim_time: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_time"} */
alter table "duckdb"."main_gold"."dim_time__dbt_tmp" rename to "dim_time"
[0m15:18:33.019578 [debug] [Thread-3  ]: On model.job_intelligent.dim_location: COMMIT
[0m15:18:33.019578 [debug] [Thread-4  ]: SQL status: OK in 0.008 seconds
[0m15:18:33.019578 [debug] [Thread-3  ]: Using duckdb connection "model.job_intelligent.dim_location"
[0m15:18:33.019578 [debug] [Thread-1  ]: SQL status: OK in 0.002 seconds
[0m15:18:33.026709 [debug] [Thread-4  ]: Using duckdb connection "model.job_intelligent.dim_company"
[0m15:18:33.026709 [debug] [Thread-3  ]: On model.job_intelligent.dim_location: COMMIT
[0m15:18:33.030328 [debug] [Thread-1  ]: On model.job_intelligent.dim_time: COMMIT
[0m15:18:33.032367 [debug] [Thread-4  ]: On model.job_intelligent.dim_company: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_company"} */

      drop table if exists "duckdb"."main_gold"."dim_company__dbt_backup" cascade
    
[0m15:18:33.034123 [debug] [Thread-1  ]: Using duckdb connection "model.job_intelligent.dim_time"
[0m15:18:33.036138 [debug] [Thread-3  ]: SQL status: OK in 0.003 seconds
[0m15:18:33.036138 [debug] [Thread-1  ]: On model.job_intelligent.dim_time: COMMIT
[0m15:18:33.041421 [debug] [Thread-3  ]: Using duckdb connection "model.job_intelligent.dim_location"
[0m15:18:33.041421 [debug] [Thread-4  ]: SQL status: OK in 0.007 seconds
[0m15:18:33.041421 [debug] [Thread-3  ]: On model.job_intelligent.dim_location: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_location"} */

      drop table if exists "duckdb"."main_gold"."dim_location__dbt_backup" cascade
    
[0m15:18:33.046522 [debug] [Thread-4  ]: On model.job_intelligent.dim_company: Close
[0m15:18:33.046522 [debug] [Thread-1  ]: SQL status: OK in 0.004 seconds
[0m15:18:33.050150 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '677da618-2f5c-4a4a-9ba5-8a53f5d835fb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024BA84FE7C0>]}
[0m15:18:33.051162 [debug] [Thread-3  ]: SQL status: OK in 0.002 seconds
[0m15:18:33.058497 [debug] [Thread-1  ]: Using duckdb connection "model.job_intelligent.dim_time"
[0m15:18:33.061595 [info ] [Thread-4  ]: 4 of 13 OK created sql table model main_gold.dim_company ....................... [[32mOK[0m in 0.40s]
[0m15:18:33.067093 [debug] [Thread-3  ]: On model.job_intelligent.dim_location: Close
[0m15:18:33.070480 [debug] [Thread-1  ]: On model.job_intelligent.dim_time: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_time"} */

      drop table if exists "duckdb"."main_gold"."dim_time__dbt_backup" cascade
    
[0m15:18:33.073182 [debug] [Thread-4  ]: Finished running node model.job_intelligent.dim_company
[0m15:18:33.075197 [debug] [Thread-3  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '677da618-2f5c-4a4a-9ba5-8a53f5d835fb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024BA8153190>]}
[0m15:18:33.079036 [debug] [Thread-1  ]: SQL status: OK in 0.003 seconds
[0m15:18:33.079036 [info ] [Thread-3  ]: 5 of 13 OK created sql table model main_gold.dim_location ...................... [[32mOK[0m in 0.42s]
[0m15:18:33.083236 [debug] [Thread-1  ]: On model.job_intelligent.dim_time: Close
[0m15:18:33.086301 [debug] [Thread-3  ]: Finished running node model.job_intelligent.dim_location
[0m15:18:33.088293 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '677da618-2f5c-4a4a-9ba5-8a53f5d835fb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024BA7AE3460>]}
[0m15:18:33.090959 [info ] [Thread-1  ]: 6 of 13 OK created sql table model main_gold.dim_time .......................... [[32mOK[0m in 0.44s]
[0m15:18:33.092967 [debug] [Thread-1  ]: Finished running node model.job_intelligent.dim_time
[0m15:18:33.094095 [debug] [Thread-4  ]: Began running node model.job_intelligent.fact_job_offers
[0m15:18:33.095589 [info ] [Thread-4  ]: 8 of 13 START sql table model main_gold.fact_job_offers ........................ [RUN]
[0m15:18:33.098430 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly model.job_intelligent.dim_company, now model.job_intelligent.fact_job_offers)
[0m15:18:33.099952 [debug] [Thread-4  ]: Began compiling node model.job_intelligent.fact_job_offers
[0m15:18:33.114533 [debug] [Thread-4  ]: Writing injected SQL for node "model.job_intelligent.fact_job_offers"
[0m15:18:33.116537 [debug] [Thread-4  ]: Began executing node model.job_intelligent.fact_job_offers
[0m15:18:33.126985 [debug] [Thread-4  ]: Writing runtime sql for node "model.job_intelligent.fact_job_offers"
[0m15:18:33.129515 [debug] [Thread-4  ]: Using duckdb connection "model.job_intelligent.fact_job_offers"
[0m15:18:33.130669 [debug] [Thread-4  ]: On model.job_intelligent.fact_job_offers: BEGIN
[0m15:18:33.130669 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m15:18:33.130669 [debug] [Thread-4  ]: SQL status: OK in 0.001 seconds
[0m15:18:33.138469 [debug] [Thread-4  ]: Using duckdb connection "model.job_intelligent.fact_job_offers"
[0m15:18:33.139533 [debug] [Thread-4  ]: On model.job_intelligent.fact_job_offers: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.fact_job_offers"} */

  
    
    

    create  table
      "duckdb"."main_gold"."fact_job_offers__dbt_tmp"
  
    as (
      -- models/gold/fact_job_offers.sql
-- Gold layer: Fact Table - Job Offers (Schéma en Étoile)



WITH jobs AS (
    SELECT
        j.*
    FROM "duckdb"."main_silver"."int_job_title_normalization" j
),

companies AS (
    SELECT * FROM "duckdb"."main_gold"."dim_company"
),

locations AS (
    SELECT * FROM "duckdb"."main_gold"."dim_location"
),

times AS (
    SELECT * FROM "duckdb"."main_gold"."dim_time"
),

fact_table AS (
    SELECT
        -- Surrogate keys
        ROW_NUMBER() OVER (ORDER BY j.job_url, j.company_name_cleaned) as job_offer_id,
        
        -- Foreign keys
        c.company_id,
        l.location_id,
        t.date_id as published_date_id,
        
        -- Job dimensions
        j.job_title_cleaned as job_title,
        j.job_category,
        j.contract_type_normalized as contract_type,
        j.work_type_normalized as work_type,
        
        -- URLs
        j.job_url,
        j.company_url,
        
        -- Description
        j.job_description_cleaned as job_description,
        
        -- Time dimension
        j.published_date,
        j.posted_time,
        j.published_year_month,
        j.published_year,
        j.published_month,
        
        -- Metrics
        LENGTH(j.job_description_cleaned) as description_length,
        (LENGTH(j.job_description_cleaned) - LENGTH(REPLACE(j.job_description_cleaned, ' ', ''))) + 1 as word_count,
        
        -- Flags
        CASE WHEN j.work_type_normalized = 'Remote' THEN 1 ELSE 0 END as is_remote,
        CASE WHEN j.contract_type_normalized = 'Permanent' THEN 1 ELSE 0 END as is_permanent,
        
        -- Metadata
        NOW() as created_at,
        j.ingestion_timestamp
        
    FROM jobs j
    LEFT JOIN companies c ON j.company_name_cleaned = c.company_name
    LEFT JOIN locations l ON j.location_cleaned = l.location_raw
    LEFT JOIN times t ON j.published_date = t.date_id
)

SELECT
    job_offer_id,
    company_id,
    location_id,
    published_date_id,
    job_title,
    job_category,
    contract_type,
    work_type,
    job_url,
    company_url,
    job_description,
    published_date,
    posted_time,
    published_year_month,
    published_year,
    published_month,
    description_length,
    word_count,
    is_remote,
    is_permanent,
    created_at,
    ingestion_timestamp
FROM fact_table
ORDER BY published_date DESC, job_offer_id
    );
  
  
[0m15:18:33.360386 [debug] [Thread-4  ]: SQL status: OK in 0.221 seconds
[0m15:18:33.360386 [debug] [Thread-4  ]: Using duckdb connection "model.job_intelligent.fact_job_offers"
[0m15:18:33.360386 [debug] [Thread-4  ]: On model.job_intelligent.fact_job_offers: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.fact_job_offers"} */

    SELECT index_name
    FROM duckdb_indexes()
    WHERE schema_name = 'main_gold'
      AND table_name = 'fact_job_offers'
  
[0m15:18:33.369136 [debug] [Thread-4  ]: SQL status: OK in 0.003 seconds
[0m15:18:33.369136 [debug] [Thread-4  ]: Using duckdb connection "model.job_intelligent.fact_job_offers"
[0m15:18:33.369136 [debug] [Thread-4  ]: On model.job_intelligent.fact_job_offers: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.fact_job_offers"} */

    SELECT COUNT(*) as remaining_indexes
    FROM duckdb_indexes()
    WHERE schema_name = 'main_gold'
      AND table_name = 'fact_job_offers'
  
[0m15:18:33.375309 [debug] [Thread-4  ]: SQL status: OK in 0.001 seconds
[0m15:18:33.385148 [debug] [Thread-4  ]: Using duckdb connection "model.job_intelligent.fact_job_offers"
[0m15:18:33.385945 [debug] [Thread-4  ]: On model.job_intelligent.fact_job_offers: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.fact_job_offers"} */
alter table "duckdb"."main_gold"."fact_job_offers" rename to "fact_job_offers__dbt_backup"
[0m15:18:33.389073 [debug] [Thread-4  ]: SQL status: OK in 0.001 seconds
[0m15:18:33.392295 [debug] [Thread-4  ]: Using duckdb connection "model.job_intelligent.fact_job_offers"
[0m15:18:33.392295 [debug] [Thread-4  ]: On model.job_intelligent.fact_job_offers: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.fact_job_offers"} */
alter table "duckdb"."main_gold"."fact_job_offers__dbt_tmp" rename to "fact_job_offers"
[0m15:18:33.392295 [debug] [Thread-4  ]: SQL status: OK in 0.001 seconds
[0m15:18:33.402498 [debug] [Thread-4  ]: On model.job_intelligent.fact_job_offers: COMMIT
[0m15:18:33.404529 [debug] [Thread-4  ]: Using duckdb connection "model.job_intelligent.fact_job_offers"
[0m15:18:33.404529 [debug] [Thread-4  ]: On model.job_intelligent.fact_job_offers: COMMIT
[0m15:18:33.469439 [debug] [Thread-4  ]: SQL status: OK in 0.062 seconds
[0m15:18:33.476965 [debug] [Thread-4  ]: Using duckdb connection "model.job_intelligent.fact_job_offers"
[0m15:18:33.478476 [debug] [Thread-4  ]: On model.job_intelligent.fact_job_offers: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.fact_job_offers"} */

      drop table if exists "duckdb"."main_gold"."fact_job_offers__dbt_backup" cascade
    
[0m15:18:33.481645 [debug] [Thread-4  ]: SQL status: OK in 0.002 seconds
[0m15:18:33.484672 [debug] [Thread-4  ]: On model.job_intelligent.fact_job_offers: Close
[0m15:18:33.488439 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '677da618-2f5c-4a4a-9ba5-8a53f5d835fb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024BA58F3430>]}
[0m15:18:33.489957 [info ] [Thread-4  ]: 8 of 13 OK created sql table model main_gold.fact_job_offers ................... [[32mOK[0m in 0.39s]
[0m15:18:33.491967 [debug] [Thread-4  ]: Finished running node model.job_intelligent.fact_job_offers
[0m15:18:33.494967 [debug] [Thread-1  ]: Began running node model.job_intelligent.agg_job_offers_by_category_time
[0m15:18:33.495981 [debug] [Thread-3  ]: Began running node model.job_intelligent.agg_location_analysis
[0m15:18:33.496968 [info ] [Thread-1  ]: 9 of 13 START sql table model main_gold.agg_job_offers_by_category_time ........ [RUN]
[0m15:18:33.499489 [info ] [Thread-3  ]: 10 of 13 START sql table model main_gold.agg_location_analysis ................. [RUN]
[0m15:18:33.501015 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.job_intelligent.dim_time, now model.job_intelligent.agg_job_offers_by_category_time)
[0m15:18:33.505658 [debug] [Thread-3  ]: Re-using an available connection from the pool (formerly model.job_intelligent.dim_location, now model.job_intelligent.agg_location_analysis)
[0m15:18:33.510787 [debug] [Thread-1  ]: Began compiling node model.job_intelligent.agg_job_offers_by_category_time
[0m15:18:33.512199 [debug] [Thread-3  ]: Began compiling node model.job_intelligent.agg_location_analysis
[0m15:18:33.521239 [debug] [Thread-1  ]: Writing injected SQL for node "model.job_intelligent.agg_job_offers_by_category_time"
[0m15:18:33.537247 [debug] [Thread-3  ]: Writing injected SQL for node "model.job_intelligent.agg_location_analysis"
[0m15:18:33.543105 [debug] [Thread-1  ]: Began executing node model.job_intelligent.agg_job_offers_by_category_time
[0m15:18:33.552986 [debug] [Thread-3  ]: Began executing node model.job_intelligent.agg_location_analysis
[0m15:18:33.566881 [debug] [Thread-3  ]: Writing runtime sql for node "model.job_intelligent.agg_location_analysis"
[0m15:18:33.573459 [debug] [Thread-1  ]: Writing runtime sql for node "model.job_intelligent.agg_job_offers_by_category_time"
[0m15:18:33.576472 [debug] [Thread-3  ]: Using duckdb connection "model.job_intelligent.agg_location_analysis"
[0m15:18:33.577470 [debug] [Thread-3  ]: On model.job_intelligent.agg_location_analysis: BEGIN
[0m15:18:33.579491 [debug] [Thread-1  ]: Using duckdb connection "model.job_intelligent.agg_job_offers_by_category_time"
[0m15:18:33.581152 [debug] [Thread-3  ]: Opening a new connection, currently in state closed
[0m15:18:33.582697 [debug] [Thread-1  ]: On model.job_intelligent.agg_job_offers_by_category_time: BEGIN
[0m15:18:33.585702 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m15:18:33.585702 [debug] [Thread-3  ]: SQL status: OK in 0.006 seconds
[0m15:18:33.591035 [debug] [Thread-1  ]: SQL status: OK in 0.006 seconds
[0m15:18:33.593061 [debug] [Thread-3  ]: Using duckdb connection "model.job_intelligent.agg_location_analysis"
[0m15:18:33.593941 [debug] [Thread-1  ]: Using duckdb connection "model.job_intelligent.agg_job_offers_by_category_time"
[0m15:18:33.595957 [debug] [Thread-3  ]: On model.job_intelligent.agg_location_analysis: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.agg_location_analysis"} */

  
    
    

    create  table
      "duckdb"."main_gold"."agg_location_analysis__dbt_tmp"
  
    as (
      -- models/gold/agg_location_analysis.sql
-- Gold layer: Aggregate - Location Analysis



SELECT
    dl.location_id,
    dl.location_raw,
    dl.city,
    dl.country,
    dl.work_location_type,
    
    COUNT(DISTINCT f.job_offer_id) as count_job_offers,
    COUNT(DISTINCT f.company_id) as count_companies,
    
    -- Distribution by job category
    COUNT(DISTINCT CASE WHEN f.job_category = 'Data Engineer' THEN f.job_offer_id END) as data_engineer_count,
    COUNT(DISTINCT CASE WHEN f.job_category = 'Data Scientist' THEN f.job_offer_id END) as data_scientist_count,
    COUNT(DISTINCT CASE WHEN f.job_category = 'Data Analyst' THEN f.job_offer_id END) as data_analyst_count,
    COUNT(DISTINCT CASE WHEN f.job_category = 'ML Engineer' THEN f.job_offer_id END) as ml_engineer_count,
    
    -- Remote percentage
    ROUND(
        100.0 * SUM(CASE WHEN f.is_remote = 1 THEN 1 ELSE 0 END) / COUNT(DISTINCT f.job_offer_id),
        2
    ) as pct_remote,
    
    NOW() as created_at
    
FROM "duckdb"."main_gold"."dim_location" dl
LEFT JOIN "duckdb"."main_gold"."fact_job_offers" f ON dl.location_id = f.location_id
GROUP BY
    dl.location_id,
    dl.location_raw,
    dl.city,
    dl.country,
    dl.work_location_type
ORDER BY count_job_offers DESC
    );
  
  
[0m15:18:33.599520 [debug] [Thread-1  ]: On model.job_intelligent.agg_job_offers_by_category_time: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.agg_job_offers_by_category_time"} */

  
    
    

    create  table
      "duckdb"."main_gold"."agg_job_offers_by_category_time__dbt_tmp"
  
    as (
      -- models/gold/agg_job_offers_by_category_time.sql
-- Gold layer: Aggregate - Job Offers by Category and Time



SELECT
    f.published_year,
    f.published_month,
    f.published_year_month,
    f.job_category,
    f.contract_type,
    f.work_type,
    
    COUNT(DISTINCT f.job_offer_id) as count_job_offers,
    COUNT(DISTINCT f.company_id) as count_companies,
    
    AVG(f.description_length) as avg_description_length,
    AVG(f.word_count) as avg_word_count,
    
    SUM(CASE WHEN f.is_remote = 1 THEN 1 ELSE 0 END) as remote_jobs,
    SUM(CASE WHEN f.is_permanent = 1 THEN 1 ELSE 0 END) as permanent_jobs,
    
    NOW() as created_at
    
FROM "duckdb"."main_gold"."fact_job_offers" f
WHERE f.published_date IS NOT NULL
GROUP BY
    f.published_year,
    f.published_month,
    f.published_year_month,
    f.job_category,
    f.contract_type,
    f.work_type
ORDER BY f.published_year_month DESC, f.job_category
    );
  
  
[0m15:18:33.626512 [debug] [Thread-1  ]: SQL status: OK in 0.023 seconds
[0m15:18:33.631104 [debug] [Thread-1  ]: Using duckdb connection "model.job_intelligent.agg_job_offers_by_category_time"
[0m15:18:33.631104 [debug] [Thread-1  ]: On model.job_intelligent.agg_job_offers_by_category_time: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.agg_job_offers_by_category_time"} */

    SELECT index_name
    FROM duckdb_indexes()
    WHERE schema_name = 'main_gold'
      AND table_name = 'agg_job_offers_by_category_time'
  
[0m15:18:33.645473 [debug] [Thread-1  ]: SQL status: OK in 0.011 seconds
[0m15:18:33.651011 [debug] [Thread-1  ]: Using duckdb connection "model.job_intelligent.agg_job_offers_by_category_time"
[0m15:18:33.652649 [debug] [Thread-1  ]: On model.job_intelligent.agg_job_offers_by_category_time: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.agg_job_offers_by_category_time"} */

    SELECT COUNT(*) as remaining_indexes
    FROM duckdb_indexes()
    WHERE schema_name = 'main_gold'
      AND table_name = 'agg_job_offers_by_category_time'
  
[0m15:18:33.656679 [debug] [Thread-3  ]: SQL status: OK in 0.056 seconds
[0m15:18:33.659231 [debug] [Thread-1  ]: SQL status: OK in 0.005 seconds
[0m15:18:33.663709 [debug] [Thread-3  ]: Using duckdb connection "model.job_intelligent.agg_location_analysis"
[0m15:18:33.673992 [debug] [Thread-1  ]: Using duckdb connection "model.job_intelligent.agg_job_offers_by_category_time"
[0m15:18:33.676006 [debug] [Thread-3  ]: On model.job_intelligent.agg_location_analysis: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.agg_location_analysis"} */

    SELECT index_name
    FROM duckdb_indexes()
    WHERE schema_name = 'main_gold'
      AND table_name = 'agg_location_analysis'
  
[0m15:18:33.678023 [debug] [Thread-1  ]: On model.job_intelligent.agg_job_offers_by_category_time: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.agg_job_offers_by_category_time"} */
alter table "duckdb"."main_gold"."agg_job_offers_by_category_time" rename to "agg_job_offers_by_category_time__dbt_backup"
[0m15:18:33.680719 [debug] [Thread-3  ]: SQL status: OK in 0.001 seconds
[0m15:18:33.682774 [debug] [Thread-1  ]: SQL status: OK in 0.002 seconds
[0m15:18:33.685873 [debug] [Thread-3  ]: Using duckdb connection "model.job_intelligent.agg_location_analysis"
[0m15:18:33.695536 [debug] [Thread-1  ]: Using duckdb connection "model.job_intelligent.agg_job_offers_by_category_time"
[0m15:18:33.697548 [debug] [Thread-3  ]: On model.job_intelligent.agg_location_analysis: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.agg_location_analysis"} */

    SELECT COUNT(*) as remaining_indexes
    FROM duckdb_indexes()
    WHERE schema_name = 'main_gold'
      AND table_name = 'agg_location_analysis'
  
[0m15:18:33.700157 [debug] [Thread-1  ]: On model.job_intelligent.agg_job_offers_by_category_time: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.agg_job_offers_by_category_time"} */
alter table "duckdb"."main_gold"."agg_job_offers_by_category_time__dbt_tmp" rename to "agg_job_offers_by_category_time"
[0m15:18:33.702655 [debug] [Thread-3  ]: SQL status: OK in 0.002 seconds
[0m15:18:33.705817 [debug] [Thread-1  ]: SQL status: OK in 0.002 seconds
[0m15:18:33.716027 [debug] [Thread-3  ]: Using duckdb connection "model.job_intelligent.agg_location_analysis"
[0m15:18:33.722531 [debug] [Thread-1  ]: On model.job_intelligent.agg_job_offers_by_category_time: COMMIT
[0m15:18:33.724545 [debug] [Thread-3  ]: On model.job_intelligent.agg_location_analysis: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.agg_location_analysis"} */
alter table "duckdb"."main_gold"."agg_location_analysis" rename to "agg_location_analysis__dbt_backup"
[0m15:18:33.727416 [debug] [Thread-1  ]: Using duckdb connection "model.job_intelligent.agg_job_offers_by_category_time"
[0m15:18:33.731515 [debug] [Thread-3  ]: SQL status: OK in 0.001 seconds
[0m15:18:33.731515 [debug] [Thread-1  ]: On model.job_intelligent.agg_job_offers_by_category_time: COMMIT
[0m15:18:33.745527 [debug] [Thread-3  ]: Using duckdb connection "model.job_intelligent.agg_location_analysis"
[0m15:18:33.748632 [debug] [Thread-3  ]: On model.job_intelligent.agg_location_analysis: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.agg_location_analysis"} */
alter table "duckdb"."main_gold"."agg_location_analysis__dbt_tmp" rename to "agg_location_analysis"
[0m15:18:33.750152 [debug] [Thread-1  ]: SQL status: OK in 0.003 seconds
[0m15:18:33.752155 [debug] [Thread-3  ]: SQL status: OK in 0.002 seconds
[0m15:18:33.763054 [debug] [Thread-1  ]: Using duckdb connection "model.job_intelligent.agg_job_offers_by_category_time"
[0m15:18:33.769663 [debug] [Thread-3  ]: On model.job_intelligent.agg_location_analysis: COMMIT
[0m15:18:33.772676 [debug] [Thread-1  ]: On model.job_intelligent.agg_job_offers_by_category_time: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.agg_job_offers_by_category_time"} */

      drop table if exists "duckdb"."main_gold"."agg_job_offers_by_category_time__dbt_backup" cascade
    
[0m15:18:33.774125 [debug] [Thread-3  ]: Using duckdb connection "model.job_intelligent.agg_location_analysis"
[0m15:18:33.775961 [debug] [Thread-3  ]: On model.job_intelligent.agg_location_analysis: COMMIT
[0m15:18:33.779552 [debug] [Thread-1  ]: SQL status: OK in 0.004 seconds
[0m15:18:33.784039 [debug] [Thread-1  ]: On model.job_intelligent.agg_job_offers_by_category_time: Close
[0m15:18:33.786041 [debug] [Thread-3  ]: SQL status: OK in 0.008 seconds
[0m15:18:33.788313 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '677da618-2f5c-4a4a-9ba5-8a53f5d835fb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024BA7A5BF40>]}
[0m15:18:33.797293 [debug] [Thread-3  ]: Using duckdb connection "model.job_intelligent.agg_location_analysis"
[0m15:18:33.800380 [info ] [Thread-1  ]: 9 of 13 OK created sql table model main_gold.agg_job_offers_by_category_time ... [[32mOK[0m in 0.29s]
[0m15:18:33.802544 [debug] [Thread-3  ]: On model.job_intelligent.agg_location_analysis: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.agg_location_analysis"} */

      drop table if exists "duckdb"."main_gold"."agg_location_analysis__dbt_backup" cascade
    
[0m15:18:33.804586 [debug] [Thread-1  ]: Finished running node model.job_intelligent.agg_job_offers_by_category_time
[0m15:18:33.809780 [debug] [Thread-3  ]: SQL status: OK in 0.002 seconds
[0m15:18:33.813018 [debug] [Thread-3  ]: On model.job_intelligent.agg_location_analysis: Close
[0m15:18:33.814126 [debug] [Thread-3  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '677da618-2f5c-4a4a-9ba5-8a53f5d835fb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024BA84559D0>]}
[0m15:18:33.817477 [info ] [Thread-3  ]: 10 of 13 OK created sql table model main_gold.agg_location_analysis ............ [[32mOK[0m in 0.31s]
[0m15:18:33.823615 [debug] [Thread-3  ]: Finished running node model.job_intelligent.agg_location_analysis
[0m15:18:35.989209 [debug] [Thread-2  ]: SQL status: OK in 3.092 seconds
[0m15:18:35.991228 [debug] [Thread-2  ]: Using duckdb connection "model.job_intelligent.int_skills_extraction"
[0m15:18:35.993220 [debug] [Thread-2  ]: On model.job_intelligent.int_skills_extraction: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.int_skills_extraction"} */

    SELECT index_name
    FROM duckdb_indexes()
    WHERE schema_name = 'main_silver'
      AND table_name = 'int_skills_extraction'
  
[0m15:18:35.995216 [debug] [Thread-2  ]: SQL status: OK in 0.001 seconds
[0m15:18:35.997238 [debug] [Thread-2  ]: Using duckdb connection "model.job_intelligent.int_skills_extraction"
[0m15:18:35.998729 [debug] [Thread-2  ]: On model.job_intelligent.int_skills_extraction: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.int_skills_extraction"} */

    SELECT COUNT(*) as remaining_indexes
    FROM duckdb_indexes()
    WHERE schema_name = 'main_silver'
      AND table_name = 'int_skills_extraction'
  
[0m15:18:36.000888 [debug] [Thread-2  ]: SQL status: OK in 0.001 seconds
[0m15:18:36.008392 [debug] [Thread-2  ]: Using duckdb connection "model.job_intelligent.int_skills_extraction"
[0m15:18:36.010514 [debug] [Thread-2  ]: On model.job_intelligent.int_skills_extraction: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.int_skills_extraction"} */
alter table "duckdb"."main_silver"."int_skills_extraction" rename to "int_skills_extraction__dbt_backup"
[0m15:18:36.011540 [debug] [Thread-2  ]: SQL status: OK in 0.001 seconds
[0m15:18:36.018167 [debug] [Thread-2  ]: Using duckdb connection "model.job_intelligent.int_skills_extraction"
[0m15:18:36.019524 [debug] [Thread-2  ]: On model.job_intelligent.int_skills_extraction: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.int_skills_extraction"} */
alter table "duckdb"."main_silver"."int_skills_extraction__dbt_tmp" rename to "int_skills_extraction"
[0m15:18:36.021173 [debug] [Thread-2  ]: SQL status: OK in 0.001 seconds
[0m15:18:36.024198 [debug] [Thread-2  ]: On model.job_intelligent.int_skills_extraction: COMMIT
[0m15:18:36.025198 [debug] [Thread-2  ]: Using duckdb connection "model.job_intelligent.int_skills_extraction"
[0m15:18:36.026198 [debug] [Thread-2  ]: On model.job_intelligent.int_skills_extraction: COMMIT
[0m15:18:36.082626 [debug] [Thread-2  ]: SQL status: OK in 0.057 seconds
[0m15:18:36.086656 [debug] [Thread-2  ]: Using duckdb connection "model.job_intelligent.int_skills_extraction"
[0m15:18:36.089222 [debug] [Thread-2  ]: On model.job_intelligent.int_skills_extraction: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.int_skills_extraction"} */

      drop table if exists "duckdb"."main_silver"."int_skills_extraction__dbt_backup" cascade
    
[0m15:18:36.360358 [debug] [Thread-2  ]: SQL status: OK in 0.273 seconds
[0m15:18:36.360358 [debug] [Thread-2  ]: On model.job_intelligent.int_skills_extraction: Close
[0m15:18:36.368417 [debug] [Thread-2  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '677da618-2f5c-4a4a-9ba5-8a53f5d835fb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024BA7E8B880>]}
[0m15:18:36.369445 [info ] [Thread-2  ]: 7 of 13 OK created sql table model main_silver.int_skills_extraction ........... [[32mOK[0m in 3.71s]
[0m15:18:36.371974 [debug] [Thread-2  ]: Finished running node model.job_intelligent.int_skills_extraction
[0m15:18:36.373990 [debug] [Thread-4  ]: Began running node model.job_intelligent.dim_skills
[0m15:18:36.376010 [info ] [Thread-4  ]: 11 of 13 START sql table model main_gold.dim_skills ............................ [RUN]
[0m15:18:36.376010 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly model.job_intelligent.fact_job_offers, now model.job_intelligent.dim_skills)
[0m15:18:36.379174 [debug] [Thread-4  ]: Began compiling node model.job_intelligent.dim_skills
[0m15:18:36.388285 [debug] [Thread-4  ]: Writing injected SQL for node "model.job_intelligent.dim_skills"
[0m15:18:36.390762 [debug] [Thread-4  ]: Began executing node model.job_intelligent.dim_skills
[0m15:18:36.411087 [debug] [Thread-4  ]: Writing runtime sql for node "model.job_intelligent.dim_skills"
[0m15:18:36.413099 [debug] [Thread-4  ]: Using duckdb connection "model.job_intelligent.dim_skills"
[0m15:18:36.415112 [debug] [Thread-4  ]: On model.job_intelligent.dim_skills: BEGIN
[0m15:18:36.415112 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m15:18:36.422817 [debug] [Thread-4  ]: SQL status: OK in 0.004 seconds
[0m15:18:36.424412 [debug] [Thread-4  ]: Using duckdb connection "model.job_intelligent.dim_skills"
[0m15:18:36.426434 [debug] [Thread-4  ]: On model.job_intelligent.dim_skills: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_skills"} */

  
    
    

    create  table
      "duckdb"."main_gold"."dim_skills__dbt_tmp"
  
    as (
      -- models/gold/dim_skills.sql
-- Gold layer: Dimension Skills



WITH skills AS (
    SELECT DISTINCT
        skill_name
    FROM "duckdb"."main_silver"."int_skills_extraction"
    WHERE skill_name IS NOT NULL
),

skill_categorization AS (
    SELECT
        skill_name,
        CASE
            WHEN skill_name IN ('Python', 'Java', 'Scala', 'R') THEN 'Programming Language'
            WHEN skill_name IN ('SQL', 'NoSQL') THEN 'Database'
            WHEN skill_name IN ('Spark', 'Hadoop', 'Hive', 'Kafka') THEN 'Big Data Framework'
            WHEN skill_name IN ('TensorFlow', 'PyTorch', 'Scikit-learn') THEN 'ML/DL Library'
            WHEN skill_name IN ('AWS', 'Azure', 'GCP') THEN 'Cloud Platform'
            WHEN skill_name IN ('Tableau', 'Power BI', 'Looker') THEN 'BI Tool'
            WHEN skill_name IN ('Airflow', 'DBT', 'Kubernetes', 'Docker') THEN 'DataOps/DevOps'
            WHEN skill_name IN ('Pandas', 'NumPy', 'Matplotlib') THEN 'Data Analysis Library'
            WHEN skill_name IN ('Machine Learning', 'Statistics', 'Data Visualization') THEN 'Domain Knowledge'
            ELSE 'Other'
        END as skill_category
    FROM skills
),

ranked_skills AS (
    SELECT
        ROW_NUMBER() OVER (ORDER BY skill_name) as skill_id,
        skill_name,
        skill_category,
        NOW() as created_at
    FROM skill_categorization
)

SELECT
    skill_id,
    skill_name,
    skill_category,
    created_at
FROM ranked_skills
ORDER BY skill_id
    );
  
  
[0m15:18:36.435523 [debug] [Thread-4  ]: SQL status: OK in 0.008 seconds
[0m15:18:36.440704 [debug] [Thread-4  ]: Using duckdb connection "model.job_intelligent.dim_skills"
[0m15:18:36.440704 [debug] [Thread-4  ]: On model.job_intelligent.dim_skills: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_skills"} */

    SELECT index_name
    FROM duckdb_indexes()
    WHERE schema_name = 'main_gold'
      AND table_name = 'dim_skills'
  
[0m15:18:36.440704 [debug] [Thread-4  ]: SQL status: OK in 0.001 seconds
[0m15:18:36.448476 [debug] [Thread-4  ]: Using duckdb connection "model.job_intelligent.dim_skills"
[0m15:18:36.449506 [debug] [Thread-4  ]: On model.job_intelligent.dim_skills: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_skills"} */

    SELECT COUNT(*) as remaining_indexes
    FROM duckdb_indexes()
    WHERE schema_name = 'main_gold'
      AND table_name = 'dim_skills'
  
[0m15:18:36.457117 [debug] [Thread-4  ]: SQL status: OK in 0.004 seconds
[0m15:18:36.466381 [debug] [Thread-4  ]: Using duckdb connection "model.job_intelligent.dim_skills"
[0m15:18:36.470132 [debug] [Thread-4  ]: On model.job_intelligent.dim_skills: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_skills"} */
alter table "duckdb"."main_gold"."dim_skills" rename to "dim_skills__dbt_backup"
[0m15:18:36.473222 [debug] [Thread-4  ]: SQL status: OK in 0.001 seconds
[0m15:18:36.481010 [debug] [Thread-4  ]: Using duckdb connection "model.job_intelligent.dim_skills"
[0m15:18:36.482042 [debug] [Thread-4  ]: On model.job_intelligent.dim_skills: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_skills"} */
alter table "duckdb"."main_gold"."dim_skills__dbt_tmp" rename to "dim_skills"
[0m15:18:36.485427 [debug] [Thread-4  ]: SQL status: OK in 0.002 seconds
[0m15:18:36.493103 [debug] [Thread-4  ]: On model.job_intelligent.dim_skills: COMMIT
[0m15:18:36.494767 [debug] [Thread-4  ]: Using duckdb connection "model.job_intelligent.dim_skills"
[0m15:18:36.496910 [debug] [Thread-4  ]: On model.job_intelligent.dim_skills: COMMIT
[0m15:18:36.502506 [debug] [Thread-4  ]: SQL status: OK in 0.004 seconds
[0m15:18:36.511731 [debug] [Thread-4  ]: Using duckdb connection "model.job_intelligent.dim_skills"
[0m15:18:36.513731 [debug] [Thread-4  ]: On model.job_intelligent.dim_skills: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_skills"} */

      drop table if exists "duckdb"."main_gold"."dim_skills__dbt_backup" cascade
    
[0m15:18:36.517859 [debug] [Thread-4  ]: SQL status: OK in 0.002 seconds
[0m15:18:36.523927 [debug] [Thread-4  ]: On model.job_intelligent.dim_skills: Close
[0m15:18:36.527276 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '677da618-2f5c-4a4a-9ba5-8a53f5d835fb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024BA9604C10>]}
[0m15:18:36.530038 [info ] [Thread-4  ]: 11 of 13 OK created sql table model main_gold.dim_skills ....................... [[32mOK[0m in 0.15s]
[0m15:18:36.534701 [debug] [Thread-4  ]: Finished running node model.job_intelligent.dim_skills
[0m15:18:36.538625 [debug] [Thread-3  ]: Began running node model.job_intelligent.fact_job_skills
[0m15:18:36.541217 [info ] [Thread-3  ]: 12 of 13 START sql table model main_gold.fact_job_skills ....................... [RUN]
[0m15:18:36.544259 [debug] [Thread-3  ]: Re-using an available connection from the pool (formerly model.job_intelligent.agg_location_analysis, now model.job_intelligent.fact_job_skills)
[0m15:18:36.546282 [debug] [Thread-3  ]: Began compiling node model.job_intelligent.fact_job_skills
[0m15:18:36.560938 [debug] [Thread-3  ]: Writing injected SQL for node "model.job_intelligent.fact_job_skills"
[0m15:18:36.563807 [debug] [Thread-3  ]: Began executing node model.job_intelligent.fact_job_skills
[0m15:18:36.576334 [debug] [Thread-3  ]: Writing runtime sql for node "model.job_intelligent.fact_job_skills"
[0m15:18:36.579376 [debug] [Thread-3  ]: Using duckdb connection "model.job_intelligent.fact_job_skills"
[0m15:18:36.579376 [debug] [Thread-3  ]: On model.job_intelligent.fact_job_skills: BEGIN
[0m15:18:36.579376 [debug] [Thread-3  ]: Opening a new connection, currently in state closed
[0m15:18:36.585041 [debug] [Thread-3  ]: SQL status: OK in 0.002 seconds
[0m15:18:36.587065 [debug] [Thread-3  ]: Using duckdb connection "model.job_intelligent.fact_job_skills"
[0m15:18:36.590028 [debug] [Thread-3  ]: On model.job_intelligent.fact_job_skills: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.fact_job_skills"} */

  
    
    

    create  table
      "duckdb"."main_gold"."fact_job_skills__dbt_tmp"
  
    as (
      -- models/gold/fact_job_skills.sql
-- Gold layer: Bridge Table - Job Skills



WITH skills_raw AS (
    SELECT DISTINCT
        job_url,
        company_name_cleaned,
        skill_name
    FROM "duckdb"."main_silver"."int_skills_extraction"
),

jobs AS (
    SELECT
        job_offer_id,
        job_url
    FROM "duckdb"."main_gold"."fact_job_offers"
),

skills_dim AS (
    SELECT
        skill_id,
        skill_name
    FROM "duckdb"."main_gold"."dim_skills"
),

fact_table AS (
    SELECT
        ROW_NUMBER() OVER (ORDER BY s.job_url, sd.skill_id) as job_skill_id,
        j.job_offer_id,
        sd.skill_id,
        s.skill_name,
        NOW() as created_at
    FROM skills_raw s
    LEFT JOIN jobs j ON s.job_url = j.job_url
    LEFT JOIN skills_dim sd ON s.skill_name = sd.skill_name
)

SELECT
    job_skill_id,
    job_offer_id,
    skill_id,
    skill_name,
    created_at
FROM fact_table
WHERE job_offer_id IS NOT NULL
ORDER BY job_offer_id, skill_id
    );
  
  
[0m15:18:36.618358 [debug] [Thread-3  ]: SQL status: OK in 0.027 seconds
[0m15:18:36.622656 [debug] [Thread-3  ]: Using duckdb connection "model.job_intelligent.fact_job_skills"
[0m15:18:36.624678 [debug] [Thread-3  ]: On model.job_intelligent.fact_job_skills: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.fact_job_skills"} */

    SELECT index_name
    FROM duckdb_indexes()
    WHERE schema_name = 'main_gold'
      AND table_name = 'fact_job_skills'
  
[0m15:18:36.626923 [debug] [Thread-3  ]: SQL status: OK in 0.001 seconds
[0m15:18:36.631057 [debug] [Thread-3  ]: Using duckdb connection "model.job_intelligent.fact_job_skills"
[0m15:18:36.633111 [debug] [Thread-3  ]: On model.job_intelligent.fact_job_skills: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.fact_job_skills"} */

    SELECT COUNT(*) as remaining_indexes
    FROM duckdb_indexes()
    WHERE schema_name = 'main_gold'
      AND table_name = 'fact_job_skills'
  
[0m15:18:36.637106 [debug] [Thread-3  ]: SQL status: OK in 0.002 seconds
[0m15:18:36.648327 [debug] [Thread-3  ]: Using duckdb connection "model.job_intelligent.fact_job_skills"
[0m15:18:36.649849 [debug] [Thread-3  ]: On model.job_intelligent.fact_job_skills: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.fact_job_skills"} */
alter table "duckdb"."main_gold"."fact_job_skills" rename to "fact_job_skills__dbt_backup"
[0m15:18:36.653071 [debug] [Thread-3  ]: SQL status: OK in 0.001 seconds
[0m15:18:36.664153 [debug] [Thread-3  ]: Using duckdb connection "model.job_intelligent.fact_job_skills"
[0m15:18:36.664153 [debug] [Thread-3  ]: On model.job_intelligent.fact_job_skills: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.fact_job_skills"} */
alter table "duckdb"."main_gold"."fact_job_skills__dbt_tmp" rename to "fact_job_skills"
[0m15:18:36.666176 [debug] [Thread-3  ]: SQL status: OK in 0.001 seconds
[0m15:18:36.670536 [debug] [Thread-3  ]: On model.job_intelligent.fact_job_skills: COMMIT
[0m15:18:36.673550 [debug] [Thread-3  ]: Using duckdb connection "model.job_intelligent.fact_job_skills"
[0m15:18:36.674455 [debug] [Thread-3  ]: On model.job_intelligent.fact_job_skills: COMMIT
[0m15:18:36.679056 [debug] [Thread-3  ]: SQL status: OK in 0.004 seconds
[0m15:18:36.684183 [debug] [Thread-3  ]: Using duckdb connection "model.job_intelligent.fact_job_skills"
[0m15:18:36.686208 [debug] [Thread-3  ]: On model.job_intelligent.fact_job_skills: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.fact_job_skills"} */

      drop table if exists "duckdb"."main_gold"."fact_job_skills__dbt_backup" cascade
    
[0m15:18:36.689325 [debug] [Thread-3  ]: SQL status: OK in 0.002 seconds
[0m15:18:36.692546 [debug] [Thread-3  ]: On model.job_intelligent.fact_job_skills: Close
[0m15:18:36.692546 [debug] [Thread-3  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '677da618-2f5c-4a4a-9ba5-8a53f5d835fb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024BA7E86730>]}
[0m15:18:36.694573 [info ] [Thread-3  ]: 12 of 13 OK created sql table model main_gold.fact_job_skills .................. [[32mOK[0m in 0.15s]
[0m15:18:36.696597 [debug] [Thread-3  ]: Finished running node model.job_intelligent.fact_job_skills
[0m15:18:36.698624 [debug] [Thread-1  ]: Began running node model.job_intelligent.agg_skills_demand
[0m15:18:36.699335 [info ] [Thread-1  ]: 13 of 13 START sql table model main_gold.agg_skills_demand ..................... [RUN]
[0m15:18:36.703142 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.job_intelligent.agg_job_offers_by_category_time, now model.job_intelligent.agg_skills_demand)
[0m15:18:36.704357 [debug] [Thread-1  ]: Began compiling node model.job_intelligent.agg_skills_demand
[0m15:18:36.711032 [debug] [Thread-1  ]: Writing injected SQL for node "model.job_intelligent.agg_skills_demand"
[0m15:18:36.711032 [debug] [Thread-1  ]: Began executing node model.job_intelligent.agg_skills_demand
[0m15:18:36.728697 [debug] [Thread-1  ]: Writing runtime sql for node "model.job_intelligent.agg_skills_demand"
[0m15:18:36.732422 [debug] [Thread-1  ]: Using duckdb connection "model.job_intelligent.agg_skills_demand"
[0m15:18:36.736766 [debug] [Thread-1  ]: On model.job_intelligent.agg_skills_demand: BEGIN
[0m15:18:36.739186 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m15:18:36.743233 [debug] [Thread-1  ]: SQL status: OK in 0.002 seconds
[0m15:18:36.743233 [debug] [Thread-1  ]: Using duckdb connection "model.job_intelligent.agg_skills_demand"
[0m15:18:36.745248 [debug] [Thread-1  ]: On model.job_intelligent.agg_skills_demand: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.agg_skills_demand"} */

  
    
    

    create  table
      "duckdb"."main_gold"."agg_skills_demand__dbt_tmp"
  
    as (
      -- models/gold/agg_skills_demand.sql
-- Gold layer: Aggregate - Skills in Demand



SELECT
    sd.skill_id,
    sd.skill_name,
    sd.skill_category,
    
    COUNT(DISTINCT fs.job_offer_id) as count_jobs_requiring_skill,
    COUNT(DISTINCT f.company_id) as count_companies_requiring_skill,
    
    -- Percentage of all jobs
    ROUND(
        100.0 * COUNT(DISTINCT fs.job_offer_id) / (
            SELECT COUNT(DISTINCT job_offer_id) FROM "duckdb"."main_gold"."fact_job_offers"
        ),
        2
    ) as pct_of_total_jobs,
    
    -- Average job details for jobs requiring this skill
    AVG(f.description_length) as avg_description_length,
    AVG(f.word_count) as avg_word_count,
    
    NOW() as created_at
    
FROM "duckdb"."main_gold"."dim_skills" sd
LEFT JOIN "duckdb"."main_gold"."fact_job_skills" fs ON sd.skill_id = fs.skill_id
LEFT JOIN "duckdb"."main_gold"."fact_job_offers" f ON fs.job_offer_id = f.job_offer_id
GROUP BY
    sd.skill_id,
    sd.skill_name,
    sd.skill_category
ORDER BY count_jobs_requiring_skill DESC
    );
  
  
[0m15:18:36.776382 [debug] [Thread-1  ]: SQL status: OK in 0.028 seconds
[0m15:18:36.782876 [debug] [Thread-1  ]: Using duckdb connection "model.job_intelligent.agg_skills_demand"
[0m15:18:36.785879 [debug] [Thread-1  ]: On model.job_intelligent.agg_skills_demand: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.agg_skills_demand"} */

    SELECT index_name
    FROM duckdb_indexes()
    WHERE schema_name = 'main_gold'
      AND table_name = 'agg_skills_demand'
  
[0m15:18:36.789918 [debug] [Thread-1  ]: SQL status: OK in 0.001 seconds
[0m15:18:36.794463 [debug] [Thread-1  ]: Using duckdb connection "model.job_intelligent.agg_skills_demand"
[0m15:18:36.796659 [debug] [Thread-1  ]: On model.job_intelligent.agg_skills_demand: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.agg_skills_demand"} */

    SELECT COUNT(*) as remaining_indexes
    FROM duckdb_indexes()
    WHERE schema_name = 'main_gold'
      AND table_name = 'agg_skills_demand'
  
[0m15:18:36.802106 [debug] [Thread-1  ]: SQL status: OK in 0.003 seconds
[0m15:18:36.813080 [debug] [Thread-1  ]: Using duckdb connection "model.job_intelligent.agg_skills_demand"
[0m15:18:36.814077 [debug] [Thread-1  ]: On model.job_intelligent.agg_skills_demand: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.agg_skills_demand"} */
alter table "duckdb"."main_gold"."agg_skills_demand" rename to "agg_skills_demand__dbt_backup"
[0m15:18:36.817078 [debug] [Thread-1  ]: SQL status: OK in 0.001 seconds
[0m15:18:36.819180 [debug] [Thread-1  ]: Using duckdb connection "model.job_intelligent.agg_skills_demand"
[0m15:18:36.828610 [debug] [Thread-1  ]: On model.job_intelligent.agg_skills_demand: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.agg_skills_demand"} */
alter table "duckdb"."main_gold"."agg_skills_demand__dbt_tmp" rename to "agg_skills_demand"
[0m15:18:36.830418 [debug] [Thread-1  ]: SQL status: OK in 0.001 seconds
[0m15:18:36.837686 [debug] [Thread-1  ]: On model.job_intelligent.agg_skills_demand: COMMIT
[0m15:18:36.840244 [debug] [Thread-1  ]: Using duckdb connection "model.job_intelligent.agg_skills_demand"
[0m15:18:36.842256 [debug] [Thread-1  ]: On model.job_intelligent.agg_skills_demand: COMMIT
[0m15:18:36.846447 [debug] [Thread-1  ]: SQL status: OK in 0.003 seconds
[0m15:18:36.855810 [debug] [Thread-1  ]: Using duckdb connection "model.job_intelligent.agg_skills_demand"
[0m15:18:36.856810 [debug] [Thread-1  ]: On model.job_intelligent.agg_skills_demand: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.agg_skills_demand"} */

      drop table if exists "duckdb"."main_gold"."agg_skills_demand__dbt_backup" cascade
    
[0m15:18:36.863928 [debug] [Thread-1  ]: SQL status: OK in 0.002 seconds
[0m15:18:36.866925 [debug] [Thread-1  ]: On model.job_intelligent.agg_skills_demand: Close
[0m15:18:36.870421 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '677da618-2f5c-4a4a-9ba5-8a53f5d835fb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024BA85C4580>]}
[0m15:18:36.873214 [info ] [Thread-1  ]: 13 of 13 OK created sql table model main_gold.agg_skills_demand ................ [[32mOK[0m in 0.17s]
[0m15:18:36.875948 [debug] [Thread-1  ]: Finished running node model.job_intelligent.agg_skills_demand
[0m15:18:36.879449 [debug] [MainThread]: Using duckdb connection "master"
[0m15:18:36.881488 [debug] [MainThread]: On master: BEGIN
[0m15:18:36.883515 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m15:18:36.886275 [debug] [MainThread]: SQL status: OK in 0.003 seconds
[0m15:18:36.889053 [debug] [MainThread]: On master: COMMIT
[0m15:18:36.892069 [debug] [MainThread]: Using duckdb connection "master"
[0m15:18:36.892069 [debug] [MainThread]: On master: COMMIT
[0m15:18:36.892069 [debug] [MainThread]: SQL status: OK in 0.001 seconds
[0m15:18:36.896852 [debug] [MainThread]: On master: Close
[0m15:18:36.898371 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:18:36.899429 [debug] [MainThread]: Connection 'create_duckdb_main_gold' was properly closed.
[0m15:18:36.901446 [debug] [MainThread]: Connection 'create_duckdb_main_bronze' was properly closed.
[0m15:18:36.903471 [debug] [MainThread]: Connection 'create_duckdb_main_silver' was properly closed.
[0m15:18:36.905633 [debug] [MainThread]: Connection 'list_duckdb_main_bronze' was properly closed.
[0m15:18:36.907644 [debug] [MainThread]: Connection 'list_duckdb_main_silver' was properly closed.
[0m15:18:36.909229 [debug] [MainThread]: Connection 'list_duckdb_main_gold' was properly closed.
[0m15:18:36.912542 [debug] [MainThread]: Connection 'model.job_intelligent.agg_skills_demand' was properly closed.
[0m15:18:36.914555 [debug] [MainThread]: Connection 'model.job_intelligent.fact_job_skills' was properly closed.
[0m15:18:36.916569 [debug] [MainThread]: Connection 'model.job_intelligent.int_skills_extraction' was properly closed.
[0m15:18:36.918646 [debug] [MainThread]: Connection 'model.job_intelligent.dim_skills' was properly closed.
[0m15:18:36.922292 [info ] [MainThread]: 
[0m15:18:36.926295 [info ] [MainThread]: Finished running 12 table models, 1 view model in 0 hours 0 minutes and 6.29 seconds (6.29s).
[0m15:18:36.939493 [debug] [MainThread]: Command end result
[0m15:18:37.016256 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Ayoub Gorry\Desktop\powerbi\jobs-power-bi\dbt_project\target\manifest.json
[0m15:18:37.019430 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Ayoub Gorry\Desktop\powerbi\jobs-power-bi\dbt_project\target\semantic_manifest.json
[0m15:18:37.038329 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\Ayoub Gorry\Desktop\powerbi\jobs-power-bi\dbt_project\target\run_results.json
[0m15:18:37.039387 [info ] [MainThread]: 
[0m15:18:37.039387 [info ] [MainThread]: [32mCompleted successfully[0m
[0m15:18:37.039387 [info ] [MainThread]: 
[0m15:18:37.039387 [info ] [MainThread]: Done. PASS=13 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=13
[0m15:18:37.048971 [debug] [MainThread]: Command `dbt run` succeeded at 15:18:37.048471 after 9.00 seconds
[0m15:18:37.050537 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024BA5542070>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024BA7A326D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024BA5F448E0>]}
[0m15:18:37.052425 [debug] [MainThread]: Flushing usage events
[0m15:18:37.839489 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:18:56.942162 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AD1B212160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AD1C5083D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AD1C508220>]}


============================== 15:18:56.949862 | 7082eda0-6862-4032-8d17-e744a28bc542 ==============================
[0m15:18:56.949862 [info ] [MainThread]: Running with dbt=1.10.18
[0m15:18:56.952880 [debug] [MainThread]: running dbt with arguments {'log_path': 'C:\\Users\\Ayoub Gorry\\Desktop\\powerbi\\jobs-power-bi\\dbt_project\\logs', 'log_format': 'default', 'partial_parse': 'True', 'version_check': 'True', 'empty': 'None', 'log_cache_events': 'False', 'cache_selected_only': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'printer_width': '80', 'send_anonymous_usage_stats': 'True', 'debug': 'False', 'introspect': 'True', 'invocation_command': 'dbt debug', 'no_print': 'None', 'profiles_dir': 'C:\\Users\\Ayoub Gorry\\Desktop\\powerbi\\jobs-power-bi\\dbt_project', 'use_experimental_parser': 'False', 'static_parser': 'True', 'use_colors': 'True', 'quiet': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'write_json': 'True', 'indirect_selection': 'eager'}
[0m15:18:56.992245 [info ] [MainThread]: dbt version: 1.10.18
[0m15:18:56.993242 [info ] [MainThread]: python version: 3.9.13
[0m15:18:56.994236 [info ] [MainThread]: python path: C:\Users\Ayoub Gorry\AppData\Local\Programs\Python\Python39\python.exe
[0m15:18:56.996230 [info ] [MainThread]: os info: Windows-10-10.0.26100-SP0
[0m15:18:57.206193 [info ] [MainThread]: Using profiles dir at C:\Users\Ayoub Gorry\Desktop\powerbi\jobs-power-bi\dbt_project
[0m15:18:57.208707 [info ] [MainThread]: Using profiles.yml file at C:\Users\Ayoub Gorry\Desktop\powerbi\jobs-power-bi\dbt_project\profiles.yml
[0m15:18:57.209248 [info ] [MainThread]: Using dbt_project.yml file at C:\Users\Ayoub Gorry\Desktop\powerbi\jobs-power-bi\dbt_project\dbt_project.yml
[0m15:18:57.210435 [info ] [MainThread]: adapter type: duckdb
[0m15:18:57.210435 [info ] [MainThread]: adapter version: 1.10.0
[0m15:18:57.624038 [info ] [MainThread]: Configuration:
[0m15:18:57.626036 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m15:18:57.627037 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m15:18:57.628547 [info ] [MainThread]: Required dependencies:
[0m15:18:57.630097 [debug] [MainThread]: Executing "git --help"
[0m15:18:57.728533 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--no-lazy-fetch]\n           [--no-optional-locks] [--no-advice] [--bare] [--git-dir=<path>]\n           [--work-tree=<path>] [--namespace=<name>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone      Clone a repository into a new directory\n   init       Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add        Add file contents to the index\n   mv         Move or rename a file, a directory, or a symlink\n   restore    Restore working tree files\n   rm         Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect     Use binary search to find the commit that introduced a bug\n   diff       Show changes between commits, commit and working tree, etc\n   grep       Print lines matching a pattern\n   log        Show commit logs\n   show       Show various types of objects\n   status     Show the working tree status\n\ngrow, mark and tweak your common history\n   backfill   Download missing objects in a partial clone\n   branch     List, create, or delete branches\n   commit     Record changes to the repository\n   merge      Join two or more development histories together\n   rebase     Reapply commits on top of another base tip\n   reset      Reset current HEAD to the specified state\n   switch     Switch branches\n   tag        Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch      Download objects and refs from another repository\n   pull       Fetch from and integrate with another repository or a local branch\n   push       Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m15:18:57.730092 [debug] [MainThread]: STDERR: "b''"
[0m15:18:57.732457 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m15:18:57.734491 [info ] [MainThread]: Connection:
[0m15:18:57.737476 [info ] [MainThread]:   database: duckdb
[0m15:18:57.739501 [info ] [MainThread]:   schema: main
[0m15:18:57.741178 [info ] [MainThread]:   path: duckdb.db
[0m15:18:57.742193 [info ] [MainThread]:   config_options: None
[0m15:18:57.744202 [info ] [MainThread]:   extensions: None
[0m15:18:57.746200 [info ] [MainThread]:   settings: {}
[0m15:18:57.748705 [info ] [MainThread]:   external_root: .
[0m15:18:57.751346 [info ] [MainThread]:   use_credential_provider: None
[0m15:18:57.753345 [info ] [MainThread]:   attach: None
[0m15:18:57.755347 [info ] [MainThread]:   filesystems: None
[0m15:18:57.757341 [info ] [MainThread]:   remote: None
[0m15:18:57.759370 [info ] [MainThread]:   plugins: None
[0m15:18:57.761793 [info ] [MainThread]:   disable_transactions: False
[0m15:18:57.764524 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m15:18:58.570680 [debug] [MainThread]: Acquiring new duckdb connection 'debug'
[0m15:18:58.872978 [debug] [MainThread]: Using duckdb connection "debug"
[0m15:18:58.876536 [debug] [MainThread]: On debug: select 1 as id
[0m15:18:58.878555 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:18:58.990241 [debug] [MainThread]: SQL status: OK in 0.111 seconds
[0m15:18:58.992256 [debug] [MainThread]: On debug: Close
[0m15:18:58.993261 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m15:18:58.994258 [info ] [MainThread]: [32mAll checks passed![0m
[0m15:18:58.997258 [debug] [MainThread]: Command `dbt debug` succeeded at 15:18:58.997258 after 2.26 seconds
[0m15:18:58.999364 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m15:18:59.000733 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AD1B212160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AD1D6BA5B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AD1C62AA30>]}
[0m15:18:59.003423 [debug] [MainThread]: Flushing usage events
[0m15:18:59.859484 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:19:03.998626 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000232C96C2190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000232CA9B7370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000232CA9B71C0>]}


============================== 15:19:04.005208 | 132c6645-99b1-4a1d-836f-a6ab29263d84 ==============================
[0m15:19:04.005208 [info ] [MainThread]: Running with dbt=1.10.18
[0m15:19:04.006208 [debug] [MainThread]: running dbt with arguments {'log_path': 'C:\\Users\\Ayoub Gorry\\Desktop\\powerbi\\jobs-power-bi\\dbt_project\\logs', 'log_format': 'default', 'partial_parse': 'True', 'version_check': 'True', 'empty': 'False', 'log_cache_events': 'False', 'cache_selected_only': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'no_print': 'None', 'send_anonymous_usage_stats': 'True', 'debug': 'False', 'target_path': 'None', 'introspect': 'True', 'invocation_command': 'dbt run', 'use_experimental_parser': 'False', 'printer_width': '80', 'profiles_dir': 'C:\\Users\\Ayoub Gorry\\Desktop\\powerbi\\jobs-power-bi\\dbt_project', 'static_parser': 'True', 'quiet': 'False', 'use_colors': 'True', 'fail_fast': 'False', 'warn_error': 'None', 'write_json': 'True', 'indirect_selection': 'eager'}
[0m15:19:04.471099 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '132c6645-99b1-4a1d-836f-a6ab29263d84', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000232CA14CA30>]}
[0m15:19:04.588506 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '132c6645-99b1-4a1d-836f-a6ab29263d84', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000232CAA85280>]}
[0m15:19:04.591588 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m15:19:04.956330 [debug] [MainThread]: checksum: 9646db7d1a5f5b9389d9c545d3ebf898b26bc1f63f6b5366f34a6af01d52cb91, vars: {}, profile: , target: , version: 1.10.18
[0m15:19:05.211587 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:19:05.211587 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:19:05.219456 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- seeds
[0m15:19:05.250501 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '132c6645-99b1-4a1d-836f-a6ab29263d84', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000232CBFEC130>]}
[0m15:19:05.349399 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Ayoub Gorry\Desktop\powerbi\jobs-power-bi\dbt_project\target\manifest.json
[0m15:19:05.349399 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Ayoub Gorry\Desktop\powerbi\jobs-power-bi\dbt_project\target\semantic_manifest.json
[0m15:19:05.400363 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '132c6645-99b1-4a1d-836f-a6ab29263d84', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000232CC0130D0>]}
[0m15:19:05.400363 [info ] [MainThread]: Found 13 models, 456 macros
[0m15:19:05.402197 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '132c6645-99b1-4a1d-836f-a6ab29263d84', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000232CBFC4070>]}
[0m15:19:05.404213 [info ] [MainThread]: 
[0m15:19:05.404213 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:19:05.406227 [info ] [MainThread]: 
[0m15:19:05.406227 [debug] [MainThread]: Acquiring new duckdb connection 'master'
[0m15:19:05.413712 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_duckdb'
[0m15:19:05.413712 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_duckdb'
[0m15:19:05.433636 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_duckdb'
[0m15:19:05.519660 [debug] [ThreadPool]: Using duckdb connection "list_duckdb"
[0m15:19:05.520849 [debug] [ThreadPool]: Using duckdb connection "list_duckdb"
[0m15:19:05.520849 [debug] [ThreadPool]: Using duckdb connection "list_duckdb"
[0m15:19:05.520849 [debug] [ThreadPool]: On list_duckdb: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_duckdb"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"duckdb"'
    
  
  
[0m15:19:05.520849 [debug] [ThreadPool]: On list_duckdb: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_duckdb"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"duckdb"'
    
  
  
[0m15:19:05.520849 [debug] [ThreadPool]: On list_duckdb: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_duckdb"} */

    
    select schema_name
    from system.information_schema.schemata
    
    where lower(catalog_name) = '"duckdb"'
    
  
  
[0m15:19:05.520849 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:19:05.520849 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:19:05.520849 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:19:05.569391 [debug] [ThreadPool]: SQL status: OK in 0.043 seconds
[0m15:19:05.569391 [debug] [ThreadPool]: SQL status: OK in 0.045 seconds
[0m15:19:05.569391 [debug] [ThreadPool]: SQL status: OK in 0.045 seconds
[0m15:19:05.569391 [debug] [ThreadPool]: On list_duckdb: Close
[0m15:19:05.569391 [debug] [ThreadPool]: On list_duckdb: Close
[0m15:19:05.569391 [debug] [ThreadPool]: On list_duckdb: Close
[0m15:19:05.578642 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_duckdb, now create_duckdb_main_silver)
[0m15:19:05.579226 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_duckdb, now create_duckdb_main_gold)
[0m15:19:05.580413 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_duckdb, now create_duckdb_main_bronze)
[0m15:19:05.580413 [debug] [ThreadPool]: Creating schema "database: "duckdb"
schema: "main_silver"
"
[0m15:19:05.582438 [debug] [ThreadPool]: Creating schema "database: "duckdb"
schema: "main_gold"
"
[0m15:19:05.584778 [debug] [ThreadPool]: Creating schema "database: "duckdb"
schema: "main_bronze"
"
[0m15:19:05.593599 [debug] [ThreadPool]: Using duckdb connection "create_duckdb_main_silver"
[0m15:19:05.596625 [debug] [ThreadPool]: Using duckdb connection "create_duckdb_main_gold"
[0m15:19:05.600507 [debug] [ThreadPool]: Using duckdb connection "create_duckdb_main_bronze"
[0m15:19:05.601518 [debug] [ThreadPool]: On create_duckdb_main_silver: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_duckdb_main_silver"} */

    
        select type from duckdb_databases()
        where lower(database_name)='duckdb'
        and type='sqlite'
    
  
[0m15:19:05.602598 [debug] [ThreadPool]: On create_duckdb_main_gold: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_duckdb_main_gold"} */

    
        select type from duckdb_databases()
        where lower(database_name)='duckdb'
        and type='sqlite'
    
  
[0m15:19:05.603580 [debug] [ThreadPool]: On create_duckdb_main_bronze: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_duckdb_main_bronze"} */

    
        select type from duckdb_databases()
        where lower(database_name)='duckdb'
        and type='sqlite'
    
  
[0m15:19:05.604515 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:19:05.604515 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:19:05.605583 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:19:05.607515 [debug] [ThreadPool]: SQL status: OK in 0.004 seconds
[0m15:19:05.609071 [debug] [ThreadPool]: SQL status: OK in 0.004 seconds
[0m15:19:05.610123 [debug] [ThreadPool]: SQL status: OK in 0.004 seconds
[0m15:19:05.612080 [debug] [ThreadPool]: Using duckdb connection "create_duckdb_main_silver"
[0m15:19:05.615468 [debug] [ThreadPool]: Using duckdb connection "create_duckdb_main_gold"
[0m15:19:05.615468 [debug] [ThreadPool]: Using duckdb connection "create_duckdb_main_bronze"
[0m15:19:05.615468 [debug] [ThreadPool]: On create_duckdb_main_silver: BEGIN
[0m15:19:05.619223 [debug] [ThreadPool]: On create_duckdb_main_gold: BEGIN
[0m15:19:05.619223 [debug] [ThreadPool]: On create_duckdb_main_bronze: BEGIN
[0m15:19:05.620434 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:19:05.620434 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m15:19:05.620434 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m15:19:05.620434 [debug] [ThreadPool]: Using duckdb connection "create_duckdb_main_silver"
[0m15:19:05.625652 [debug] [ThreadPool]: Using duckdb connection "create_duckdb_main_gold"
[0m15:19:05.625652 [debug] [ThreadPool]: Using duckdb connection "create_duckdb_main_bronze"
[0m15:19:05.625652 [debug] [ThreadPool]: On create_duckdb_main_silver: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_duckdb_main_silver"} */

    
    
        create schema if not exists "duckdb"."main_silver"
    
[0m15:19:05.627666 [debug] [ThreadPool]: On create_duckdb_main_gold: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_duckdb_main_gold"} */

    
    
        create schema if not exists "duckdb"."main_gold"
    
[0m15:19:05.628677 [debug] [ThreadPool]: On create_duckdb_main_bronze: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "create_duckdb_main_bronze"} */

    
    
        create schema if not exists "duckdb"."main_bronze"
    
[0m15:19:05.629216 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m15:19:05.630970 [debug] [ThreadPool]: SQL status: OK in 0.002 seconds
[0m15:19:05.630970 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m15:19:05.630970 [debug] [ThreadPool]: On create_duckdb_main_silver: COMMIT
[0m15:19:05.636866 [debug] [ThreadPool]: On create_duckdb_main_gold: COMMIT
[0m15:19:05.636866 [debug] [ThreadPool]: On create_duckdb_main_bronze: COMMIT
[0m15:19:05.638487 [debug] [ThreadPool]: Using duckdb connection "create_duckdb_main_silver"
[0m15:19:05.639506 [debug] [ThreadPool]: Using duckdb connection "create_duckdb_main_gold"
[0m15:19:05.640643 [debug] [ThreadPool]: Using duckdb connection "create_duckdb_main_bronze"
[0m15:19:05.640643 [debug] [ThreadPool]: On create_duckdb_main_silver: COMMIT
[0m15:19:05.640643 [debug] [ThreadPool]: On create_duckdb_main_gold: COMMIT
[0m15:19:05.640643 [debug] [ThreadPool]: On create_duckdb_main_bronze: COMMIT
[0m15:19:05.640643 [debug] [ThreadPool]: SQL status: OK in 0.000 seconds
[0m15:19:05.640643 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m15:19:05.646676 [debug] [ThreadPool]: SQL status: OK in 0.001 seconds
[0m15:19:05.646676 [debug] [ThreadPool]: On create_duckdb_main_silver: Close
[0m15:19:05.649405 [debug] [ThreadPool]: On create_duckdb_main_gold: Close
[0m15:19:05.649405 [debug] [ThreadPool]: On create_duckdb_main_bronze: Close
[0m15:19:05.654356 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_duckdb_main_silver'
[0m15:19:05.662465 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_duckdb_main_gold'
[0m15:19:05.665775 [debug] [ThreadPool]: Using duckdb connection "list_duckdb_main_silver"
[0m15:19:05.665775 [debug] [ThreadPool]: Acquiring new duckdb connection 'list_duckdb_main_bronze'
[0m15:19:05.669065 [debug] [ThreadPool]: Using duckdb connection "list_duckdb_main_gold"
[0m15:19:05.669065 [debug] [ThreadPool]: On list_duckdb_main_silver: BEGIN
[0m15:19:05.669065 [debug] [ThreadPool]: Using duckdb connection "list_duckdb_main_bronze"
[0m15:19:05.679141 [debug] [ThreadPool]: On list_duckdb_main_gold: BEGIN
[0m15:19:05.680288 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:19:05.681192 [debug] [ThreadPool]: On list_duckdb_main_bronze: BEGIN
[0m15:19:05.683205 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:19:05.683205 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:19:05.683205 [debug] [ThreadPool]: SQL status: OK in 0.005 seconds
[0m15:19:05.683205 [debug] [ThreadPool]: SQL status: OK in 0.004 seconds
[0m15:19:05.689442 [debug] [ThreadPool]: Using duckdb connection "list_duckdb_main_silver"
[0m15:19:05.689442 [debug] [ThreadPool]: SQL status: OK in 0.005 seconds
[0m15:19:05.689442 [debug] [ThreadPool]: Using duckdb connection "list_duckdb_main_gold"
[0m15:19:05.689442 [debug] [ThreadPool]: On list_duckdb_main_silver: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_duckdb_main_silver"} */
select
      'duckdb' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main_silver'
    and lower(table_catalog) = 'duckdb'
  
[0m15:19:05.694580 [debug] [ThreadPool]: Using duckdb connection "list_duckdb_main_bronze"
[0m15:19:05.694580 [debug] [ThreadPool]: On list_duckdb_main_gold: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_duckdb_main_gold"} */
select
      'duckdb' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main_gold'
    and lower(table_catalog) = 'duckdb'
  
[0m15:19:05.694580 [debug] [ThreadPool]: On list_duckdb_main_bronze: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "connection_name": "list_duckdb_main_bronze"} */
select
      'duckdb' as database,
      table_name as name,
      table_schema as schema,
      CASE table_type
        WHEN 'BASE TABLE' THEN 'table'
        WHEN 'VIEW' THEN 'view'
        WHEN 'LOCAL TEMPORARY' THEN 'table'
        END as type
    from system.information_schema.tables
    where lower(table_schema) = 'main_bronze'
    and lower(table_catalog) = 'duckdb'
  
[0m15:19:05.721644 [debug] [ThreadPool]: SQL status: OK in 0.024 seconds
[0m15:19:05.722837 [debug] [ThreadPool]: SQL status: OK in 0.023 seconds
[0m15:19:05.723379 [debug] [ThreadPool]: SQL status: OK in 0.023 seconds
[0m15:19:05.725692 [debug] [ThreadPool]: On list_duckdb_main_silver: ROLLBACK
[0m15:19:05.728691 [debug] [ThreadPool]: On list_duckdb_main_gold: ROLLBACK
[0m15:19:05.730910 [debug] [ThreadPool]: On list_duckdb_main_bronze: ROLLBACK
[0m15:19:05.732665 [debug] [ThreadPool]: Failed to rollback 'list_duckdb_main_silver'
[0m15:19:05.734354 [debug] [ThreadPool]: Failed to rollback 'list_duckdb_main_bronze'
[0m15:19:05.734923 [debug] [ThreadPool]: Failed to rollback 'list_duckdb_main_gold'
[0m15:19:05.735461 [debug] [ThreadPool]: On list_duckdb_main_silver: Close
[0m15:19:05.736531 [debug] [ThreadPool]: On list_duckdb_main_bronze: Close
[0m15:19:05.737051 [debug] [ThreadPool]: On list_duckdb_main_gold: Close
[0m15:19:05.743409 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '132c6645-99b1-4a1d-836f-a6ab29263d84', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000232CAA85280>]}
[0m15:19:05.744409 [debug] [MainThread]: Using duckdb connection "master"
[0m15:19:05.745412 [debug] [MainThread]: On master: BEGIN
[0m15:19:05.746411 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:19:05.748434 [debug] [MainThread]: SQL status: OK in 0.002 seconds
[0m15:19:05.749615 [debug] [MainThread]: On master: COMMIT
[0m15:19:05.750202 [debug] [MainThread]: Using duckdb connection "master"
[0m15:19:05.751484 [debug] [MainThread]: On master: COMMIT
[0m15:19:05.752727 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:19:05.753912 [debug] [MainThread]: On master: Close
[0m15:19:05.761484 [debug] [Thread-1  ]: Began running node model.job_intelligent.stg_jobs_raw
[0m15:19:05.762492 [info ] [Thread-1  ]: 1 of 13 START sql view model main_bronze.stg_jobs_raw .......................... [RUN]
[0m15:19:05.764435 [debug] [Thread-1  ]: Acquiring new duckdb connection 'model.job_intelligent.stg_jobs_raw'
[0m15:19:05.765446 [debug] [Thread-1  ]: Began compiling node model.job_intelligent.stg_jobs_raw
[0m15:19:05.775114 [debug] [Thread-1  ]: Writing injected SQL for node "model.job_intelligent.stg_jobs_raw"
[0m15:19:05.777053 [debug] [Thread-1  ]: Began executing node model.job_intelligent.stg_jobs_raw
[0m15:19:05.814953 [debug] [Thread-1  ]: Writing runtime sql for node "model.job_intelligent.stg_jobs_raw"
[0m15:19:05.814953 [debug] [Thread-1  ]: Using duckdb connection "model.job_intelligent.stg_jobs_raw"
[0m15:19:05.814953 [debug] [Thread-1  ]: On model.job_intelligent.stg_jobs_raw: BEGIN
[0m15:19:05.814953 [debug] [Thread-1  ]: Opening a new connection, currently in state init
[0m15:19:05.818479 [debug] [Thread-1  ]: SQL status: OK in 0.001 seconds
[0m15:19:05.819550 [debug] [Thread-1  ]: Using duckdb connection "model.job_intelligent.stg_jobs_raw"
[0m15:19:05.820750 [debug] [Thread-1  ]: On model.job_intelligent.stg_jobs_raw: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.stg_jobs_raw"} */

  
  create view "duckdb"."main_bronze"."stg_jobs_raw__dbt_tmp" as (
    -- models/bronze/stg_jobs_raw.sql
-- Bronze layer: Lecture directe des données brutes depuis final_data.csv
-- Renommage et typage minimal



-- Read directly from CSV file
SELECT
    -- Renommer les colonnes pour cohérence
    title as job_title,
    location,
    postedTime as posted_time,
    publishedAt as published_at,
    jobUrl as job_url,
    companyName as company_name,
    companyUrl as company_url,
    description as job_description,
    contractType as contract_type,
    workType as work_type,
    
    -- Métadonnées de traçabilité
    NOW() as ingestion_timestamp,
    '2026-01-08 14:19:03.961277+00:00' as dbt_run_id

FROM read_csv_auto('../data/bronze/final_data.csv')

WHERE 1=1
    -- Filtrer les lignes vides
    AND title IS NOT NULL
    AND description IS NOT NULL
  );

[0m15:19:06.536982 [debug] [Thread-1  ]: SQL status: OK in 0.716 seconds
[0m15:19:06.545721 [debug] [Thread-1  ]: Using duckdb connection "model.job_intelligent.stg_jobs_raw"
[0m15:19:06.546722 [debug] [Thread-1  ]: On model.job_intelligent.stg_jobs_raw: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.stg_jobs_raw"} */
alter view "duckdb"."main_bronze"."stg_jobs_raw" rename to "stg_jobs_raw__dbt_backup"
[0m15:19:06.548753 [debug] [Thread-1  ]: SQL status: OK in 0.001 seconds
[0m15:19:06.550579 [debug] [Thread-1  ]: Using duckdb connection "model.job_intelligent.stg_jobs_raw"
[0m15:19:06.550579 [debug] [Thread-1  ]: On model.job_intelligent.stg_jobs_raw: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.stg_jobs_raw"} */
alter view "duckdb"."main_bronze"."stg_jobs_raw__dbt_tmp" rename to "stg_jobs_raw"
[0m15:19:06.550579 [debug] [Thread-1  ]: SQL status: OK in 0.001 seconds
[0m15:19:06.570546 [debug] [Thread-1  ]: On model.job_intelligent.stg_jobs_raw: COMMIT
[0m15:19:06.570546 [debug] [Thread-1  ]: Using duckdb connection "model.job_intelligent.stg_jobs_raw"
[0m15:19:06.570546 [debug] [Thread-1  ]: On model.job_intelligent.stg_jobs_raw: COMMIT
[0m15:19:06.570546 [debug] [Thread-1  ]: SQL status: OK in 0.002 seconds
[0m15:19:06.583690 [debug] [Thread-1  ]: Using duckdb connection "model.job_intelligent.stg_jobs_raw"
[0m15:19:06.583690 [debug] [Thread-1  ]: On model.job_intelligent.stg_jobs_raw: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.stg_jobs_raw"} */

      drop view if exists "duckdb"."main_bronze"."stg_jobs_raw__dbt_backup" cascade
    
[0m15:19:06.583690 [debug] [Thread-1  ]: SQL status: OK in 0.002 seconds
[0m15:19:06.589299 [debug] [Thread-1  ]: On model.job_intelligent.stg_jobs_raw: Close
[0m15:19:06.594503 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '132c6645-99b1-4a1d-836f-a6ab29263d84', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000232CC4AB2E0>]}
[0m15:19:06.594503 [info ] [Thread-1  ]: 1 of 13 OK created sql view model main_bronze.stg_jobs_raw ..................... [[32mOK[0m in 0.83s]
[0m15:19:06.594503 [debug] [Thread-1  ]: Finished running node model.job_intelligent.stg_jobs_raw
[0m15:19:06.598361 [debug] [Thread-3  ]: Began running node model.job_intelligent.int_jobs_cleaned
[0m15:19:06.599450 [info ] [Thread-3  ]: 2 of 13 START sql table model main_silver.int_jobs_cleaned ..................... [RUN]
[0m15:19:06.600518 [debug] [Thread-3  ]: Acquiring new duckdb connection 'model.job_intelligent.int_jobs_cleaned'
[0m15:19:06.600518 [debug] [Thread-3  ]: Began compiling node model.job_intelligent.int_jobs_cleaned
[0m15:19:06.600518 [debug] [Thread-3  ]: Writing injected SQL for node "model.job_intelligent.int_jobs_cleaned"
[0m15:19:06.600518 [debug] [Thread-3  ]: Began executing node model.job_intelligent.int_jobs_cleaned
[0m15:19:06.633402 [debug] [Thread-3  ]: Writing runtime sql for node "model.job_intelligent.int_jobs_cleaned"
[0m15:19:06.633402 [debug] [Thread-3  ]: Using duckdb connection "model.job_intelligent.int_jobs_cleaned"
[0m15:19:06.633402 [debug] [Thread-3  ]: On model.job_intelligent.int_jobs_cleaned: BEGIN
[0m15:19:06.636797 [debug] [Thread-3  ]: Opening a new connection, currently in state init
[0m15:19:06.638903 [debug] [Thread-3  ]: SQL status: OK in 0.002 seconds
[0m15:19:06.639449 [debug] [Thread-3  ]: Using duckdb connection "model.job_intelligent.int_jobs_cleaned"
[0m15:19:06.639449 [debug] [Thread-3  ]: On model.job_intelligent.int_jobs_cleaned: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.int_jobs_cleaned"} */

  
    
    

    create  table
      "duckdb"."main_silver"."int_jobs_cleaned__dbt_tmp"
  
    as (
      -- models/silver/int_jobs_cleaned.sql
-- Silver layer: Nettoyage et normalisation des données brutes



WITH raw_jobs AS (
    SELECT * FROM "duckdb"."main_bronze"."stg_jobs_raw"
),

cleaned_jobs AS (
    SELECT
        -- Texte: lowercase, trim, remove special characters
        LOWER(TRIM(job_title)) as job_title_cleaned,
        LOWER(TRIM(location)) as location_cleaned,
        LOWER(TRIM(company_name)) as company_name_cleaned,
        LOWER(TRIM(job_description)) as job_description_cleaned,
        LOWER(TRIM(contract_type)) as contract_type_cleaned,
        LOWER(TRIM(work_type)) as work_type_cleaned,
        
        -- URLs as-is
        job_url,
        company_url,
        
        -- Dates
        TRY_CAST(published_at AS DATE) as published_date,
        posted_time,
        
        -- Extract year-month for time-based analysis
        DATE_TRUNC('month', TRY_CAST(published_at AS DATE)) as published_year_month,
        EXTRACT(YEAR FROM TRY_CAST(published_at AS DATE)) as published_year,
        EXTRACT(MONTH FROM TRY_CAST(published_at AS DATE)) as published_month,
        
        -- Métadonnées
        ingestion_timestamp
    FROM raw_jobs
)

SELECT
    *,
    -- Deduplication flag
    ROW_NUMBER() OVER (
        PARTITION BY job_title_cleaned, company_name_cleaned, location_cleaned 
        ORDER BY published_date DESC
    ) as dedup_rank
FROM cleaned_jobs
    );
  
  
[0m15:19:06.797438 [debug] [Thread-3  ]: SQL status: OK in 0.156 seconds
[0m15:19:06.805117 [debug] [Thread-3  ]: Using duckdb connection "model.job_intelligent.int_jobs_cleaned"
[0m15:19:06.805117 [debug] [Thread-3  ]: On model.job_intelligent.int_jobs_cleaned: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.int_jobs_cleaned"} */

    SELECT index_name
    FROM duckdb_indexes()
    WHERE schema_name = 'main_silver'
      AND table_name = 'int_jobs_cleaned'
  
[0m15:19:06.807089 [debug] [Thread-3  ]: SQL status: OK in 0.001 seconds
[0m15:19:06.809179 [debug] [Thread-3  ]: Using duckdb connection "model.job_intelligent.int_jobs_cleaned"
[0m15:19:06.809179 [debug] [Thread-3  ]: On model.job_intelligent.int_jobs_cleaned: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.int_jobs_cleaned"} */

    SELECT COUNT(*) as remaining_indexes
    FROM duckdb_indexes()
    WHERE schema_name = 'main_silver'
      AND table_name = 'int_jobs_cleaned'
  
[0m15:19:06.811218 [debug] [Thread-3  ]: SQL status: OK in 0.001 seconds
[0m15:19:06.815193 [debug] [Thread-3  ]: Using duckdb connection "model.job_intelligent.int_jobs_cleaned"
[0m15:19:06.815193 [debug] [Thread-3  ]: On model.job_intelligent.int_jobs_cleaned: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.int_jobs_cleaned"} */
alter table "duckdb"."main_silver"."int_jobs_cleaned" rename to "int_jobs_cleaned__dbt_backup"
[0m15:19:06.818389 [debug] [Thread-3  ]: SQL status: OK in 0.001 seconds
[0m15:19:06.820646 [debug] [Thread-3  ]: Using duckdb connection "model.job_intelligent.int_jobs_cleaned"
[0m15:19:06.820646 [debug] [Thread-3  ]: On model.job_intelligent.int_jobs_cleaned: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.int_jobs_cleaned"} */
alter table "duckdb"."main_silver"."int_jobs_cleaned__dbt_tmp" rename to "int_jobs_cleaned"
[0m15:19:06.820646 [debug] [Thread-3  ]: SQL status: OK in 0.000 seconds
[0m15:19:06.832052 [debug] [Thread-3  ]: On model.job_intelligent.int_jobs_cleaned: COMMIT
[0m15:19:06.832052 [debug] [Thread-3  ]: Using duckdb connection "model.job_intelligent.int_jobs_cleaned"
[0m15:19:06.833073 [debug] [Thread-3  ]: On model.job_intelligent.int_jobs_cleaned: COMMIT
[0m15:19:06.893601 [debug] [Thread-3  ]: SQL status: OK in 0.060 seconds
[0m15:19:06.897740 [debug] [Thread-3  ]: Using duckdb connection "model.job_intelligent.int_jobs_cleaned"
[0m15:19:06.899352 [debug] [Thread-3  ]: On model.job_intelligent.int_jobs_cleaned: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.int_jobs_cleaned"} */

      drop table if exists "duckdb"."main_silver"."int_jobs_cleaned__dbt_backup" cascade
    
[0m15:19:06.900543 [debug] [Thread-3  ]: SQL status: OK in 0.004 seconds
[0m15:19:06.900543 [debug] [Thread-3  ]: On model.job_intelligent.int_jobs_cleaned: Close
[0m15:19:06.900543 [debug] [Thread-3  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '132c6645-99b1-4a1d-836f-a6ab29263d84', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000232CE571CA0>]}
[0m15:19:06.908580 [info ] [Thread-3  ]: 2 of 13 OK created sql table model main_silver.int_jobs_cleaned ................ [[32mOK[0m in 0.30s]
[0m15:19:06.909139 [debug] [Thread-3  ]: Finished running node model.job_intelligent.int_jobs_cleaned
[0m15:19:06.909139 [debug] [Thread-2  ]: Began running node model.job_intelligent.int_job_title_normalization
[0m15:19:06.909139 [info ] [Thread-2  ]: 3 of 13 START sql table model main_silver.int_job_title_normalization .......... [RUN]
[0m15:19:06.909139 [debug] [Thread-2  ]: Acquiring new duckdb connection 'model.job_intelligent.int_job_title_normalization'
[0m15:19:06.916289 [debug] [Thread-2  ]: Began compiling node model.job_intelligent.int_job_title_normalization
[0m15:19:06.920519 [debug] [Thread-2  ]: Writing injected SQL for node "model.job_intelligent.int_job_title_normalization"
[0m15:19:06.920519 [debug] [Thread-2  ]: Began executing node model.job_intelligent.int_job_title_normalization
[0m15:19:06.920519 [debug] [Thread-2  ]: Writing runtime sql for node "model.job_intelligent.int_job_title_normalization"
[0m15:19:06.929351 [debug] [Thread-2  ]: Using duckdb connection "model.job_intelligent.int_job_title_normalization"
[0m15:19:06.929950 [debug] [Thread-2  ]: On model.job_intelligent.int_job_title_normalization: BEGIN
[0m15:19:06.931219 [debug] [Thread-2  ]: Opening a new connection, currently in state init
[0m15:19:06.932221 [debug] [Thread-2  ]: SQL status: OK in 0.001 seconds
[0m15:19:06.933220 [debug] [Thread-2  ]: Using duckdb connection "model.job_intelligent.int_job_title_normalization"
[0m15:19:06.934221 [debug] [Thread-2  ]: On model.job_intelligent.int_job_title_normalization: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.int_job_title_normalization"} */

  
    
    

    create  table
      "duckdb"."main_silver"."int_job_title_normalization__dbt_tmp"
  
    as (
      -- models/silver/int_job_title_normalization.sql
-- Silver layer: Normaliser les intitulés de postes



WITH cleaned_jobs AS (
    SELECT * FROM "duckdb"."main_silver"."int_jobs_cleaned"
),

title_normalized AS (
    SELECT
        *,
        CASE
            WHEN job_title_cleaned LIKE '%data engineer%' THEN 'Data Engineer'
            WHEN job_title_cleaned LIKE '%data scientist%' THEN 'Data Scientist'
            WHEN job_title_cleaned LIKE '%data analyst%' THEN 'Data Analyst'
            WHEN job_title_cleaned LIKE '%analytics engineer%' THEN 'Analytics Engineer'
            WHEN job_title_cleaned LIKE '%ml engineer%' OR job_title_cleaned LIKE '%machine learning%' THEN 'ML Engineer'
            WHEN job_title_cleaned LIKE '%data architect%' THEN 'Data Architect'
            WHEN job_title_cleaned LIKE '%bi developer%' OR job_title_cleaned LIKE '%business intelligence%' THEN 'BI Developer'
            WHEN job_title_cleaned LIKE '%etl%' OR job_title_cleaned LIKE '%pipeline%' THEN 'ETL/Pipeline Engineer'
            WHEN job_title_cleaned LIKE '%data engineer%' THEN 'Data Engineer'
            ELSE 'Other Data Role'
        END as job_category,
        
        CASE
            WHEN contract_type_cleaned LIKE '%cdi%' OR contract_type_cleaned LIKE '%permanent%' THEN 'Permanent'
            WHEN contract_type_cleaned LIKE '%cdd%' OR contract_type_cleaned LIKE '%contract%' THEN 'Contract'
            WHEN contract_type_cleaned LIKE '%stage%' OR contract_type_cleaned LIKE '%internship%' THEN 'Internship'
            WHEN contract_type_cleaned LIKE '%freelance%' THEN 'Freelance'
            ELSE 'Not Specified'
        END as contract_type_normalized,
        
        CASE
            WHEN work_type_cleaned LIKE '%remote%' THEN 'Remote'
            WHEN work_type_cleaned LIKE '%hybrid%' THEN 'Hybrid'
            WHEN work_type_cleaned LIKE '%onsite%' OR work_type_cleaned LIKE '%on-site%' THEN 'On-site'
            ELSE 'Not Specified'
        END as work_type_normalized
    
    FROM cleaned_jobs
    WHERE dedup_rank = 1  -- Keep only first occurrence (most recent)
)

SELECT * FROM title_normalized
    );
  
  
[0m15:19:06.968340 [debug] [Thread-2  ]: SQL status: OK in 0.034 seconds
[0m15:19:06.970819 [debug] [Thread-2  ]: Using duckdb connection "model.job_intelligent.int_job_title_normalization"
[0m15:19:06.970819 [debug] [Thread-2  ]: On model.job_intelligent.int_job_title_normalization: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.int_job_title_normalization"} */

    SELECT index_name
    FROM duckdb_indexes()
    WHERE schema_name = 'main_silver'
      AND table_name = 'int_job_title_normalization'
  
[0m15:19:06.972759 [debug] [Thread-2  ]: SQL status: OK in 0.001 seconds
[0m15:19:06.974831 [debug] [Thread-2  ]: Using duckdb connection "model.job_intelligent.int_job_title_normalization"
[0m15:19:06.975786 [debug] [Thread-2  ]: On model.job_intelligent.int_job_title_normalization: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.int_job_title_normalization"} */

    SELECT COUNT(*) as remaining_indexes
    FROM duckdb_indexes()
    WHERE schema_name = 'main_silver'
      AND table_name = 'int_job_title_normalization'
  
[0m15:19:06.976825 [debug] [Thread-2  ]: SQL status: OK in 0.001 seconds
[0m15:19:06.982040 [debug] [Thread-2  ]: Using duckdb connection "model.job_intelligent.int_job_title_normalization"
[0m15:19:06.983076 [debug] [Thread-2  ]: On model.job_intelligent.int_job_title_normalization: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.int_job_title_normalization"} */
alter table "duckdb"."main_silver"."int_job_title_normalization" rename to "int_job_title_normalization__dbt_backup"
[0m15:19:06.984991 [debug] [Thread-2  ]: SQL status: OK in 0.001 seconds
[0m15:19:06.989236 [debug] [Thread-2  ]: Using duckdb connection "model.job_intelligent.int_job_title_normalization"
[0m15:19:06.989236 [debug] [Thread-2  ]: On model.job_intelligent.int_job_title_normalization: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.int_job_title_normalization"} */
alter table "duckdb"."main_silver"."int_job_title_normalization__dbt_tmp" rename to "int_job_title_normalization"
[0m15:19:06.991246 [debug] [Thread-2  ]: SQL status: OK in 0.000 seconds
[0m15:19:06.993321 [debug] [Thread-2  ]: On model.job_intelligent.int_job_title_normalization: COMMIT
[0m15:19:06.994310 [debug] [Thread-2  ]: Using duckdb connection "model.job_intelligent.int_job_title_normalization"
[0m15:19:06.996303 [debug] [Thread-2  ]: On model.job_intelligent.int_job_title_normalization: COMMIT
[0m15:19:07.040392 [debug] [Thread-2  ]: SQL status: OK in 0.043 seconds
[0m15:19:07.040392 [debug] [Thread-2  ]: Using duckdb connection "model.job_intelligent.int_job_title_normalization"
[0m15:19:07.040392 [debug] [Thread-2  ]: On model.job_intelligent.int_job_title_normalization: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.int_job_title_normalization"} */

      drop table if exists "duckdb"."main_silver"."int_job_title_normalization__dbt_backup" cascade
    
[0m15:19:07.231120 [debug] [Thread-2  ]: SQL status: OK in 0.190 seconds
[0m15:19:07.238389 [debug] [Thread-2  ]: On model.job_intelligent.int_job_title_normalization: Close
[0m15:19:07.239460 [debug] [Thread-2  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '132c6645-99b1-4a1d-836f-a6ab29263d84', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000232CE5B5F70>]}
[0m15:19:07.239460 [info ] [Thread-2  ]: 3 of 13 OK created sql table model main_silver.int_job_title_normalization ..... [[32mOK[0m in 0.33s]
[0m15:19:07.239460 [debug] [Thread-2  ]: Finished running node model.job_intelligent.int_job_title_normalization
[0m15:19:07.239460 [debug] [Thread-4  ]: Began running node model.job_intelligent.dim_company
[0m15:19:07.239460 [debug] [Thread-3  ]: Began running node model.job_intelligent.dim_location
[0m15:19:07.244909 [debug] [Thread-2  ]: Began running node model.job_intelligent.int_skills_extraction
[0m15:19:07.239460 [debug] [Thread-1  ]: Began running node model.job_intelligent.dim_time
[0m15:19:07.246926 [info ] [Thread-4  ]: 4 of 13 START sql table model main_gold.dim_company ............................ [RUN]
[0m15:19:07.249123 [info ] [Thread-3  ]: 5 of 13 START sql table model main_gold.dim_location ........................... [RUN]
[0m15:19:07.250580 [info ] [Thread-2  ]: 7 of 13 START sql table model main_silver.int_skills_extraction ................ [RUN]
[0m15:19:07.250580 [info ] [Thread-1  ]: 6 of 13 START sql table model main_gold.dim_time ............................... [RUN]
[0m15:19:07.250580 [debug] [Thread-4  ]: Acquiring new duckdb connection 'model.job_intelligent.dim_company'
[0m15:19:07.250580 [debug] [Thread-3  ]: Re-using an available connection from the pool (formerly model.job_intelligent.int_jobs_cleaned, now model.job_intelligent.dim_location)
[0m15:19:07.250580 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly model.job_intelligent.int_job_title_normalization, now model.job_intelligent.int_skills_extraction)
[0m15:19:07.250580 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.job_intelligent.stg_jobs_raw, now model.job_intelligent.dim_time)
[0m15:19:07.258706 [debug] [Thread-4  ]: Began compiling node model.job_intelligent.dim_company
[0m15:19:07.259286 [debug] [Thread-3  ]: Began compiling node model.job_intelligent.dim_location
[0m15:19:07.259286 [debug] [Thread-2  ]: Began compiling node model.job_intelligent.int_skills_extraction
[0m15:19:07.259286 [debug] [Thread-1  ]: Began compiling node model.job_intelligent.dim_time
[0m15:19:07.269523 [debug] [Thread-4  ]: Writing injected SQL for node "model.job_intelligent.dim_company"
[0m15:19:07.270639 [debug] [Thread-3  ]: Writing injected SQL for node "model.job_intelligent.dim_location"
[0m15:19:07.279403 [debug] [Thread-2  ]: Writing injected SQL for node "model.job_intelligent.int_skills_extraction"
[0m15:19:07.286744 [debug] [Thread-1  ]: Writing injected SQL for node "model.job_intelligent.dim_time"
[0m15:19:07.288743 [debug] [Thread-4  ]: Began executing node model.job_intelligent.dim_company
[0m15:19:07.289254 [debug] [Thread-3  ]: Began executing node model.job_intelligent.dim_location
[0m15:19:07.290956 [debug] [Thread-2  ]: Began executing node model.job_intelligent.int_skills_extraction
[0m15:19:07.295969 [debug] [Thread-4  ]: Writing runtime sql for node "model.job_intelligent.dim_company"
[0m15:19:07.297757 [debug] [Thread-1  ]: Began executing node model.job_intelligent.dim_time
[0m15:19:07.303332 [debug] [Thread-3  ]: Writing runtime sql for node "model.job_intelligent.dim_location"
[0m15:19:07.309378 [debug] [Thread-2  ]: Writing runtime sql for node "model.job_intelligent.int_skills_extraction"
[0m15:19:07.316206 [debug] [Thread-1  ]: Writing runtime sql for node "model.job_intelligent.dim_time"
[0m15:19:07.317138 [debug] [Thread-4  ]: Using duckdb connection "model.job_intelligent.dim_company"
[0m15:19:07.320227 [debug] [Thread-3  ]: Using duckdb connection "model.job_intelligent.dim_location"
[0m15:19:07.321226 [debug] [Thread-4  ]: On model.job_intelligent.dim_company: BEGIN
[0m15:19:07.322227 [debug] [Thread-2  ]: Using duckdb connection "model.job_intelligent.int_skills_extraction"
[0m15:19:07.323228 [debug] [Thread-1  ]: Using duckdb connection "model.job_intelligent.dim_time"
[0m15:19:07.324231 [debug] [Thread-3  ]: On model.job_intelligent.dim_location: BEGIN
[0m15:19:07.325226 [debug] [Thread-4  ]: Opening a new connection, currently in state init
[0m15:19:07.326228 [debug] [Thread-2  ]: On model.job_intelligent.int_skills_extraction: BEGIN
[0m15:19:07.328226 [debug] [Thread-1  ]: On model.job_intelligent.dim_time: BEGIN
[0m15:19:07.329261 [debug] [Thread-3  ]: Opening a new connection, currently in state closed
[0m15:19:07.330912 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m15:19:07.331929 [debug] [Thread-4  ]: SQL status: OK in 0.006 seconds
[0m15:19:07.332929 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m15:19:07.333925 [debug] [Thread-3  ]: SQL status: OK in 0.005 seconds
[0m15:19:07.334929 [debug] [Thread-2  ]: SQL status: OK in 0.004 seconds
[0m15:19:07.334929 [debug] [Thread-4  ]: Using duckdb connection "model.job_intelligent.dim_company"
[0m15:19:07.336921 [debug] [Thread-3  ]: Using duckdb connection "model.job_intelligent.dim_location"
[0m15:19:07.336921 [debug] [Thread-1  ]: SQL status: OK in 0.005 seconds
[0m15:19:07.338434 [debug] [Thread-2  ]: Using duckdb connection "model.job_intelligent.int_skills_extraction"
[0m15:19:07.339510 [debug] [Thread-4  ]: On model.job_intelligent.dim_company: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_company"} */

  
    
    

    create  table
      "duckdb"."main_gold"."dim_company__dbt_tmp"
  
    as (
      -- models/gold/dim_company.sql
-- Gold layer: Dimension Company



WITH jobs AS (
    SELECT DISTINCT
        company_name_cleaned,
        company_url
    FROM "duckdb"."main_silver"."int_job_title_normalization"
    WHERE company_name_cleaned IS NOT NULL
),

ranked_companies AS (
    SELECT
        ROW_NUMBER() OVER (ORDER BY company_name_cleaned) as company_id,
        company_name_cleaned as company_name,
        company_url,
        NOW() as created_at
    FROM jobs
)

SELECT
    company_id,
    company_name,
    company_url,
    created_at
FROM ranked_companies
ORDER BY company_id
    );
  
  
[0m15:19:07.340660 [debug] [Thread-3  ]: On model.job_intelligent.dim_location: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_location"} */

  
    
    

    create  table
      "duckdb"."main_gold"."dim_location__dbt_tmp"
  
    as (
      -- models/gold/dim_location.sql
-- Gold layer: Dimension Location



WITH jobs AS (
    SELECT DISTINCT
        location_cleaned as location_raw
    FROM "duckdb"."main_silver"."int_job_title_normalization"
    WHERE location_cleaned IS NOT NULL
),

location_parsed AS (
    SELECT
        location_raw,
        -- Extract city (before comma)
        CASE 
            WHEN POSITION(',' IN location_raw) > 0 
            THEN TRIM(SUBSTRING(location_raw, 1, POSITION(',' IN location_raw) - 1))
            ELSE location_raw
        END as city,
        
        -- Extract country (after comma)
        CASE 
            WHEN POSITION(',' IN location_raw) > 0 
            THEN TRIM(SUBSTRING(location_raw, POSITION(',' IN location_raw) + 1))
            ELSE 'Not Specified'
        END as country,
        
        -- Detect if remote
        CASE 
            WHEN location_raw LIKE '%remote%' 
            THEN 'Remote'
            ELSE 'On-site'
        END as work_location_type
    FROM jobs
),

ranked_locations AS (
    SELECT
        ROW_NUMBER() OVER (ORDER BY location_raw) as location_id,
        location_raw,
        city,
        country,
        work_location_type,
        NOW() as created_at
    FROM location_parsed
)

SELECT
    location_id,
    location_raw,
    city,
    country,
    work_location_type,
    created_at
FROM ranked_locations
ORDER BY location_id
    );
  
  
[0m15:19:07.341654 [debug] [Thread-1  ]: Using duckdb connection "model.job_intelligent.dim_time"
[0m15:19:07.341654 [debug] [Thread-2  ]: On model.job_intelligent.int_skills_extraction: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.int_skills_extraction"} */

  
    
    

    create  table
      "duckdb"."main_silver"."int_skills_extraction__dbt_tmp"
  
    as (
      -- models/silver/int_skills_extraction.sql
-- Silver layer: Extraction des compétences depuis la description



WITH jobs_with_titles AS (
    SELECT * FROM "duckdb"."main_silver"."int_job_title_normalization"
),

skills_mapping AS (
    -- Définir un mapping de compétences communes Data/Tech
    SELECT
        'Python' as skill_name,
        'python|py |\.py' as skill_pattern
    UNION ALL SELECT 'SQL', 'sql|sql|sql server|postgres|oracle'
    UNION ALL SELECT 'Spark', 'spark|pyspark'
    UNION ALL SELECT 'Hadoop', 'hadoop|hdfs'
    UNION ALL SELECT 'Scala', 'scala'
    UNION ALL SELECT 'Java', '\bjava\b'
    UNION ALL SELECT 'R', '\br\b|r programming'
    UNION ALL SELECT 'Tableau', 'tableau'
    UNION ALL SELECT 'Power BI', 'power bi|powerbi'
    UNION ALL SELECT 'Looker', 'looker'
    UNION ALL SELECT 'AWS', 'aws|amazon web|s3 |ec2|redshift'
    UNION ALL SELECT 'Azure', 'azure|microsoft azure|synapse|cosmos'
    UNION ALL SELECT 'GCP', 'gcp|google cloud|bigquery'
    UNION ALL SELECT 'Airflow', 'airflow'
    UNION ALL SELECT 'DBT', '\bdbt\b|dbt'
    UNION ALL SELECT 'Kubernetes', 'kubernetes|k8s'
    UNION ALL SELECT 'Docker', 'docker'
    UNION ALL SELECT 'Git', 'git|github|gitlab'
    UNION ALL SELECT 'TensorFlow', 'tensorflow'
    UNION ALL SELECT 'PyTorch', 'pytorch'
    UNION ALL SELECT 'Scikit-learn', 'scikit|sklearn'
    UNION ALL SELECT 'Pandas', 'pandas'
    UNION ALL SELECT 'NumPy', 'numpy'
    UNION ALL SELECT 'Machine Learning', 'machine learning|deep learning|ml|artificial intelligence'
    UNION ALL SELECT 'Statistics', 'statistics|statistical|probability'
    UNION ALL SELECT 'Data Visualization', 'data visualization|visualization|charts|graphs'
),

jobs_exploded AS (
    SELECT
        j.*,
        s.skill_name,
        CASE 
            WHEN job_description_cleaned ILIKE '%' || s.skill_pattern || '%' THEN 1 
            ELSE 0 
        END as has_skill
    FROM jobs_with_titles j
    CROSS JOIN skills_mapping s
)

SELECT
    *
FROM jobs_exploded
WHERE has_skill = 1

ORDER BY job_title_cleaned, published_date DESC
    );
  
  
[0m15:19:07.344657 [debug] [Thread-1  ]: On model.job_intelligent.dim_time: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_time"} */

  
    
    

    create  table
      "duckdb"."main_gold"."dim_time__dbt_tmp"
  
    as (
      -- models/gold/dim_time.sql
-- Gold layer: Dimension Time



-- Generate a date dimension for time-based analysis
WITH date_spine AS (
    SELECT
        published_date,
        EXTRACT(YEAR FROM published_date) as year,
        EXTRACT(MONTH FROM published_date) as month,
        EXTRACT(QUARTER FROM published_date) as quarter,
        EXTRACT(WEEK FROM published_date) as week,
        EXTRACT(DAYOFWEEK FROM published_date) as day_of_week,
        DATE_TRUNC('month', published_date) as month_start,
        DATE_TRUNC('quarter', published_date) as quarter_start,
        DATE_TRUNC('year', published_date) as year_start,
        
        CASE 
            WHEN EXTRACT(MONTH FROM published_date) IN (1,2,3) THEN 'Q1'
            WHEN EXTRACT(MONTH FROM published_date) IN (4,5,6) THEN 'Q2'
            WHEN EXTRACT(MONTH FROM published_date) IN (7,8,9) THEN 'Q3'
            ELSE 'Q4'
        END as quarter_name,
        
        CASE EXTRACT(DAYOFWEEK FROM published_date)
            WHEN 0 THEN 'Sunday'
            WHEN 1 THEN 'Monday'
            WHEN 2 THEN 'Tuesday'
            WHEN 3 THEN 'Wednesday'
            WHEN 4 THEN 'Thursday'
            WHEN 5 THEN 'Friday'
            WHEN 6 THEN 'Saturday'
        END as day_name,
        
        CASE EXTRACT(MONTH FROM published_date)
            WHEN 1 THEN 'January'
            WHEN 2 THEN 'February'
            WHEN 3 THEN 'March'
            WHEN 4 THEN 'April'
            WHEN 5 THEN 'May'
            WHEN 6 THEN 'June'
            WHEN 7 THEN 'July'
            WHEN 8 THEN 'August'
            WHEN 9 THEN 'September'
            WHEN 10 THEN 'October'
            WHEN 11 THEN 'November'
            WHEN 12 THEN 'December'
        END as month_name
        
    FROM (
        SELECT DISTINCT published_date
        FROM "duckdb"."main_silver"."int_job_title_normalization"
        WHERE published_date IS NOT NULL
    )
)

SELECT
    published_date as date_id,
    year,
    month,
    quarter,
    week,
    day_of_week,
    day_name,
    month_name,
    quarter_name,
    month_start,
    quarter_start,
    year_start,
    NOW() as created_at
FROM date_spine
ORDER BY published_date
    );
  
  
[0m15:19:07.350448 [debug] [Thread-3  ]: SQL status: OK in 0.008 seconds
[0m15:19:07.350448 [debug] [Thread-4  ]: SQL status: OK in 0.010 seconds
[0m15:19:07.350448 [debug] [Thread-3  ]: Using duckdb connection "model.job_intelligent.dim_location"
[0m15:19:07.359200 [debug] [Thread-4  ]: Using duckdb connection "model.job_intelligent.dim_company"
[0m15:19:07.359200 [debug] [Thread-3  ]: On model.job_intelligent.dim_location: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_location"} */

    SELECT index_name
    FROM duckdb_indexes()
    WHERE schema_name = 'main_gold'
      AND table_name = 'dim_location'
  
[0m15:19:07.359200 [debug] [Thread-1  ]: SQL status: OK in 0.013 seconds
[0m15:19:07.359200 [debug] [Thread-4  ]: On model.job_intelligent.dim_company: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_company"} */

    SELECT index_name
    FROM duckdb_indexes()
    WHERE schema_name = 'main_gold'
      AND table_name = 'dim_company'
  
[0m15:19:07.366387 [debug] [Thread-1  ]: Using duckdb connection "model.job_intelligent.dim_time"
[0m15:19:07.366387 [debug] [Thread-3  ]: SQL status: OK in 0.003 seconds
[0m15:19:07.368398 [debug] [Thread-1  ]: On model.job_intelligent.dim_time: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_time"} */

    SELECT index_name
    FROM duckdb_indexes()
    WHERE schema_name = 'main_gold'
      AND table_name = 'dim_time'
  
[0m15:19:07.370484 [debug] [Thread-4  ]: SQL status: OK in 0.002 seconds
[0m15:19:07.370484 [debug] [Thread-3  ]: Using duckdb connection "model.job_intelligent.dim_location"
[0m15:19:07.370484 [debug] [Thread-4  ]: Using duckdb connection "model.job_intelligent.dim_company"
[0m15:19:07.375698 [debug] [Thread-1  ]: SQL status: OK in 0.002 seconds
[0m15:19:07.375698 [debug] [Thread-3  ]: On model.job_intelligent.dim_location: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_location"} */

    SELECT COUNT(*) as remaining_indexes
    FROM duckdb_indexes()
    WHERE schema_name = 'main_gold'
      AND table_name = 'dim_location'
  
[0m15:19:07.375698 [debug] [Thread-4  ]: On model.job_intelligent.dim_company: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_company"} */

    SELECT COUNT(*) as remaining_indexes
    FROM duckdb_indexes()
    WHERE schema_name = 'main_gold'
      AND table_name = 'dim_company'
  
[0m15:19:07.379275 [debug] [Thread-1  ]: Using duckdb connection "model.job_intelligent.dim_time"
[0m15:19:07.381976 [debug] [Thread-3  ]: SQL status: OK in 0.002 seconds
[0m15:19:07.383343 [debug] [Thread-4  ]: SQL status: OK in 0.002 seconds
[0m15:19:07.384410 [debug] [Thread-1  ]: On model.job_intelligent.dim_time: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_time"} */

    SELECT COUNT(*) as remaining_indexes
    FROM duckdb_indexes()
    WHERE schema_name = 'main_gold'
      AND table_name = 'dim_time'
  
[0m15:19:07.389355 [debug] [Thread-3  ]: Using duckdb connection "model.job_intelligent.dim_location"
[0m15:19:07.393964 [debug] [Thread-4  ]: Using duckdb connection "model.job_intelligent.dim_company"
[0m15:19:07.394962 [debug] [Thread-3  ]: On model.job_intelligent.dim_location: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_location"} */
alter table "duckdb"."main_gold"."dim_location" rename to "dim_location__dbt_backup"
[0m15:19:07.395964 [debug] [Thread-1  ]: SQL status: OK in 0.001 seconds
[0m15:19:07.397378 [debug] [Thread-4  ]: On model.job_intelligent.dim_company: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_company"} */
alter table "duckdb"."main_gold"."dim_company" rename to "dim_company__dbt_backup"
[0m15:19:07.403114 [debug] [Thread-1  ]: Using duckdb connection "model.job_intelligent.dim_time"
[0m15:19:07.404124 [debug] [Thread-3  ]: SQL status: OK in 0.006 seconds
[0m15:19:07.405081 [debug] [Thread-4  ]: SQL status: OK in 0.001 seconds
[0m15:19:07.406144 [debug] [Thread-1  ]: On model.job_intelligent.dim_time: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_time"} */
alter table "duckdb"."main_gold"."dim_time" rename to "dim_time__dbt_backup"
[0m15:19:07.410761 [debug] [Thread-3  ]: Using duckdb connection "model.job_intelligent.dim_location"
[0m15:19:07.414173 [debug] [Thread-4  ]: Using duckdb connection "model.job_intelligent.dim_company"
[0m15:19:07.418432 [debug] [Thread-3  ]: On model.job_intelligent.dim_location: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_location"} */
alter table "duckdb"."main_gold"."dim_location__dbt_tmp" rename to "dim_location"
[0m15:19:07.419497 [debug] [Thread-1  ]: SQL status: OK in 0.002 seconds
[0m15:19:07.419497 [debug] [Thread-4  ]: On model.job_intelligent.dim_company: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_company"} */
alter table "duckdb"."main_gold"."dim_company__dbt_tmp" rename to "dim_company"
[0m15:19:07.419497 [debug] [Thread-1  ]: Using duckdb connection "model.job_intelligent.dim_time"
[0m15:19:07.419497 [debug] [Thread-3  ]: SQL status: OK in 0.005 seconds
[0m15:19:07.428367 [debug] [Thread-1  ]: On model.job_intelligent.dim_time: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_time"} */
alter table "duckdb"."main_gold"."dim_time__dbt_tmp" rename to "dim_time"
[0m15:19:07.430188 [debug] [Thread-4  ]: SQL status: OK in 0.002 seconds
[0m15:19:07.433884 [debug] [Thread-3  ]: On model.job_intelligent.dim_location: COMMIT
[0m15:19:07.438400 [debug] [Thread-4  ]: On model.job_intelligent.dim_company: COMMIT
[0m15:19:07.438400 [debug] [Thread-1  ]: SQL status: OK in 0.003 seconds
[0m15:19:07.439933 [debug] [Thread-3  ]: Using duckdb connection "model.job_intelligent.dim_location"
[0m15:19:07.440950 [debug] [Thread-4  ]: Using duckdb connection "model.job_intelligent.dim_company"
[0m15:19:07.443949 [debug] [Thread-1  ]: On model.job_intelligent.dim_time: COMMIT
[0m15:19:07.445952 [debug] [Thread-3  ]: On model.job_intelligent.dim_location: COMMIT
[0m15:19:07.446948 [debug] [Thread-4  ]: On model.job_intelligent.dim_company: COMMIT
[0m15:19:07.449417 [debug] [Thread-1  ]: Using duckdb connection "model.job_intelligent.dim_time"
[0m15:19:07.451030 [debug] [Thread-1  ]: On model.job_intelligent.dim_time: COMMIT
[0m15:19:07.456038 [debug] [Thread-3  ]: SQL status: OK in 0.006 seconds
[0m15:19:07.462095 [debug] [Thread-3  ]: Using duckdb connection "model.job_intelligent.dim_location"
[0m15:19:07.463092 [debug] [Thread-4  ]: SQL status: OK in 0.012 seconds
[0m15:19:07.464090 [debug] [Thread-1  ]: SQL status: OK in 0.012 seconds
[0m15:19:07.465184 [debug] [Thread-3  ]: On model.job_intelligent.dim_location: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_location"} */

      drop table if exists "duckdb"."main_gold"."dim_location__dbt_backup" cascade
    
[0m15:19:07.469249 [debug] [Thread-4  ]: Using duckdb connection "model.job_intelligent.dim_company"
[0m15:19:07.472909 [debug] [Thread-1  ]: Using duckdb connection "model.job_intelligent.dim_time"
[0m15:19:07.473910 [debug] [Thread-4  ]: On model.job_intelligent.dim_company: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_company"} */

      drop table if exists "duckdb"."main_gold"."dim_company__dbt_backup" cascade
    
[0m15:19:07.474909 [debug] [Thread-1  ]: On model.job_intelligent.dim_time: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_time"} */

      drop table if exists "duckdb"."main_gold"."dim_time__dbt_backup" cascade
    
[0m15:19:07.475900 [debug] [Thread-3  ]: SQL status: OK in 0.002 seconds
[0m15:19:07.479430 [debug] [Thread-3  ]: On model.job_intelligent.dim_location: Close
[0m15:19:07.480652 [debug] [Thread-4  ]: SQL status: OK in 0.003 seconds
[0m15:19:07.482003 [debug] [Thread-1  ]: SQL status: OK in 0.004 seconds
[0m15:19:07.483000 [debug] [Thread-3  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '132c6645-99b1-4a1d-836f-a6ab29263d84', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000232CE63A940>]}
[0m15:19:07.484999 [debug] [Thread-4  ]: On model.job_intelligent.dim_company: Close
[0m15:19:07.487001 [debug] [Thread-1  ]: On model.job_intelligent.dim_time: Close
[0m15:19:07.488545 [info ] [Thread-3  ]: 5 of 13 OK created sql table model main_gold.dim_location ...................... [[32mOK[0m in 0.23s]
[0m15:19:07.489553 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '132c6645-99b1-4a1d-836f-a6ab29263d84', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000232CE61AD90>]}
[0m15:19:07.491071 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '132c6645-99b1-4a1d-836f-a6ab29263d84', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000232CC4BEA30>]}
[0m15:19:07.492071 [debug] [Thread-3  ]: Finished running node model.job_intelligent.dim_location
[0m15:19:07.493069 [info ] [Thread-4  ]: 4 of 13 OK created sql table model main_gold.dim_company ....................... [[32mOK[0m in 0.24s]
[0m15:19:07.494071 [info ] [Thread-1  ]: 6 of 13 OK created sql table model main_gold.dim_time .......................... [[32mOK[0m in 0.24s]
[0m15:19:07.496070 [debug] [Thread-4  ]: Finished running node model.job_intelligent.dim_company
[0m15:19:07.497083 [debug] [Thread-1  ]: Finished running node model.job_intelligent.dim_time
[0m15:19:07.499167 [debug] [Thread-3  ]: Began running node model.job_intelligent.fact_job_offers
[0m15:19:07.499167 [info ] [Thread-3  ]: 8 of 13 START sql table model main_gold.fact_job_offers ........................ [RUN]
[0m15:19:07.499167 [debug] [Thread-3  ]: Re-using an available connection from the pool (formerly model.job_intelligent.dim_location, now model.job_intelligent.fact_job_offers)
[0m15:19:07.499167 [debug] [Thread-3  ]: Began compiling node model.job_intelligent.fact_job_offers
[0m15:19:07.499167 [debug] [Thread-3  ]: Writing injected SQL for node "model.job_intelligent.fact_job_offers"
[0m15:19:07.509262 [debug] [Thread-3  ]: Began executing node model.job_intelligent.fact_job_offers
[0m15:19:07.514395 [debug] [Thread-3  ]: Writing runtime sql for node "model.job_intelligent.fact_job_offers"
[0m15:19:07.518420 [debug] [Thread-3  ]: Using duckdb connection "model.job_intelligent.fact_job_offers"
[0m15:19:07.519441 [debug] [Thread-3  ]: On model.job_intelligent.fact_job_offers: BEGIN
[0m15:19:07.519441 [debug] [Thread-3  ]: Opening a new connection, currently in state closed
[0m15:19:07.519441 [debug] [Thread-3  ]: SQL status: OK in 0.001 seconds
[0m15:19:07.519441 [debug] [Thread-3  ]: Using duckdb connection "model.job_intelligent.fact_job_offers"
[0m15:19:07.523339 [debug] [Thread-3  ]: On model.job_intelligent.fact_job_offers: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.fact_job_offers"} */

  
    
    

    create  table
      "duckdb"."main_gold"."fact_job_offers__dbt_tmp"
  
    as (
      -- models/gold/fact_job_offers.sql
-- Gold layer: Fact Table - Job Offers (Schéma en Étoile)



WITH jobs AS (
    SELECT
        j.*
    FROM "duckdb"."main_silver"."int_job_title_normalization" j
),

companies AS (
    SELECT * FROM "duckdb"."main_gold"."dim_company"
),

locations AS (
    SELECT * FROM "duckdb"."main_gold"."dim_location"
),

times AS (
    SELECT * FROM "duckdb"."main_gold"."dim_time"
),

fact_table AS (
    SELECT
        -- Surrogate keys
        ROW_NUMBER() OVER (ORDER BY j.job_url, j.company_name_cleaned) as job_offer_id,
        
        -- Foreign keys
        c.company_id,
        l.location_id,
        t.date_id as published_date_id,
        
        -- Job dimensions
        j.job_title_cleaned as job_title,
        j.job_category,
        j.contract_type_normalized as contract_type,
        j.work_type_normalized as work_type,
        
        -- URLs
        j.job_url,
        j.company_url,
        
        -- Description
        j.job_description_cleaned as job_description,
        
        -- Time dimension
        j.published_date,
        j.posted_time,
        j.published_year_month,
        j.published_year,
        j.published_month,
        
        -- Metrics
        LENGTH(j.job_description_cleaned) as description_length,
        (LENGTH(j.job_description_cleaned) - LENGTH(REPLACE(j.job_description_cleaned, ' ', ''))) + 1 as word_count,
        
        -- Flags
        CASE WHEN j.work_type_normalized = 'Remote' THEN 1 ELSE 0 END as is_remote,
        CASE WHEN j.contract_type_normalized = 'Permanent' THEN 1 ELSE 0 END as is_permanent,
        
        -- Metadata
        NOW() as created_at,
        j.ingestion_timestamp
        
    FROM jobs j
    LEFT JOIN companies c ON j.company_name_cleaned = c.company_name
    LEFT JOIN locations l ON j.location_cleaned = l.location_raw
    LEFT JOIN times t ON j.published_date = t.date_id
)

SELECT
    job_offer_id,
    company_id,
    location_id,
    published_date_id,
    job_title,
    job_category,
    contract_type,
    work_type,
    job_url,
    company_url,
    job_description,
    published_date,
    posted_time,
    published_year_month,
    published_year,
    published_month,
    description_length,
    word_count,
    is_remote,
    is_permanent,
    created_at,
    ingestion_timestamp
FROM fact_table
ORDER BY published_date DESC, job_offer_id
    );
  
  
[0m15:19:07.689152 [debug] [Thread-3  ]: SQL status: OK in 0.165 seconds
[0m15:19:07.690982 [debug] [Thread-3  ]: Using duckdb connection "model.job_intelligent.fact_job_offers"
[0m15:19:07.691999 [debug] [Thread-3  ]: On model.job_intelligent.fact_job_offers: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.fact_job_offers"} */

    SELECT index_name
    FROM duckdb_indexes()
    WHERE schema_name = 'main_gold'
      AND table_name = 'fact_job_offers'
  
[0m15:19:07.694652 [debug] [Thread-3  ]: SQL status: OK in 0.001 seconds
[0m15:19:07.698361 [debug] [Thread-3  ]: Using duckdb connection "model.job_intelligent.fact_job_offers"
[0m15:19:07.699254 [debug] [Thread-3  ]: On model.job_intelligent.fact_job_offers: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.fact_job_offers"} */

    SELECT COUNT(*) as remaining_indexes
    FROM duckdb_indexes()
    WHERE schema_name = 'main_gold'
      AND table_name = 'fact_job_offers'
  
[0m15:19:07.699254 [debug] [Thread-3  ]: SQL status: OK in 0.001 seconds
[0m15:19:07.773903 [debug] [Thread-3  ]: Using duckdb connection "model.job_intelligent.fact_job_offers"
[0m15:19:07.774906 [debug] [Thread-3  ]: On model.job_intelligent.fact_job_offers: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.fact_job_offers"} */
alter table "duckdb"."main_gold"."fact_job_offers" rename to "fact_job_offers__dbt_backup"
[0m15:19:07.776929 [debug] [Thread-3  ]: SQL status: OK in 0.001 seconds
[0m15:19:07.783530 [debug] [Thread-3  ]: Using duckdb connection "model.job_intelligent.fact_job_offers"
[0m15:19:07.784508 [debug] [Thread-3  ]: On model.job_intelligent.fact_job_offers: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.fact_job_offers"} */
alter table "duckdb"."main_gold"."fact_job_offers__dbt_tmp" rename to "fact_job_offers"
[0m15:19:07.786504 [debug] [Thread-3  ]: SQL status: OK in 0.001 seconds
[0m15:19:07.789515 [debug] [Thread-3  ]: On model.job_intelligent.fact_job_offers: COMMIT
[0m15:19:07.791143 [debug] [Thread-3  ]: Using duckdb connection "model.job_intelligent.fact_job_offers"
[0m15:19:07.791143 [debug] [Thread-3  ]: On model.job_intelligent.fact_job_offers: COMMIT
[0m15:19:07.843330 [debug] [Thread-3  ]: SQL status: OK in 0.051 seconds
[0m15:19:07.848389 [debug] [Thread-3  ]: Using duckdb connection "model.job_intelligent.fact_job_offers"
[0m15:19:07.850701 [debug] [Thread-3  ]: On model.job_intelligent.fact_job_offers: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.fact_job_offers"} */

      drop table if exists "duckdb"."main_gold"."fact_job_offers__dbt_backup" cascade
    
[0m15:19:07.850701 [debug] [Thread-3  ]: SQL status: OK in 0.002 seconds
[0m15:19:07.856067 [debug] [Thread-3  ]: On model.job_intelligent.fact_job_offers: Close
[0m15:19:07.856067 [debug] [Thread-3  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '132c6645-99b1-4a1d-836f-a6ab29263d84', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000232CEE27640>]}
[0m15:19:07.859126 [info ] [Thread-3  ]: 8 of 13 OK created sql table model main_gold.fact_job_offers ................... [[32mOK[0m in 0.36s]
[0m15:19:07.859126 [debug] [Thread-3  ]: Finished running node model.job_intelligent.fact_job_offers
[0m15:19:07.859126 [debug] [Thread-1  ]: Began running node model.job_intelligent.agg_job_offers_by_category_time
[0m15:19:07.859126 [debug] [Thread-4  ]: Began running node model.job_intelligent.agg_location_analysis
[0m15:19:07.864808 [info ] [Thread-1  ]: 9 of 13 START sql table model main_gold.agg_job_offers_by_category_time ........ [RUN]
[0m15:19:07.864808 [info ] [Thread-4  ]: 10 of 13 START sql table model main_gold.agg_location_analysis ................. [RUN]
[0m15:19:07.869166 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.job_intelligent.dim_time, now model.job_intelligent.agg_job_offers_by_category_time)
[0m15:19:07.869166 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly model.job_intelligent.dim_company, now model.job_intelligent.agg_location_analysis)
[0m15:19:07.869166 [debug] [Thread-1  ]: Began compiling node model.job_intelligent.agg_job_offers_by_category_time
[0m15:19:07.869166 [debug] [Thread-4  ]: Began compiling node model.job_intelligent.agg_location_analysis
[0m15:19:07.880401 [debug] [Thread-1  ]: Writing injected SQL for node "model.job_intelligent.agg_job_offers_by_category_time"
[0m15:19:07.888368 [debug] [Thread-4  ]: Writing injected SQL for node "model.job_intelligent.agg_location_analysis"
[0m15:19:07.889396 [debug] [Thread-1  ]: Began executing node model.job_intelligent.agg_job_offers_by_category_time
[0m15:19:07.889396 [debug] [Thread-4  ]: Began executing node model.job_intelligent.agg_location_analysis
[0m15:19:07.900614 [debug] [Thread-1  ]: Writing runtime sql for node "model.job_intelligent.agg_job_offers_by_category_time"
[0m15:19:07.909174 [debug] [Thread-4  ]: Writing runtime sql for node "model.job_intelligent.agg_location_analysis"
[0m15:19:07.916893 [debug] [Thread-1  ]: Using duckdb connection "model.job_intelligent.agg_job_offers_by_category_time"
[0m15:19:07.918404 [debug] [Thread-1  ]: On model.job_intelligent.agg_job_offers_by_category_time: BEGIN
[0m15:19:07.920540 [debug] [Thread-4  ]: Using duckdb connection "model.job_intelligent.agg_location_analysis"
[0m15:19:07.920540 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m15:19:07.922576 [debug] [Thread-4  ]: On model.job_intelligent.agg_location_analysis: BEGIN
[0m15:19:07.922576 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m15:19:07.922576 [debug] [Thread-1  ]: SQL status: OK in 0.004 seconds
[0m15:19:07.928613 [debug] [Thread-1  ]: Using duckdb connection "model.job_intelligent.agg_job_offers_by_category_time"
[0m15:19:07.929167 [debug] [Thread-4  ]: SQL status: OK in 0.004 seconds
[0m15:19:07.930681 [debug] [Thread-1  ]: On model.job_intelligent.agg_job_offers_by_category_time: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.agg_job_offers_by_category_time"} */

  
    
    

    create  table
      "duckdb"."main_gold"."agg_job_offers_by_category_time__dbt_tmp"
  
    as (
      -- models/gold/agg_job_offers_by_category_time.sql
-- Gold layer: Aggregate - Job Offers by Category and Time



SELECT
    f.published_year,
    f.published_month,
    f.published_year_month,
    f.job_category,
    f.contract_type,
    f.work_type,
    
    COUNT(DISTINCT f.job_offer_id) as count_job_offers,
    COUNT(DISTINCT f.company_id) as count_companies,
    
    AVG(f.description_length) as avg_description_length,
    AVG(f.word_count) as avg_word_count,
    
    SUM(CASE WHEN f.is_remote = 1 THEN 1 ELSE 0 END) as remote_jobs,
    SUM(CASE WHEN f.is_permanent = 1 THEN 1 ELSE 0 END) as permanent_jobs,
    
    NOW() as created_at
    
FROM "duckdb"."main_gold"."fact_job_offers" f
WHERE f.published_date IS NOT NULL
GROUP BY
    f.published_year,
    f.published_month,
    f.published_year_month,
    f.job_category,
    f.contract_type,
    f.work_type
ORDER BY f.published_year_month DESC, f.job_category
    );
  
  
[0m15:19:07.933061 [debug] [Thread-4  ]: Using duckdb connection "model.job_intelligent.agg_location_analysis"
[0m15:19:07.935072 [debug] [Thread-4  ]: On model.job_intelligent.agg_location_analysis: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.agg_location_analysis"} */

  
    
    

    create  table
      "duckdb"."main_gold"."agg_location_analysis__dbt_tmp"
  
    as (
      -- models/gold/agg_location_analysis.sql
-- Gold layer: Aggregate - Location Analysis



SELECT
    dl.location_id,
    dl.location_raw,
    dl.city,
    dl.country,
    dl.work_location_type,
    
    COUNT(DISTINCT f.job_offer_id) as count_job_offers,
    COUNT(DISTINCT f.company_id) as count_companies,
    
    -- Distribution by job category
    COUNT(DISTINCT CASE WHEN f.job_category = 'Data Engineer' THEN f.job_offer_id END) as data_engineer_count,
    COUNT(DISTINCT CASE WHEN f.job_category = 'Data Scientist' THEN f.job_offer_id END) as data_scientist_count,
    COUNT(DISTINCT CASE WHEN f.job_category = 'Data Analyst' THEN f.job_offer_id END) as data_analyst_count,
    COUNT(DISTINCT CASE WHEN f.job_category = 'ML Engineer' THEN f.job_offer_id END) as ml_engineer_count,
    
    -- Remote percentage
    ROUND(
        100.0 * SUM(CASE WHEN f.is_remote = 1 THEN 1 ELSE 0 END) / COUNT(DISTINCT f.job_offer_id),
        2
    ) as pct_remote,
    
    NOW() as created_at
    
FROM "duckdb"."main_gold"."dim_location" dl
LEFT JOIN "duckdb"."main_gold"."fact_job_offers" f ON dl.location_id = f.location_id
GROUP BY
    dl.location_id,
    dl.location_raw,
    dl.city,
    dl.country,
    dl.work_location_type
ORDER BY count_job_offers DESC
    );
  
  
[0m15:19:07.954550 [debug] [Thread-1  ]: SQL status: OK in 0.020 seconds
[0m15:19:07.958594 [debug] [Thread-1  ]: Using duckdb connection "model.job_intelligent.agg_job_offers_by_category_time"
[0m15:19:07.960324 [debug] [Thread-1  ]: On model.job_intelligent.agg_job_offers_by_category_time: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.agg_job_offers_by_category_time"} */

    SELECT index_name
    FROM duckdb_indexes()
    WHERE schema_name = 'main_gold'
      AND table_name = 'agg_job_offers_by_category_time'
  
[0m15:19:07.969614 [debug] [Thread-1  ]: SQL status: OK in 0.006 seconds
[0m15:19:07.975502 [debug] [Thread-1  ]: Using duckdb connection "model.job_intelligent.agg_job_offers_by_category_time"
[0m15:19:07.977151 [debug] [Thread-1  ]: On model.job_intelligent.agg_job_offers_by_category_time: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.agg_job_offers_by_category_time"} */

    SELECT COUNT(*) as remaining_indexes
    FROM duckdb_indexes()
    WHERE schema_name = 'main_gold'
      AND table_name = 'agg_job_offers_by_category_time'
  
[0m15:19:07.979611 [debug] [Thread-4  ]: SQL status: OK in 0.043 seconds
[0m15:19:07.984597 [debug] [Thread-4  ]: Using duckdb connection "model.job_intelligent.agg_location_analysis"
[0m15:19:07.986367 [debug] [Thread-1  ]: SQL status: OK in 0.004 seconds
[0m15:19:07.987472 [debug] [Thread-4  ]: On model.job_intelligent.agg_location_analysis: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.agg_location_analysis"} */

    SELECT index_name
    FROM duckdb_indexes()
    WHERE schema_name = 'main_gold'
      AND table_name = 'agg_location_analysis'
  
[0m15:19:07.997347 [debug] [Thread-1  ]: Using duckdb connection "model.job_intelligent.agg_job_offers_by_category_time"
[0m15:19:08.000208 [debug] [Thread-1  ]: On model.job_intelligent.agg_job_offers_by_category_time: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.agg_job_offers_by_category_time"} */
alter table "duckdb"."main_gold"."agg_job_offers_by_category_time" rename to "agg_job_offers_by_category_time__dbt_backup"
[0m15:19:08.001217 [debug] [Thread-4  ]: SQL status: OK in 0.003 seconds
[0m15:19:08.004696 [debug] [Thread-4  ]: Using duckdb connection "model.job_intelligent.agg_location_analysis"
[0m15:19:08.005705 [debug] [Thread-1  ]: SQL status: OK in 0.003 seconds
[0m15:19:08.006699 [debug] [Thread-4  ]: On model.job_intelligent.agg_location_analysis: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.agg_location_analysis"} */

    SELECT COUNT(*) as remaining_indexes
    FROM duckdb_indexes()
    WHERE schema_name = 'main_gold'
      AND table_name = 'agg_location_analysis'
  
[0m15:19:08.016270 [debug] [Thread-1  ]: Using duckdb connection "model.job_intelligent.agg_job_offers_by_category_time"
[0m15:19:08.018444 [debug] [Thread-1  ]: On model.job_intelligent.agg_job_offers_by_category_time: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.agg_job_offers_by_category_time"} */
alter table "duckdb"."main_gold"."agg_job_offers_by_category_time__dbt_tmp" rename to "agg_job_offers_by_category_time"
[0m15:19:08.019472 [debug] [Thread-4  ]: SQL status: OK in 0.002 seconds
[0m15:19:08.020645 [debug] [Thread-4  ]: Using duckdb connection "model.job_intelligent.agg_location_analysis"
[0m15:19:08.020645 [debug] [Thread-1  ]: SQL status: OK in 0.008 seconds
[0m15:19:08.028264 [debug] [Thread-4  ]: On model.job_intelligent.agg_location_analysis: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.agg_location_analysis"} */
alter table "duckdb"."main_gold"."agg_location_analysis" rename to "agg_location_analysis__dbt_backup"
[0m15:19:08.031313 [debug] [Thread-1  ]: On model.job_intelligent.agg_job_offers_by_category_time: COMMIT
[0m15:19:08.031313 [debug] [Thread-1  ]: Using duckdb connection "model.job_intelligent.agg_job_offers_by_category_time"
[0m15:19:08.031313 [debug] [Thread-4  ]: SQL status: OK in 0.002 seconds
[0m15:19:08.038421 [debug] [Thread-1  ]: On model.job_intelligent.agg_job_offers_by_category_time: COMMIT
[0m15:19:08.044602 [debug] [Thread-4  ]: Using duckdb connection "model.job_intelligent.agg_location_analysis"
[0m15:19:08.047304 [debug] [Thread-4  ]: On model.job_intelligent.agg_location_analysis: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.agg_location_analysis"} */
alter table "duckdb"."main_gold"."agg_location_analysis__dbt_tmp" rename to "agg_location_analysis"
[0m15:19:08.049351 [debug] [Thread-1  ]: SQL status: OK in 0.004 seconds
[0m15:19:08.049351 [debug] [Thread-4  ]: SQL status: OK in 0.002 seconds
[0m15:19:08.049351 [debug] [Thread-1  ]: Using duckdb connection "model.job_intelligent.agg_job_offers_by_category_time"
[0m15:19:08.060520 [debug] [Thread-4  ]: On model.job_intelligent.agg_location_analysis: COMMIT
[0m15:19:08.060520 [debug] [Thread-1  ]: On model.job_intelligent.agg_job_offers_by_category_time: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.agg_job_offers_by_category_time"} */

      drop table if exists "duckdb"."main_gold"."agg_job_offers_by_category_time__dbt_backup" cascade
    
[0m15:19:08.064060 [debug] [Thread-4  ]: Using duckdb connection "model.job_intelligent.agg_location_analysis"
[0m15:19:08.066599 [debug] [Thread-4  ]: On model.job_intelligent.agg_location_analysis: COMMIT
[0m15:19:08.068591 [debug] [Thread-1  ]: SQL status: OK in 0.002 seconds
[0m15:19:08.071119 [debug] [Thread-1  ]: On model.job_intelligent.agg_job_offers_by_category_time: Close
[0m15:19:08.072119 [debug] [Thread-4  ]: SQL status: OK in 0.004 seconds
[0m15:19:08.074119 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '132c6645-99b1-4a1d-836f-a6ab29263d84', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000232CC4B7760>]}
[0m15:19:08.078665 [debug] [Thread-4  ]: Using duckdb connection "model.job_intelligent.agg_location_analysis"
[0m15:19:08.081248 [info ] [Thread-1  ]: 9 of 13 OK created sql table model main_gold.agg_job_offers_by_category_time ... [[32mOK[0m in 0.20s]
[0m15:19:08.083262 [debug] [Thread-4  ]: On model.job_intelligent.agg_location_analysis: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.agg_location_analysis"} */

      drop table if exists "duckdb"."main_gold"."agg_location_analysis__dbt_backup" cascade
    
[0m15:19:08.085257 [debug] [Thread-1  ]: Finished running node model.job_intelligent.agg_job_offers_by_category_time
[0m15:19:08.088770 [debug] [Thread-4  ]: SQL status: OK in 0.003 seconds
[0m15:19:08.091318 [debug] [Thread-4  ]: On model.job_intelligent.agg_location_analysis: Close
[0m15:19:08.093310 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '132c6645-99b1-4a1d-836f-a6ab29263d84', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000232CBFFD070>]}
[0m15:19:08.094822 [info ] [Thread-4  ]: 10 of 13 OK created sql table model main_gold.agg_location_analysis ............ [[32mOK[0m in 0.22s]
[0m15:19:08.096837 [debug] [Thread-4  ]: Finished running node model.job_intelligent.agg_location_analysis
[0m15:19:10.729331 [debug] [Thread-2  ]: SQL status: OK in 3.385 seconds
[0m15:19:10.729331 [debug] [Thread-2  ]: Using duckdb connection "model.job_intelligent.int_skills_extraction"
[0m15:19:10.729331 [debug] [Thread-2  ]: On model.job_intelligent.int_skills_extraction: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.int_skills_extraction"} */

    SELECT index_name
    FROM duckdb_indexes()
    WHERE schema_name = 'main_silver'
      AND table_name = 'int_skills_extraction'
  
[0m15:19:10.729331 [debug] [Thread-2  ]: SQL status: OK in 0.001 seconds
[0m15:19:10.739265 [debug] [Thread-2  ]: Using duckdb connection "model.job_intelligent.int_skills_extraction"
[0m15:19:10.739265 [debug] [Thread-2  ]: On model.job_intelligent.int_skills_extraction: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.int_skills_extraction"} */

    SELECT COUNT(*) as remaining_indexes
    FROM duckdb_indexes()
    WHERE schema_name = 'main_silver'
      AND table_name = 'int_skills_extraction'
  
[0m15:19:10.740462 [debug] [Thread-2  ]: SQL status: OK in 0.001 seconds
[0m15:19:10.749317 [debug] [Thread-2  ]: Using duckdb connection "model.job_intelligent.int_skills_extraction"
[0m15:19:10.749317 [debug] [Thread-2  ]: On model.job_intelligent.int_skills_extraction: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.int_skills_extraction"} */
alter table "duckdb"."main_silver"."int_skills_extraction" rename to "int_skills_extraction__dbt_backup"
[0m15:19:10.749317 [debug] [Thread-2  ]: SQL status: OK in 0.001 seconds
[0m15:19:10.756776 [debug] [Thread-2  ]: Using duckdb connection "model.job_intelligent.int_skills_extraction"
[0m15:19:10.758313 [debug] [Thread-2  ]: On model.job_intelligent.int_skills_extraction: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.int_skills_extraction"} */
alter table "duckdb"."main_silver"."int_skills_extraction__dbt_tmp" rename to "int_skills_extraction"
[0m15:19:10.759268 [debug] [Thread-2  ]: SQL status: OK in 0.001 seconds
[0m15:19:10.763778 [debug] [Thread-2  ]: On model.job_intelligent.int_skills_extraction: COMMIT
[0m15:19:10.765062 [debug] [Thread-2  ]: Using duckdb connection "model.job_intelligent.int_skills_extraction"
[0m15:19:10.765062 [debug] [Thread-2  ]: On model.job_intelligent.int_skills_extraction: COMMIT
[0m15:19:10.821019 [debug] [Thread-2  ]: SQL status: OK in 0.055 seconds
[0m15:19:10.825077 [debug] [Thread-2  ]: Using duckdb connection "model.job_intelligent.int_skills_extraction"
[0m15:19:10.826075 [debug] [Thread-2  ]: On model.job_intelligent.int_skills_extraction: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.int_skills_extraction"} */

      drop table if exists "duckdb"."main_silver"."int_skills_extraction__dbt_backup" cascade
    
[0m15:19:11.080271 [debug] [Thread-2  ]: SQL status: OK in 0.253 seconds
[0m15:19:11.080873 [debug] [Thread-2  ]: On model.job_intelligent.int_skills_extraction: Close
[0m15:19:11.080873 [debug] [Thread-2  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '132c6645-99b1-4a1d-836f-a6ab29263d84', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000232CE5374C0>]}
[0m15:19:11.080873 [info ] [Thread-2  ]: 7 of 13 OK created sql table model main_silver.int_skills_extraction ........... [[32mOK[0m in 3.83s]
[0m15:19:11.080873 [debug] [Thread-2  ]: Finished running node model.job_intelligent.int_skills_extraction
[0m15:19:11.080873 [debug] [Thread-3  ]: Began running node model.job_intelligent.dim_skills
[0m15:19:11.089150 [info ] [Thread-3  ]: 11 of 13 START sql table model main_gold.dim_skills ............................ [RUN]
[0m15:19:11.090285 [debug] [Thread-3  ]: Re-using an available connection from the pool (formerly model.job_intelligent.fact_job_offers, now model.job_intelligent.dim_skills)
[0m15:19:11.090285 [debug] [Thread-3  ]: Began compiling node model.job_intelligent.dim_skills
[0m15:19:11.090285 [debug] [Thread-3  ]: Writing injected SQL for node "model.job_intelligent.dim_skills"
[0m15:19:11.098353 [debug] [Thread-3  ]: Began executing node model.job_intelligent.dim_skills
[0m15:19:11.104003 [debug] [Thread-3  ]: Writing runtime sql for node "model.job_intelligent.dim_skills"
[0m15:19:11.105751 [debug] [Thread-3  ]: Using duckdb connection "model.job_intelligent.dim_skills"
[0m15:19:11.106961 [debug] [Thread-3  ]: On model.job_intelligent.dim_skills: BEGIN
[0m15:19:11.108470 [debug] [Thread-3  ]: Opening a new connection, currently in state closed
[0m15:19:11.109826 [debug] [Thread-3  ]: SQL status: OK in 0.001 seconds
[0m15:19:11.110624 [debug] [Thread-3  ]: Using duckdb connection "model.job_intelligent.dim_skills"
[0m15:19:11.111153 [debug] [Thread-3  ]: On model.job_intelligent.dim_skills: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_skills"} */

  
    
    

    create  table
      "duckdb"."main_gold"."dim_skills__dbt_tmp"
  
    as (
      -- models/gold/dim_skills.sql
-- Gold layer: Dimension Skills



WITH skills AS (
    SELECT DISTINCT
        skill_name
    FROM "duckdb"."main_silver"."int_skills_extraction"
    WHERE skill_name IS NOT NULL
),

skill_categorization AS (
    SELECT
        skill_name,
        CASE
            WHEN skill_name IN ('Python', 'Java', 'Scala', 'R') THEN 'Programming Language'
            WHEN skill_name IN ('SQL', 'NoSQL') THEN 'Database'
            WHEN skill_name IN ('Spark', 'Hadoop', 'Hive', 'Kafka') THEN 'Big Data Framework'
            WHEN skill_name IN ('TensorFlow', 'PyTorch', 'Scikit-learn') THEN 'ML/DL Library'
            WHEN skill_name IN ('AWS', 'Azure', 'GCP') THEN 'Cloud Platform'
            WHEN skill_name IN ('Tableau', 'Power BI', 'Looker') THEN 'BI Tool'
            WHEN skill_name IN ('Airflow', 'DBT', 'Kubernetes', 'Docker') THEN 'DataOps/DevOps'
            WHEN skill_name IN ('Pandas', 'NumPy', 'Matplotlib') THEN 'Data Analysis Library'
            WHEN skill_name IN ('Machine Learning', 'Statistics', 'Data Visualization') THEN 'Domain Knowledge'
            ELSE 'Other'
        END as skill_category
    FROM skills
),

ranked_skills AS (
    SELECT
        ROW_NUMBER() OVER (ORDER BY skill_name) as skill_id,
        skill_name,
        skill_category,
        NOW() as created_at
    FROM skill_categorization
)

SELECT
    skill_id,
    skill_name,
    skill_category,
    created_at
FROM ranked_skills
ORDER BY skill_id
    );
  
  
[0m15:19:11.118461 [debug] [Thread-3  ]: SQL status: OK in 0.006 seconds
[0m15:19:11.120036 [debug] [Thread-3  ]: Using duckdb connection "model.job_intelligent.dim_skills"
[0m15:19:11.121044 [debug] [Thread-3  ]: On model.job_intelligent.dim_skills: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_skills"} */

    SELECT index_name
    FROM duckdb_indexes()
    WHERE schema_name = 'main_gold'
      AND table_name = 'dim_skills'
  
[0m15:19:11.122047 [debug] [Thread-3  ]: SQL status: OK in 0.001 seconds
[0m15:19:11.125044 [debug] [Thread-3  ]: Using duckdb connection "model.job_intelligent.dim_skills"
[0m15:19:11.126047 [debug] [Thread-3  ]: On model.job_intelligent.dim_skills: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_skills"} */

    SELECT COUNT(*) as remaining_indexes
    FROM duckdb_indexes()
    WHERE schema_name = 'main_gold'
      AND table_name = 'dim_skills'
  
[0m15:19:11.127097 [debug] [Thread-3  ]: SQL status: OK in 0.001 seconds
[0m15:19:11.133853 [debug] [Thread-3  ]: Using duckdb connection "model.job_intelligent.dim_skills"
[0m15:19:11.133853 [debug] [Thread-3  ]: On model.job_intelligent.dim_skills: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_skills"} */
alter table "duckdb"."main_gold"."dim_skills" rename to "dim_skills__dbt_backup"
[0m15:19:11.135856 [debug] [Thread-3  ]: SQL status: OK in 0.001 seconds
[0m15:19:11.142047 [debug] [Thread-3  ]: Using duckdb connection "model.job_intelligent.dim_skills"
[0m15:19:11.143595 [debug] [Thread-3  ]: On model.job_intelligent.dim_skills: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_skills"} */
alter table "duckdb"."main_gold"."dim_skills__dbt_tmp" rename to "dim_skills"
[0m15:19:11.144613 [debug] [Thread-3  ]: SQL status: OK in 0.001 seconds
[0m15:19:11.149914 [debug] [Thread-3  ]: On model.job_intelligent.dim_skills: COMMIT
[0m15:19:11.150642 [debug] [Thread-3  ]: Using duckdb connection "model.job_intelligent.dim_skills"
[0m15:19:11.151651 [debug] [Thread-3  ]: On model.job_intelligent.dim_skills: COMMIT
[0m15:19:11.155978 [debug] [Thread-3  ]: SQL status: OK in 0.004 seconds
[0m15:19:11.160246 [debug] [Thread-3  ]: Using duckdb connection "model.job_intelligent.dim_skills"
[0m15:19:11.161222 [debug] [Thread-3  ]: On model.job_intelligent.dim_skills: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.dim_skills"} */

      drop table if exists "duckdb"."main_gold"."dim_skills__dbt_backup" cascade
    
[0m15:19:11.164781 [debug] [Thread-3  ]: SQL status: OK in 0.002 seconds
[0m15:19:11.167452 [debug] [Thread-3  ]: On model.job_intelligent.dim_skills: Close
[0m15:19:11.168405 [debug] [Thread-3  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '132c6645-99b1-4a1d-836f-a6ab29263d84', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000232CE5374C0>]}
[0m15:19:11.169787 [info ] [Thread-3  ]: 11 of 13 OK created sql table model main_gold.dim_skills ....................... [[32mOK[0m in 0.08s]
[0m15:19:11.170608 [debug] [Thread-3  ]: Finished running node model.job_intelligent.dim_skills
[0m15:19:11.171921 [debug] [Thread-4  ]: Began running node model.job_intelligent.fact_job_skills
[0m15:19:11.173537 [info ] [Thread-4  ]: 12 of 13 START sql table model main_gold.fact_job_skills ....................... [RUN]
[0m15:19:11.174726 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly model.job_intelligent.agg_location_analysis, now model.job_intelligent.fact_job_skills)
[0m15:19:11.175726 [debug] [Thread-4  ]: Began compiling node model.job_intelligent.fact_job_skills
[0m15:19:11.183377 [debug] [Thread-4  ]: Writing injected SQL for node "model.job_intelligent.fact_job_skills"
[0m15:19:11.185630 [debug] [Thread-4  ]: Began executing node model.job_intelligent.fact_job_skills
[0m15:19:11.192138 [debug] [Thread-4  ]: Writing runtime sql for node "model.job_intelligent.fact_job_skills"
[0m15:19:11.193723 [debug] [Thread-4  ]: Using duckdb connection "model.job_intelligent.fact_job_skills"
[0m15:19:11.194999 [debug] [Thread-4  ]: On model.job_intelligent.fact_job_skills: BEGIN
[0m15:19:11.196005 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m15:19:11.198147 [debug] [Thread-4  ]: SQL status: OK in 0.002 seconds
[0m15:19:11.198458 [debug] [Thread-4  ]: Using duckdb connection "model.job_intelligent.fact_job_skills"
[0m15:19:11.199515 [debug] [Thread-4  ]: On model.job_intelligent.fact_job_skills: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.fact_job_skills"} */

  
    
    

    create  table
      "duckdb"."main_gold"."fact_job_skills__dbt_tmp"
  
    as (
      -- models/gold/fact_job_skills.sql
-- Gold layer: Bridge Table - Job Skills



WITH skills_raw AS (
    SELECT DISTINCT
        job_url,
        company_name_cleaned,
        skill_name
    FROM "duckdb"."main_silver"."int_skills_extraction"
),

jobs AS (
    SELECT
        job_offer_id,
        job_url
    FROM "duckdb"."main_gold"."fact_job_offers"
),

skills_dim AS (
    SELECT
        skill_id,
        skill_name
    FROM "duckdb"."main_gold"."dim_skills"
),

fact_table AS (
    SELECT
        ROW_NUMBER() OVER (ORDER BY s.job_url, sd.skill_id) as job_skill_id,
        j.job_offer_id,
        sd.skill_id,
        s.skill_name,
        NOW() as created_at
    FROM skills_raw s
    LEFT JOIN jobs j ON s.job_url = j.job_url
    LEFT JOIN skills_dim sd ON s.skill_name = sd.skill_name
)

SELECT
    job_skill_id,
    job_offer_id,
    skill_id,
    skill_name,
    created_at
FROM fact_table
WHERE job_offer_id IS NOT NULL
ORDER BY job_offer_id, skill_id
    );
  
  
[0m15:19:11.217633 [debug] [Thread-4  ]: SQL status: OK in 0.017 seconds
[0m15:19:11.219444 [debug] [Thread-4  ]: Using duckdb connection "model.job_intelligent.fact_job_skills"
[0m15:19:11.220456 [debug] [Thread-4  ]: On model.job_intelligent.fact_job_skills: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.fact_job_skills"} */

    SELECT index_name
    FROM duckdb_indexes()
    WHERE schema_name = 'main_gold'
      AND table_name = 'fact_job_skills'
  
[0m15:19:11.222843 [debug] [Thread-4  ]: SQL status: OK in 0.001 seconds
[0m15:19:11.224842 [debug] [Thread-4  ]: Using duckdb connection "model.job_intelligent.fact_job_skills"
[0m15:19:11.225797 [debug] [Thread-4  ]: On model.job_intelligent.fact_job_skills: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.fact_job_skills"} */

    SELECT COUNT(*) as remaining_indexes
    FROM duckdb_indexes()
    WHERE schema_name = 'main_gold'
      AND table_name = 'fact_job_skills'
  
[0m15:19:11.227044 [debug] [Thread-4  ]: SQL status: OK in 0.001 seconds
[0m15:19:11.233604 [debug] [Thread-4  ]: Using duckdb connection "model.job_intelligent.fact_job_skills"
[0m15:19:11.234780 [debug] [Thread-4  ]: On model.job_intelligent.fact_job_skills: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.fact_job_skills"} */
alter table "duckdb"."main_gold"."fact_job_skills" rename to "fact_job_skills__dbt_backup"
[0m15:19:11.236719 [debug] [Thread-4  ]: SQL status: OK in 0.001 seconds
[0m15:19:11.241910 [debug] [Thread-4  ]: Using duckdb connection "model.job_intelligent.fact_job_skills"
[0m15:19:11.241910 [debug] [Thread-4  ]: On model.job_intelligent.fact_job_skills: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.fact_job_skills"} */
alter table "duckdb"."main_gold"."fact_job_skills__dbt_tmp" rename to "fact_job_skills"
[0m15:19:11.243593 [debug] [Thread-4  ]: SQL status: OK in 0.001 seconds
[0m15:19:11.246973 [debug] [Thread-4  ]: On model.job_intelligent.fact_job_skills: COMMIT
[0m15:19:11.249018 [debug] [Thread-4  ]: Using duckdb connection "model.job_intelligent.fact_job_skills"
[0m15:19:11.249866 [debug] [Thread-4  ]: On model.job_intelligent.fact_job_skills: COMMIT
[0m15:19:11.254062 [debug] [Thread-4  ]: SQL status: OK in 0.003 seconds
[0m15:19:11.259521 [debug] [Thread-4  ]: Using duckdb connection "model.job_intelligent.fact_job_skills"
[0m15:19:11.260533 [debug] [Thread-4  ]: On model.job_intelligent.fact_job_skills: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.fact_job_skills"} */

      drop table if exists "duckdb"."main_gold"."fact_job_skills__dbt_backup" cascade
    
[0m15:19:11.272954 [debug] [Thread-4  ]: SQL status: OK in 0.012 seconds
[0m15:19:11.275941 [debug] [Thread-4  ]: On model.job_intelligent.fact_job_skills: Close
[0m15:19:11.276945 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '132c6645-99b1-4a1d-836f-a6ab29263d84', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000232CE54C550>]}
[0m15:19:11.278454 [info ] [Thread-4  ]: 12 of 13 OK created sql table model main_gold.fact_job_skills .................. [[32mOK[0m in 0.10s]
[0m15:19:11.281039 [debug] [Thread-4  ]: Finished running node model.job_intelligent.fact_job_skills
[0m15:19:11.284078 [debug] [Thread-1  ]: Began running node model.job_intelligent.agg_skills_demand
[0m15:19:11.287047 [info ] [Thread-1  ]: 13 of 13 START sql table model main_gold.agg_skills_demand ..................... [RUN]
[0m15:19:11.289080 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.job_intelligent.agg_job_offers_by_category_time, now model.job_intelligent.agg_skills_demand)
[0m15:19:11.290091 [debug] [Thread-1  ]: Began compiling node model.job_intelligent.agg_skills_demand
[0m15:19:11.299500 [debug] [Thread-1  ]: Writing injected SQL for node "model.job_intelligent.agg_skills_demand"
[0m15:19:11.299500 [debug] [Thread-1  ]: Began executing node model.job_intelligent.agg_skills_demand
[0m15:19:11.308460 [debug] [Thread-1  ]: Writing runtime sql for node "model.job_intelligent.agg_skills_demand"
[0m15:19:11.310694 [debug] [Thread-1  ]: Using duckdb connection "model.job_intelligent.agg_skills_demand"
[0m15:19:11.311405 [debug] [Thread-1  ]: On model.job_intelligent.agg_skills_demand: BEGIN
[0m15:19:11.313565 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m15:19:11.314581 [debug] [Thread-1  ]: SQL status: OK in 0.002 seconds
[0m15:19:11.315584 [debug] [Thread-1  ]: Using duckdb connection "model.job_intelligent.agg_skills_demand"
[0m15:19:11.318482 [debug] [Thread-1  ]: On model.job_intelligent.agg_skills_demand: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.agg_skills_demand"} */

  
    
    

    create  table
      "duckdb"."main_gold"."agg_skills_demand__dbt_tmp"
  
    as (
      -- models/gold/agg_skills_demand.sql
-- Gold layer: Aggregate - Skills in Demand



SELECT
    sd.skill_id,
    sd.skill_name,
    sd.skill_category,
    
    COUNT(DISTINCT fs.job_offer_id) as count_jobs_requiring_skill,
    COUNT(DISTINCT f.company_id) as count_companies_requiring_skill,
    
    -- Percentage of all jobs
    ROUND(
        100.0 * COUNT(DISTINCT fs.job_offer_id) / (
            SELECT COUNT(DISTINCT job_offer_id) FROM "duckdb"."main_gold"."fact_job_offers"
        ),
        2
    ) as pct_of_total_jobs,
    
    -- Average job details for jobs requiring this skill
    AVG(f.description_length) as avg_description_length,
    AVG(f.word_count) as avg_word_count,
    
    NOW() as created_at
    
FROM "duckdb"."main_gold"."dim_skills" sd
LEFT JOIN "duckdb"."main_gold"."fact_job_skills" fs ON sd.skill_id = fs.skill_id
LEFT JOIN "duckdb"."main_gold"."fact_job_offers" f ON fs.job_offer_id = f.job_offer_id
GROUP BY
    sd.skill_id,
    sd.skill_name,
    sd.skill_category
ORDER BY count_jobs_requiring_skill DESC
    );
  
  
[0m15:19:11.334909 [debug] [Thread-1  ]: SQL status: OK in 0.015 seconds
[0m15:19:11.336908 [debug] [Thread-1  ]: Using duckdb connection "model.job_intelligent.agg_skills_demand"
[0m15:19:11.336908 [debug] [Thread-1  ]: On model.job_intelligent.agg_skills_demand: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.agg_skills_demand"} */

    SELECT index_name
    FROM duckdb_indexes()
    WHERE schema_name = 'main_gold'
      AND table_name = 'agg_skills_demand'
  
[0m15:19:11.339442 [debug] [Thread-1  ]: SQL status: OK in 0.001 seconds
[0m15:19:11.341649 [debug] [Thread-1  ]: Using duckdb connection "model.job_intelligent.agg_skills_demand"
[0m15:19:11.342649 [debug] [Thread-1  ]: On model.job_intelligent.agg_skills_demand: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.agg_skills_demand"} */

    SELECT COUNT(*) as remaining_indexes
    FROM duckdb_indexes()
    WHERE schema_name = 'main_gold'
      AND table_name = 'agg_skills_demand'
  
[0m15:19:11.344166 [debug] [Thread-1  ]: SQL status: OK in 0.001 seconds
[0m15:19:11.352949 [debug] [Thread-1  ]: Using duckdb connection "model.job_intelligent.agg_skills_demand"
[0m15:19:11.353949 [debug] [Thread-1  ]: On model.job_intelligent.agg_skills_demand: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.agg_skills_demand"} */
alter table "duckdb"."main_gold"."agg_skills_demand" rename to "agg_skills_demand__dbt_backup"
[0m15:19:11.354951 [debug] [Thread-1  ]: SQL status: OK in 0.000 seconds
[0m15:19:11.360556 [debug] [Thread-1  ]: Using duckdb connection "model.job_intelligent.agg_skills_demand"
[0m15:19:11.361561 [debug] [Thread-1  ]: On model.job_intelligent.agg_skills_demand: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.agg_skills_demand"} */
alter table "duckdb"."main_gold"."agg_skills_demand__dbt_tmp" rename to "agg_skills_demand"
[0m15:19:11.363559 [debug] [Thread-1  ]: SQL status: OK in 0.001 seconds
[0m15:19:11.367755 [debug] [Thread-1  ]: On model.job_intelligent.agg_skills_demand: COMMIT
[0m15:19:11.369267 [debug] [Thread-1  ]: Using duckdb connection "model.job_intelligent.agg_skills_demand"
[0m15:19:11.370521 [debug] [Thread-1  ]: On model.job_intelligent.agg_skills_demand: COMMIT
[0m15:19:11.372864 [debug] [Thread-1  ]: SQL status: OK in 0.002 seconds
[0m15:19:11.378413 [debug] [Thread-1  ]: Using duckdb connection "model.job_intelligent.agg_skills_demand"
[0m15:19:11.379440 [debug] [Thread-1  ]: On model.job_intelligent.agg_skills_demand: /* {"app": "dbt", "dbt_version": "1.10.18", "profile_name": "job_intelligent", "target_name": "dev", "node_id": "model.job_intelligent.agg_skills_demand"} */

      drop table if exists "duckdb"."main_gold"."agg_skills_demand__dbt_backup" cascade
    
[0m15:19:11.383329 [debug] [Thread-1  ]: SQL status: OK in 0.003 seconds
[0m15:19:11.385328 [debug] [Thread-1  ]: On model.job_intelligent.agg_skills_demand: Close
[0m15:19:11.386382 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '132c6645-99b1-4a1d-836f-a6ab29263d84', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000232CBAD4DF0>]}
[0m15:19:11.387376 [info ] [Thread-1  ]: 13 of 13 OK created sql table model main_gold.agg_skills_demand ................ [[32mOK[0m in 0.10s]
[0m15:19:11.389395 [debug] [Thread-1  ]: Finished running node model.job_intelligent.agg_skills_demand
[0m15:19:11.391081 [debug] [MainThread]: Using duckdb connection "master"
[0m15:19:11.392126 [debug] [MainThread]: On master: BEGIN
[0m15:19:11.393095 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m15:19:11.395096 [debug] [MainThread]: SQL status: OK in 0.001 seconds
[0m15:19:11.395096 [debug] [MainThread]: On master: COMMIT
[0m15:19:11.396097 [debug] [MainThread]: Using duckdb connection "master"
[0m15:19:11.397096 [debug] [MainThread]: On master: COMMIT
[0m15:19:11.399135 [debug] [MainThread]: SQL status: OK in 0.000 seconds
[0m15:19:11.400193 [debug] [MainThread]: On master: Close
[0m15:19:11.401146 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:19:11.402192 [debug] [MainThread]: Connection 'create_duckdb_main_gold' was properly closed.
[0m15:19:11.402192 [debug] [MainThread]: Connection 'create_duckdb_main_bronze' was properly closed.
[0m15:19:11.403145 [debug] [MainThread]: Connection 'create_duckdb_main_silver' was properly closed.
[0m15:19:11.403145 [debug] [MainThread]: Connection 'list_duckdb_main_silver' was properly closed.
[0m15:19:11.404142 [debug] [MainThread]: Connection 'list_duckdb_main_gold' was properly closed.
[0m15:19:11.405148 [debug] [MainThread]: Connection 'list_duckdb_main_bronze' was properly closed.
[0m15:19:11.406193 [debug] [MainThread]: Connection 'model.job_intelligent.agg_skills_demand' was properly closed.
[0m15:19:11.407145 [debug] [MainThread]: Connection 'model.job_intelligent.dim_skills' was properly closed.
[0m15:19:11.407145 [debug] [MainThread]: Connection 'model.job_intelligent.int_skills_extraction' was properly closed.
[0m15:19:11.407145 [debug] [MainThread]: Connection 'model.job_intelligent.fact_job_skills' was properly closed.
[0m15:19:11.409232 [info ] [MainThread]: 
[0m15:19:11.410243 [info ] [MainThread]: Finished running 12 table models, 1 view model in 0 hours 0 minutes and 6.00 seconds (6.00s).
[0m15:19:11.415412 [debug] [MainThread]: Command end result
[0m15:19:11.451671 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Ayoub Gorry\Desktop\powerbi\jobs-power-bi\dbt_project\target\manifest.json
[0m15:19:11.456046 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Ayoub Gorry\Desktop\powerbi\jobs-power-bi\dbt_project\target\semantic_manifest.json
[0m15:19:11.470179 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\Ayoub Gorry\Desktop\powerbi\jobs-power-bi\dbt_project\target\run_results.json
[0m15:19:11.470783 [info ] [MainThread]: 
[0m15:19:11.471810 [info ] [MainThread]: [32mCompleted successfully[0m
[0m15:19:11.472812 [info ] [MainThread]: 
[0m15:19:11.473827 [info ] [MainThread]: Done. PASS=13 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=13
[0m15:19:11.474819 [debug] [MainThread]: Command `dbt run` succeeded at 15:19:11.474819 after 7.61 seconds
[0m15:19:11.475819 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000232C96C2190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000232CA9B7190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000232CBFE87F0>]}
[0m15:19:11.476820 [debug] [MainThread]: Flushing usage events
[0m15:19:12.177143 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:19:16.344296 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001313ABD2130>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001313CE983A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001313CE981F0>]}


============================== 15:19:16.350856 | 79298e91-ea22-4c1c-a7ff-1d5ad954c36d ==============================
[0m15:19:16.350856 [info ] [MainThread]: Running with dbt=1.10.18
[0m15:19:16.352867 [debug] [MainThread]: running dbt with arguments {'log_path': 'C:\\Users\\Ayoub Gorry\\Desktop\\powerbi\\jobs-power-bi\\dbt_project\\logs', 'log_format': 'default', 'partial_parse': 'True', 'version_check': 'True', 'empty': 'None', 'log_cache_events': 'False', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Ayoub Gorry\\Desktop\\powerbi\\jobs-power-bi\\dbt_project', 'no_print': 'None', 'send_anonymous_usage_stats': 'True', 'debug': 'False', 'printer_width': '80', 'introspect': 'True', 'invocation_command': 'dbt test', 'target_path': 'None', 'use_experimental_parser': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'use_colors': 'True', 'static_parser': 'True', 'quiet': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'write_json': 'True', 'indirect_selection': 'eager'}
[0m15:19:16.812443 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '79298e91-ea22-4c1c-a7ff-1d5ad954c36d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001313913A9A0>]}
[0m15:19:16.940551 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '79298e91-ea22-4c1c-a7ff-1d5ad954c36d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001313CE4B460>]}
[0m15:19:16.950402 [info ] [MainThread]: Registered adapter: duckdb=1.10.0
[0m15:19:17.340473 [debug] [MainThread]: checksum: 9646db7d1a5f5b9389d9c545d3ebf898b26bc1f63f6b5366f34a6af01d52cb91, vars: {}, profile: , target: , version: 1.10.18
[0m15:19:17.589040 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:19:17.589551 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:19:17.599306 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- seeds
[0m15:19:17.630745 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '79298e91-ea22-4c1c-a7ff-1d5ad954c36d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001313E4CA130>]}
[0m15:19:17.721423 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Ayoub Gorry\Desktop\powerbi\jobs-power-bi\dbt_project\target\manifest.json
[0m15:19:17.730657 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Ayoub Gorry\Desktop\powerbi\jobs-power-bi\dbt_project\target\semantic_manifest.json
[0m15:19:17.789454 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '79298e91-ea22-4c1c-a7ff-1d5ad954c36d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001313E5CE940>]}
[0m15:19:17.790464 [info ] [MainThread]: Found 13 models, 456 macros
[0m15:19:17.791045 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '79298e91-ea22-4c1c-a7ff-1d5ad954c36d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001313E4E1700>]}
[0m15:19:17.793049 [warn ] [MainThread]: Nothing to do. Try checking your model configs and model specification args
[0m15:19:17.796056 [debug] [MainThread]: Command end result
[0m15:19:17.819483 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Ayoub Gorry\Desktop\powerbi\jobs-power-bi\dbt_project\target\manifest.json
[0m15:19:17.819483 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Ayoub Gorry\Desktop\powerbi\jobs-power-bi\dbt_project\target\semantic_manifest.json
[0m15:19:17.830225 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\Ayoub Gorry\Desktop\powerbi\jobs-power-bi\dbt_project\target\run_results.json
[0m15:19:17.830225 [debug] [MainThread]: Command `dbt test` succeeded at 15:19:17.830225 after 1.63 seconds
[0m15:19:17.830225 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001313ABD2130>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001313E05AD30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013139EEEFA0>]}
[0m15:19:17.830225 [debug] [MainThread]: Flushing usage events
[0m15:19:18.369341 [debug] [MainThread]: An error was encountered while trying to flush usage events
